# CC6205 - Natural Language Processing
This is a course on natural language processing.

* Lecturer: [Felipe Bravo-Marquez](https://felipebravom.com/)
* TAs: [Gabriel Iturra-Bocaz](https://giturra.cl/), [Jorge Ortiz](https://www.ortizfuentes.com/), Consuelo Rojas, Sebastián Tinoco and [Felipe Urrutia](http://www.dim.uchile.cl/~furrutia/).

* [Course Notes (in Spanish)](https://raw.githubusercontent.com/dccuchile/CC6205/master/apunte/apunte.pdf)

* Lectures: Tuesday 14:30 - 16:00, Thursday 14:30 - 16:00  

* [Course Program](https://docs.google.com/document/d/1DNja7nf0b26aRWF_gMNJf9L6SLtvtyFpucDhfcgG4d0/edit?usp=sharing) (in Spanish)

* [Course Calendar](calendar.md)

* [Youtube Playlist with lectures](https://www.youtube.com/playlist?list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi)


### Info

This course aims to provide a comprehensive introduction to Natural Language Processing (NLP) by covering essential concepts. We strive to strike a balance between traditional techniques, such as N-gram language models, Naive Bayes, and Hidden Markov Models (HMMs), and modern deep neural networks, including word embeddings, recurrent neural networks (RNNs), and transformers.

The course material draws from various sources. In many instances, sentences from these sources are directly incorporated into the slides. The neural network topics primarily rely on the book [Neural Network Methods for Natural Language Processing](https://link.springer.com/book/10.1007/978-3-031-02165-7) by Goldberg. Non-neural network topics, such as Probabilistic Language Models, Naive Bayes, and HMMs, are sourced from [Michael Collins' course](http://www.cs.columbia.edu/~mcollins/) and [Dan Jurafsky's book](https://web.stanford.edu/~jurafsky/slp3/). Additionally, some slides are adapted from online tutorials and other courses, such as [Manning's Stanford course](http://web.stanford.edu/class/cs224n/).


## Slides

1. [Introduction to Natural Language Processing](slides/NLP-introduction.pdf) | ([tex source file](slides/NLP-introduction.tex)), [video 1](https://youtu.be/HEKTNOttGvU), [video 2](https://youtu.be/P8cwnI-f-Kg)
2. [Vector Space Model and Information Retrieval](slides/NLP-IR.pdf) | ([tex source file](slides/NLP-IR.tex)), [video 1](https://www.youtube.com/watch?v=FXIVClF370w&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=2&t=0s), [video 2](https://www.youtube.com/watch?v=f8nG1EMmPZk&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=2)
3. [Probabilistic Language Models](slides/NLP-PLM.pdf) | ([tex source file](slides/NLP-PLM.tex)), [notes](http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf), [video 1](https://www.youtube.com/watch?v=9E2jJ6kcb4Y&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=3), [video 2](https://www.youtube.com/watch?v=ZWqbEQXLra0&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=4), [video 3](https://www.youtube.com/watch?v=tsumFqwFlaA&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=5), [video 4](https://www.youtube.com/watch?v=s3TWdv4sqkg&list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&index=6)
4. [Text Classification and Naive Bayes](slides/NLP-NB.pdf) | ([tex source file](slides/NLP-NB.tex)) , [notes](https://web.stanford.edu/~jurafsky/slp3/4.pdf), [video 1](https://youtu.be/kG9BK9Oy1hU), [video 2](https://youtu.be/Iqte5kKHvzE), [video 3](https://youtu.be/TSJg0_X3Abk)
5. [Linear Models](slides/NLP-linear.pdf) | ([tex source file](slides/NLP-linear.tex)), [video 1](https://youtu.be/zhBxDsNLZEA), [video 2](https://youtu.be/Fooua_uaWSE), [video 3](https://youtu.be/DqbzhdQa1eQ), [video 4](https://youtu.be/1nfWWXqfAzA)
6. [Neural Networks](slides/NLP-neural.pdf) | ([tex source file](slides/NLP-neural.tex)), [video 1](https://youtu.be/oHZHA8h2xN0), [video 2](https://youtu.be/2lXank0W6G4), [video 3](https://youtu.be/BUDIi9qItzY), [video 4](https://youtu.be/KKN2Ipy-vGk)       
7. [Word Vectors](slides/NLP-wordvectors.pdf) | ([tex source file](slides/NLP-wordvectors.tex)) [video 1](https://youtu.be/wtwUsJMC9CA), [video 2](https://youtu.be/XDxzQ7JU95U), [video 3](https://youtu.be/Ikyc3DRVodk)
8. [Sequence Labeling and Hidden Markov Models ](slides/NLP-HMM.pdf) | ([tex source file](slides/NLP-HMM.tex)), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       
9. [MEMMs and CRFs](slides/NLP-CRF.pdf) | ([tex source file](slides/NLP-CRF.tex)), [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)
10. [Convolutional Neural Networks](slides/NLP-CNN.pdf) | ([tex source file](slides/NLP-CNN.tex)), [video](https://youtu.be/lLZW5Fn40r8)
11. [Recurrent Neural Networks](slides/NLP-RNN.pdf) | ([tex source file](slides/NLP-RNN.tex)), [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)
12. [Sequence to Sequence Models and Attention](slides/NLP-seq2seq.pdf) | ([tex source file](slides/NLP-seq2seq.tex)), [video 1](https://youtu.be/OpKxRjISqmM), [video 2](https://youtu.be/WQ7ihm5voB0)
13. [Transformer Architecture](slides/NLP-transformer.pdf) | ([tex source file](slides/NLP-seq2seq.tex)), [video 1](https://youtu.be/8RE23Uq8rU0)
14. [Contextualized Embeddings and Large Language Models](slides/NLP-LLM.pdf), [video 1](https://youtu.be/sSGbgZpHymI), [video 2](https://youtu.be/C-QfzWU6eUE), [video 3](https://youtu.be/5j4Mgl3GuVY)


## NLP Libraries and Tools

1. [NLTK: Natural Language Toolkit](https://www.nltk.org/)
2. [Gensim](https://radimrehurek.com/gensim/)
3. [spaCy: Industrial-strength NLP](https://spacy.io/)
4. [Torchtext](https://torchtext.readthedocs.io/en/latest/)
5. [AllenNLP: Open source project for designing deep leaning-based NLP models](https://allennlp.org/)
6. [HuggingFace Transformers](https://huggingface.co/docs/transformers/index)
7. [ChatGPT](https://chat.openai.com/)
8. [Google Bard](https://bard.google.com)
9. [Stanza - A Python NLP Library for Many Human Languages](https://stanfordnlp.github.io/stanza/)
10. [FlairNLP: A very simple framework for state-of-the-art Natural Language Processing (NLP)](https://github.com/flairNLP/flair)
11. [WEFE: The Word Embeddings Fairness Evaluation Framework](https://wefe.readthedocs.io/en/latest/)
12. [WhatLies: A library that tries help you to understand. "What lies in word embeddings?"](https://rasahq.github.io/whatlies/)
13. [LASER:a library to calculate and use multilingual sentence embeddings](https://github.com/facebookresearch/LASER)
14. [Sentence Transformers: Multilingual Sentence Embeddings using BERT / RoBERTa / XLM-RoBERTa & Co. with PyTorch](https://github.com/UKPLab/sentence-transformers)
15. [Datasets: a lightweight library with one-line dataloaders for many public datasets in NLP](https://github.com/huggingface/datasets)
16. [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://dccuchile.github.io/rivertext/)


## Notes and Books 
1. [Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/).
2. [Michael Collins' NLP notes](http://www.cs.columbia.edu/~mcollins/).
3. [A Primer on Neural Network Models for Natural Language Processing by Joav Goldberg](https://u.cs.biu.ac.il/~yogo/nnlp.pdf).
4. [Natural Language Understanding with Distributed Representation by Kyunghyun Cho](https://arxiv.org/abs/1511.07916)
5. [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
6. [Natural Language Processing Book by Jacob Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)
7. [NLTK book](http://www.nltk.org/book/)
8. [Embeddings in Natural Language Processing by Mohammad Taher Pilehvar and Jose Camacho-Collados](http://josecamachocollados.com/book_embNLP_draft.pdf)
9. [Dive into Deep Learning Book](https://d2l.ai/)
10. [Contextual Word Representations: A Contextual Introduction by Noah A. Smith](https://arxiv.org/pdf/1902.06006.pdf)

## Other NLP Courses
1. [CS224n: Natural Language Processing with Deep Learning, Stanford course](http://web.stanford.edu/class/cs224n/)
2. [Deep Learning in NLP: slides by Horacio Rodríguez](https://www.cs.upc.edu/~horacio/ahlt/DeepLearning02.pdf)
3. [David Bamman NLP Slides @Berkley](http://people.ischool.berkeley.edu/~dbamman/nlp18.html)
4. [CS 521: Statistical Natural Language Processing by Natalie Parde, University of Illinois](http://www.natalieparde.com/teaching/cs521_spring2020.html)
5. [10 Free Top Notch Natural Language Processing Courses](https://www.kdnuggets.com/2019/10/10-free-top-notch-courses-natural-language-processing.html)


### Videos 

1. [Natural Language Processing MOOC videos by Dan Jurafsky and Chris Manning, 2012](https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&disable_polymer=true)
2. [Natural Language Processing MOOC videos by Michael Collins, 2013](https://www.youtube.com/channel/UCB_JX4jH3QQmp69rmkWpl1A/playlists?shelf_id=3&view=50&sort=dd)
3. [Natural Language Processing with Deep Learning by Chris Manning and Richard Socher, 2017](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)
4. [CS224N: Natural Language Processing with Deep Learning | Winter 2019](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
5. [Computational Linguistics I by Jordan Boyd-Graber  University of Maryland](https://www.youtube.com/playlist?list=PLegWUnz91WfuPebLI97-WueAP90JO-15i)  
5. [Visualizing and Understanding Recurrent Networks](https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks)
5. [BERT Research Series by  Chris McCormick](https://www.youtube.com/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6)
5. [Successes and Challenges in Neural Models for Speech and Language - Michael Collins](https://www.youtube.com/watch?v=jfwqRMdTmLo)
5. [More on Transforemers: BERT and Friends by Jorge Pérez](https://tv.vera.com.uy/video/55388)


## Other Resources
1. [ACL Portal](https://www.aclweb.org/portal/)
2. [Awesome-nlp: A curated list of resources dedicated to Natural Language Processing](https://github.com/keon/awesome-nlp)
3. [NLP-progress: Repository to track the progress in Natural Language Processing (NLP)](http://nlpprogress.com/)
4. [Corpora Mailing List](https://mailman.uib.no/listinfo/corpora)
5. [🤗 Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
6. [Real World NLP Book: AllenNLP tutorials](http://www.realworldnlpbook.com/)
7. [The Illustrated Transformer: a very illustrative blog post about the Transformer](http://jalammar.github.io/illustrated-transformer/)
8. [Better Language Models
   and Their Implications OpenAI Blog](https://openai.com/blog/better-language-models/)
9. [Understanding LoRA and QLoRA — The Powerhouses of Efficient Finetuning in Large Language Models](https://medium.com/@gitlostmurali/understanding-lora-and-qlora-the-powerhouses-of-efficient-finetuning-in-large-language-models-7ac1adf6c0cf)
10. [RNN effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
11. [SuperGLUE: an benchmark of Natural Language Understanding Tasks](https://super.gluebenchmark.com/)
12. [decaNLP The Natural Language Decathlon: a benchmark for studying general NLP models that can perform a variety of complex, natural language tasks](http://decanlp.com/).
13. [Chatbot and Related Research Paper Notes with Images](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images)
14. [Ben Trevett's torchtext tutorials](https://github.com/bentrevett/)
15. [PLMpapers: a collection of papers about Pre-Trained Language Models](https://github.com/thunlp/PLMpapers)
16. [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)
17. [Linguistics, NLP, and Interdisciplinarity Or: Look at Your Data, by Emily M. Bender](https://medium.com/@emilymenonbender/linguistics-nlp-and-interdisciplinarity-or-look-at-your-data-e49e03d37c9c)
18. [The State of NLP Literature: Part I, by Saif Mohammad](https://medium.com/@nlpscholar/state-of-nlp-cbf768492f90)
19. [From Word to Sense Embeddings:A Survey on Vector Representations of Meaning](https://arxiv.org/pdf/1805.04032.pdf)
20. [10 ML & NLP Research Highlights of 2019 by Sebastian Ruder](https://ruder.io/research-highlights-2019/index.html)
21. [Towards a Conversational Agent that Can Chat About…Anything](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html?m=1)
22. [The Super Duper NLP Repo:  a collection of Colab notebooks covering a wide array of NLP task implementations](https://notebooks.quantumstat.com/)
23. [The Big Bad NLP Database, a collection of nearly 300 well-organized, sortable, and searchable natural language processing datasets](https://datasets.quantumstat.com/)
24. [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)
25. [How Self-Attention with Relative Position Representations works](https://link.medium.com/wFxx3d96f7)
26. [Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/pdf/2004.03705.pdf)
27. [Teaching NLP is quite depressing, and I don't know how to do it well by Yoav Goldberg](https://twitter.com/yoavgo/status/1318567498653061122)
28. [The NLP index](https://index.quantumstat.com/)
29. [100 Must-Read NLP Papers](https://github.com/amanchadha/100-nlp-papers)



