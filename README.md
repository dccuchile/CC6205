# CC6205 - Natural Language Processing
This is a course on natural language processing.

* Lecturer: [Felipe Bravo-Marquez](https://felipebravom.com/)
* TAs: [Pablo Badilla](https://github.com/pabloBad), [Gabriel Chaperón](https://github.com/gchaperon), [Cristián Tamblay](https://github.com/cristian-tamblay)

* Lectures: Tuesday 14:30 - 16:00, Thursday 14:30 - 16:00  (Lecture Room B104, Beauchef 851, Piso 1, Edificio Norte)

* [Course Program](https://docs.google.com/document/d/1DNja7nf0b26aRWF_gMNJf9L6SLtvtyFpucDhfcgG4d0/edit?usp=sharing) (in Spanish)

* [Course Calendar](calendar.md)

* [Youtube Playlist with lectures](https://www.youtube.com/playlist?list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi)


### Info
The neural network-related topics of the course are taken from the book of Yoav Goldberg: 
[Neural Network Methods for Natural Language Processing](https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037). The non-neural network topics (e.g., grammars, HMMS) are taken from the course of [Michael Collins](http://www.cs.columbia.edu/~mcollins/). 



## Slides

1. [Introduction to Natural Language Processing](slides/NLP-introduction.pdf) | ([tex source file](slides/NLP-introduction.tex))
1. [Vector Space Model and Information Retrieval](slides/NLP-IR.pdf) | ([tex source file](slides/NLP-IR.tex))
2. [Language Models](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/lmslides.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf), [videos 1](https://www.youtube.com/playlist?list=PLlQBy7xY8mbJONAWxZmZsHj0iqjMpKONi), [videos 2](https://www.youtube.com/playlist?list=PLlQBy7xY8mbIzwoPRvZSjq9PO84CfJQb6), [videos 3](https://www.youtube.com/playlist?list=PLlQBy7xY8mbKXp8iLHVO56g1XvcXhr8BT)
2. [Text Classification and Naive Bayes](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) (slides by Dan Jurafsky), [notes](https://web.stanford.edu/~jurafsky/slp3/4.pdf), [video 1](https://www.youtube.com/watch?v=MgjrV_oXCrk&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=25&t=0s), [video 2](https://www.youtube.com/watch?v=S_Jdoubh-fE&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=26&t=0s), [video 3](https://www.youtube.com/watch?v=AAcIGGv7Q2g&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=27&t=0s), [video 4](https://www.youtube.com/watch?v=gSWf4EHvgBo&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=28&t=0s), [video 5](https://www.youtube.com/watch?v=x2sZQvIeYLg&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=29&t=0s), [video 6](https://www.youtube.com/watch?v=HQG7hZRoCpE&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=30&t=0s), [video 7](https://www.youtube.com/watch?v=0lY6D5WOzC8&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=31&t=0s), [video 8](https://www.youtube.com/watch?v=vDpGw-r-uNQ&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=32&t=0s), [video 9](https://www.youtube.com/watch?v=yvfZdqt2qek&list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&index=33&t=0s) 
2. [Linear Models](slides/NLP-linear.pdf) | ([tex source file](slides/NLP-linear.tex))
2. [Neural Networks](slides/NLP-neural.pdf) | ([tex source file](slides/NLP-neural.tex))
3. [Word Vectors](slides/NLP-wordvectors.pdf) | ([tex source file](slides/NLP-wordvectors.tex))
2. [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [videos](https://www.youtube.com/playlist?list=PLlQBy7xY8mbI13gwXZz4r55MeatSZOqm7)
4. [MEMMs and CRFs](slides/NLP-CRF.pdf) | ([tex source file](slides/NLP-CRF.tex))
4. [Convolutional Neural Networks](slides/NLP-CNN.pdf) | ([tex source file](slides/NLP-CNN.tex)), [video](https://youtu.be/lLZW5Fn40r8)
5. [Recurrent Neural Networks](slides/NLP-RNN.pdf) | ([tex source file](slides/NLP-RNN.tex)), [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)
6. [Sequence to Sequence Models, Attention, and the Transformer](slides/NLP-seq2seq.pdf) | ([tex source file](slides/NLP-seq2seq.tex)), [video 1](https://youtu.be/OpKxRjISqmM), [video 2](https://youtu.be/WQ7ihm5voB0), [video 3](https://youtu.be/8RE23Uq8rU0)
2. Constituency Parsing [slides 1](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/parsing1.pdf), [slides 2](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/parsing2.pdf), [slides 3](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/parsing2.2.pdf), [slides 4](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/parsing3.pdf)    (slides by Michael Collins), [notes 1](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lexpcfgs.pdf), [videos 1](https://www.youtube.com/watch?v=0tGFWbc2834&list=PLlQBy7xY8mbK9Uy9i7MTGSwyLJPii3w1L), [videos 2](https://www.youtube.com/watch?v=2hLBHSKbS44&list=PLlQBy7xY8mbKypSJe_AjVtCuXXsdODiDi), [videos 3](https://www.youtube.com/watch?v=8rD1Y6rz4Q0&list=PLlQBy7xY8mbKz9QvQU_IsOlbhshjkOwR9), [videos 4](https://www.youtube.com/watch?v=qDiVCxLq2As&list=PLlQBy7xY8mbI5o81CWHt50RtFDLYrlaKN) 
7. [Recursive Networks and Paragraph Vectors](slides/NLP-recursive.pdf) | ([tex source file](slides/NLP-recursive.tex))



## Other Resources



1. [Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/).
2. [Michael Collins' NLP notes](http://www.cs.columbia.edu/~mcollins/).
3. [A Primer on Neural Network Models for Natural Language Processing by Joav Goldberg](https://u.cs.biu.ac.il/~yogo/nnlp.pdf).
4. [Natural Language Understanding with Distributed Representation by Kyunghyun Cho](https://arxiv.org/abs/1511.07916)
5. [Natural Language Processing Book by Jacob Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)
6. [CS224n: Natural Language Processing with Deep Learning, Stanford course](http://web.stanford.edu/class/cs224n/)
7. [NLP-progress: Repository to track the progress in Natural Language Processing (NLP)](http://nlpprogress.com/)
8. [NLTK book](http://www.nltk.org/book/)
9. [AllenNLP: Open source project for designing deep leaning-based NLP models](https://allennlp.org/)
10. [Real World NLP Book: AllenNLP tutorials](http://www.realworldnlpbook.com/)
11. [Attention is all you need explained](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
11. [The Illustrated Transformer: a very illustrative blog post about the Transformer](http://jalammar.github.io/illustrated-transformer/)
12. [ELMO explained](http://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/)
13. [BERT exaplained](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)
14. [Better Language Models
    and Their Implications OpenAI Blog](https://openai.com/blog/better-language-models/)
15. [David Bamman NLP Slides @Berkley](http://people.ischool.berkeley.edu/~dbamman/nlp18.html)
16. [RNN effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
17. [SuperGLUE: an benchmark of Natural Language Understanding Tasks](https://super.gluebenchmark.com/)
18. [decaNLP The Natural Language Decathlon: a benchmark for studying general NLP models that can perform a variety of complex, natural language tasks](http://decanlp.com/).
19. [Deep Learning in NLP: slides by Horacio Rodríguez](https://www.cs.upc.edu/~horacio/ahlt/DeepLearning02.pdf)
20. [Chatbot and Related Research Paper Notes with Images](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images)
21. [XLNet Explained](http://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/)
22. [PyTorch-Transformers: a library of state-of-the-art pre-trained models for Natural Language Processing (NLP)](https://huggingface.co/pytorch-transformers/index.html)
23. [Ben Trevett's torchtext tutorials](https://github.com/bentrevett/)
24. [PLMpapers: a collection of papers about Pre-Trained Language Models](https://github.com/thunlp/PLMpapers)
25. [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)
26. [Linguistics, NLP, and Interdisciplinarity Or: Look at Your Data, by Emily M. Bender](https://medium.com/@emilymenonbender/linguistics-nlp-and-interdisciplinarity-or-look-at-your-data-e49e03d37c9c)
27. [The State of NLP Literature: Part I, by Saif Mohammad](https://medium.com/@nlpscholar/state-of-nlp-cbf768492f90)
28. [From Word to Sense Embeddings:A Survey on Vector Representations of Meaning](https://arxiv.org/pdf/1805.04032.pdf)
29. [10 ML & NLP Research Highlights of 2019 by Sebastian Ruder](https://ruder.io/research-highlights-2019/index.html)
30. [Towards a Conversational Agent that Can Chat About…Anything](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html?m=1)
##  





### Videos 

1. [Natural Language Processing MOOC videos by Dan Jurafsky and Chris Manning, 2012](https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&disable_polymer=true)
2. [Natural Language Processing MOOC videos by Michael Collins, 2013](https://www.youtube.com/channel/UCB_JX4jH3QQmp69rmkWpl1A/playlists?shelf_id=3&view=50&sort=dd)
3. [Natural Language Processing with Deep Learning by Chris Manning and Richard Socher, 2017](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)
4. [CS224N: Natural Language Processing with Deep Learning | Winter 2019](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
5. [Computational Linguistics I by Jordan Boyd-Graber  University of Maryland](https://www.youtube.com/playlist?list=PLegWUnz91WfuPebLI97-WueAP90JO-15i)  
5. [Visualizing and Understanding Recurrent Networks](https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks)
5. [BERT Research Series by  Chris McCormick](https://www.youtube.com/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6)
