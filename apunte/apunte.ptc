\babel@toc {spanish}{}\relax 
\contentsline {chapter}{Prefacio}{11}{chapter*.2}%
\contentsline {chapter}{\numberline {1}Introducción}{13}{chapter.1}%
\ttl@starttoc {default@1}
\contentsline {section}{\numberline {1.1}Desafíos del Procesamiento del Lenguaje Natural (PLN)}{13}{section.1.1}%
\contentsline {paragraph}{Ambigüedad}{13}{section*.3}%
\contentsline {paragraph}{Dinamismo}{14}{section*.4}%
\contentsline {paragraph}{Discretitud}{14}{section*.5}%
\contentsline {paragraph}{Composicionalidad}{14}{section*.6}%
\contentsline {paragraph}{Dispersión (sparseness)}{14}{section*.7}%
\contentsline {section}{\numberline {1.2}PLN y Lingüística Computacional}{14}{section.1.2}%
\contentsline {section}{\numberline {1.3}Tareas en Procesamiento del Lenguaje Natural (PLN)}{15}{section.1.3}%
\contentsline {section}{\numberline {1.4}Niveles de descripción lingüística}{16}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Fonética}{16}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Fonología}{16}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Morfología}{16}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Sintaxis}{17}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}Semántica}{17}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}Pragmática}{18}{subsection.1.4.6}%
\contentsline {section}{\numberline {1.5}Aprendizaje Automático en PLN}{18}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Ejemplo 1: Clasificación de Tópicos}{19}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Ejemplo 2: Análisis de Sentimiento}{19}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Lingüística y Procesamiento del Lenguaje Natural (PNL)}{21}{subsection.1.5.3}%
\contentsline {subsection}{\numberline {1.5.4}Limitaciones del Aprendizaje Supervisado}{21}{subsection.1.5.4}%
\contentsline {section}{\numberline {1.6}Etiquetado de Datos en PLN}{21}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Supervisión a Distancia}{22}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Crowdsourcing}{23}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Paradigmas de Aprendizaje Automático}{23}{section.1.7}%
\contentsline {paragraph}{Aprendizaje Profundo y Conceptos Lingüísticos}{24}{section*.8}%
\contentsline {section}{\numberline {1.8}Historia de PLN}{24}{section.1.8}%
\contentsline {section}{\numberline {1.9}Conclusiones y Estructura del Apunte}{25}{section.1.9}%
\contentsline {chapter}{\numberline {2}Modelo de Espacio Vectorial}{27}{chapter.2}%
\ttl@stoptoc {default@1}
\ttl@starttoc {default@2}
\contentsline {section}{\numberline {2.1}Tokens y Tipos}{27}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Tipos}{27}{subsection.2.1.1}%
\contentsline {paragraph}{Extracción de Vocabulario}{28}{section*.9}%
\contentsline {section}{\numberline {2.2}Eliminación de Stopwords}{28}{section.2.2}%
\contentsline {section}{\numberline {2.3}Stemming}{28}{section.2.3}%
\contentsline {section}{\numberline {2.4}Lematización}{29}{section.2.4}%
\contentsline {section}{\numberline {2.5}Ley de Zipf}{29}{section.2.5}%
\contentsline {section}{\numberline {2.6}Listas de posteo y el índice invertido}{30}{section.2.6}%
\contentsline {section}{\numberline {2.7}Motores de búsqueda web}{30}{section.2.7}%
\contentsline {section}{\numberline {2.8}El modelo de espacio vectorial}{31}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}El modelo TF-IDF}{32}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Similitud entre vectores}{33}{subsection.2.8.2}%
\contentsline {paragraph}{Ejercicio}{33}{section*.10}%
\contentsline {section}{\numberline {2.9}Clustering de Documentos}{34}{section.2.9}%
\contentsline {section}{\numberline {2.10}Modelos de Tópicos}{35}{section.2.10}%
\contentsline {section}{\numberline {2.11}Conclusiones y Conceptos Adicionales}{36}{section.2.11}%
\contentsline {chapter}{\numberline {3}Modelos de Lenguaje Probabilísticos}{37}{chapter.3}%
\ttl@stoptoc {default@2}
\ttl@starttoc {default@3}
\contentsline {section}{\numberline {3.1}El Problema del Modelado del Lenguaje}{37}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}¿Por qué querríamos hacer esto?}{38}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Regla de la Cadena en Modelos de Lenguaje}{39}{section.3.2}%
\contentsline {section}{\numberline {3.3}Mirada Predictiva}{39}{section.3.3}%
\contentsline {section}{\numberline {3.4}Mirada Generativa}{40}{section.3.4}%
\contentsline {section}{\numberline {3.5}Un Método Ingenuo}{41}{section.3.5}%
\contentsline {section}{\numberline {3.6}Procesos de Markov}{41}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Modelado de secuencias de longitud variable}{41}{subsection.3.6.1}%
\contentsline {section}{\numberline {3.7}Modelos de lenguaje de Trigramas}{42}{section.3.7}%
\contentsline {section}{\numberline {3.8}Evaluación de un modelo de lenguaje: Perplejidad}{43}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Intuición sobre la perplejidad}{43}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}El trade-off entre sesgo y varianza}{44}{subsection.3.8.2}%
\contentsline {section}{\numberline {3.9}Interpolación Lineal}{45}{section.3.9}%
\contentsline {subsection}{\numberline {3.9.1}Estimación de los Valores $\lambda $}{46}{subsection.3.9.1}%
\contentsline {section}{\numberline {3.10}Modelos de Descuento (Katz Back-Off)}{46}{section.3.10}%
\contentsline {section}{\numberline {3.11}Historia}{49}{section.3.11}%
\contentsline {section}{\numberline {3.12}Conclusiones}{50}{section.3.12}%
\contentsline {chapter}{\numberline {4}Clasificación de Texto y Naïve Bayes}{51}{chapter.4}%
\ttl@stoptoc {default@3}
\ttl@starttoc {default@4}
\contentsline {section}{\numberline {4.1}Ejemplos de Problemas de Clasificación}{52}{section.4.1}%
\contentsline {section}{\numberline {4.2}Modelos Generativos}{53}{section.4.2}%
\contentsline {section}{\numberline {4.3}Naïve Bayes Multinomial}{54}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Estimación de parámetros}{55}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Problemas al multiplicar muchas probabilidades}{55}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Suavizamiento de Laplace}{56}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Naïve Bayes como modelo de lenguaje}{58}{subsection.4.3.4}%
\contentsline {section}{\numberline {4.4}Evaluación}{59}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Matriz de Confusión}{60}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Métricas de Desempeño}{60}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Una Medida Combinada: Medida F}{61}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Conjuntos de Prueba de Desarrollo ("Devsets")}{61}{subsection.4.4.4}%
\contentsline {subsection}{\numberline {4.4.5}Validación Cruzada: Múltiples Divisiones}{62}{subsection.4.4.5}%
\contentsline {section}{\numberline {4.5}Conjuntos de entrenamiento, prueba y validación}{62}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Matriz de Confusión para clasificación de 3 clases}{63}{subsection.4.5.1}%
\contentsline {chapter}{\numberline {5}Modelos Lineales}{65}{chapter.5}%
\ttl@stoptoc {default@4}
\ttl@starttoc {default@5}
\contentsline {section}{\numberline {5.1}Ejemplo: Detección de Idiomas}{65}{section.5.1}%
\contentsline {section}{\numberline {5.2}Clasificación binaria log-lineal}{67}{section.5.2}%
\contentsline {section}{\numberline {5.3}Clasificación multiclase}{68}{section.5.3}%
\contentsline {section}{\numberline {5.4}Representaciones}{68}{section.5.4}%
\contentsline {section}{\numberline {5.5}Representación de Vectores One-Hot}{69}{section.5.5}%
\contentsline {section}{\numberline {5.6}Entrenamiento}{70}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Optimización basada en Gradiente}{71}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Descenso de Gradiente Estocástico en Línea}{71}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Descenso de Gradiente Estocástico en Mini-batch}{72}{subsection.5.6.3}%
\contentsline {subsection}{\numberline {5.6.4}Funciones de Pérdida}{73}{subsection.5.6.4}%
\contentsline {section}{\numberline {5.7}Regularización}{74}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Regularización L$_2$}{74}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Regularización L$_1$}{75}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Elastic-Net}{75}{subsection.5.7.3}%
\contentsline {section}{\numberline {5.8}Más allá del SGD}{75}{section.5.8}%
\contentsline {section}{\numberline {5.9}Una limitación de los modelos lineales: el problema XOR}{75}{section.5.9}%
\contentsline {subsection}{\numberline {5.9.1}Transformaciones no lineales de las entradas}{76}{subsection.5.9.1}%
\contentsline {chapter}{\numberline {6}Redes Neuronales}{79}{chapter.6}%
\ttl@stoptoc {default@5}
\ttl@starttoc {default@6}
\contentsline {section}{\numberline {6.1}Redes neuronales feedforward}{79}{section.6.1}%
\contentsline {paragraph}{Capas completamente conectadas como multiplicaciones de vectores y matrices}{80}{section*.11}%
\contentsline {subsection}{\numberline {6.1.1}Redes neuronales como funciones matemáticas}{80}{subsection.6.1.1}%
\contentsline {section}{\numberline {6.2}Capacidad de representación}{81}{section.6.2}%
\contentsline {section}{\numberline {6.3}Funciones de activación}{81}{section.6.3}%
\contentsline {paragraph}{Sigmoide}{81}{section*.12}%
\contentsline {paragraph}{Tangente hiperbólica (tanh)}{82}{section*.13}%
\contentsline {paragraph}{Hard tanh}{82}{section*.14}%
\contentsline {paragraph}{ReLU}{82}{section*.15}%
\contentsline {paragraph}{Leaky ReLU}{83}{section*.16}%
\contentsline {paragraph}{ELU}{83}{section*.17}%
\contentsline {subsection}{\numberline {6.3.1}Problemas Prácticos}{83}{subsection.6.3.1}%
\contentsline {section}{\numberline {6.4}Capas de Embedding}{84}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Vectores Densos vs Representaciones One-hot}{84}{subsection.6.4.1}%
\contentsline {paragraph}{Ejemplo: Vectores Densos vs Representaciones One-hot}{85}{section*.18}%
\contentsline {section}{\numberline {6.5}Entrenamiento de Redes Neuronales}{86}{section.6.5}%
\contentsline {section}{\numberline {6.6}Recordatorio de la Regla de la Cadena en Derivadas}{86}{section.6.6}%
\contentsline {section}{\numberline {6.7}Retropropagación}{88}{section.6.7}%
\contentsline {section}{\numberline {6.8}La Abstracción del Grafo de Cómputo}{91}{section.6.8}%
\contentsline {subsection}{\numberline {6.8.1}Cómputo hacia Adelante}{91}{subsection.6.8.1}%
\contentsline {subsection}{\numberline {6.8.2}Cómputo hacia Atrás (Retropropagación)}{92}{subsection.6.8.2}%
\contentsline {subsection}{\numberline {6.8.3}Resumen de la Abstracción del Grafo de Cómputo}{92}{subsection.6.8.3}%
\contentsline {subsection}{\numberline {6.8.4}Derivadas de funciones no matemáticas}{93}{subsection.6.8.4}%
\contentsline {section}{\numberline {6.9}Regularización y Dropout}{93}{section.6.9}%
\contentsline {section}{\numberline {6.10}Frameworks de Aprendizaje Profundo}{94}{section.6.10}%
\contentsline {chapter}{\numberline {7}Vectores de Palabra}{95}{chapter.7}%
\ttl@stoptoc {default@6}
\ttl@starttoc {default@7}
\contentsline {section}{\numberline {7.1}Hipótesis Distribucional y Matrices Palabra Contexto}{95}{section.7.1}%
\contentsline {section}{\numberline {7.2}PPMI}{96}{section.7.2}%
\contentsline {section}{\numberline {7.3}Modelo de Lenguaje Neuronal}{97}{section.7.3}%
\contentsline {section}{\numberline {7.4}Word2Vec}{98}{section.7.4}%
\contentsline {subsection}{\numberline {7.4.1}Parametrización del modelo Skip-gram}{100}{subsection.7.4.1}%
\contentsline {subsection}{\numberline {7.4.2}Skip-gram con Negative Sampling}{101}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Continuous Bag of Words: CBOW}{101}{subsection.7.4.3}%
\contentsline {subsection}{\numberline {7.4.4}GloVe}{102}{subsection.7.4.4}%
\contentsline {section}{\numberline {7.5}Analogías de palabras}{102}{section.7.5}%
\contentsline {section}{\numberline {7.6}Evaluación}{102}{section.7.6}%
\contentsline {section}{\numberline {7.7}Correspondencia entre modelos distribuidos y distribucionales}{103}{section.7.7}%
\contentsline {section}{\numberline {7.8}FastText}{104}{section.7.8}%
\contentsline {section}{\numberline {7.9}Embbedings de frases específicas de sentimiento}{104}{section.7.9}%
\contentsline {section}{\numberline {7.10}Gensim}{105}{section.7.10}%
\contentsline {chapter}{\numberline {8}Etiquetado de Secuencias}{107}{chapter.8}%
\ttl@stoptoc {default@7}
\ttl@starttoc {default@8}
\contentsline {section}{\numberline {8.1}Tareas de Etiquetado de Secuencias o Etiquetado}{107}{section.8.1}%
\contentsline {section}{\numberline {8.2}Etiquetado de Partes del Discurso}{107}{section.8.2}%
\contentsline {section}{\numberline {8.3}Reconocimiento de Entidades Nombradas (NER)}{108}{section.8.3}%
\contentsline {paragraph}{Etiquetado BIO: NER como Etiquetado de Secuencias}{108}{section*.19}%
\contentsline {subsection}{\numberline {8.3.1}Etiquetado de secuencias como aprendizaje supervisado}{109}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Enfoque generativo para el etiquetado de secuencias}{109}{subsection.8.3.2}%
\contentsline {section}{\numberline {8.4}Modelos Ocultos de Markov}{109}{section.8.4}%
\contentsline {section}{\numberline {8.5}Modelos Ocultos de Markov Trigramas (Trigram HMM)}{109}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Parámetros del modelo}{110}{subsection.8.5.1}%
\contentsline {paragraph}{Un ejemplo}{110}{section*.20}%
\contentsline {section}{\numberline {8.6}Supuestos de independencia en los HMM trigramas}{110}{section.8.6}%
\contentsline {section}{\numberline {8.7}¿Por qué el nombre?}{111}{section.8.7}%
\contentsline {section}{\numberline {8.8}Estimación Suavizada}{111}{section.8.8}%
\contentsline {section}{\numberline {8.9}Tratando con Palabras de Baja Frecuencia}{112}{section.8.9}%
\contentsline {section}{\numberline {8.10}Problema de Decodificación}{112}{section.8.10}%
\contentsline {subsection}{\numberline {8.10.1}Método Bruto Ingenuo}{113}{subsection.8.10.1}%
\contentsline {section}{\numberline {8.11}Decodificación de Viterbi con Programación Dinámica}{113}{section.8.11}%
\contentsline {paragraph}{Factorial}{113}{section*.21}%
\contentsline {paragraph}{Fibonacci}{114}{section*.22}%
\contentsline {paragraph}{Complejidad}{114}{section*.23}%
\contentsline {section}{\numberline {8.12}El Algoritmo de Viterbi}{114}{section.8.12}%
\contentsline {paragraph}{Un Ejemplo}{115}{section*.24}%
\contentsline {subsection}{\numberline {8.12.1}Una Definición Recursiva}{115}{subsection.8.12.1}%
\contentsline {paragraph}{Justificación de la Definición Recursiva}{115}{section*.25}%
\contentsline {subsection}{\numberline {8.12.2}El Algoritmo de Viterbi}{116}{subsection.8.12.2}%
\contentsline {subsection}{\numberline {8.12.3}El Algoritmo de Viterbi con Punteros de Retroceso}{116}{subsection.8.12.3}%
\contentsline {subsection}{\numberline {8.12.4}El Algoritmo de Viterbi: Tiempo de Ejecución}{117}{subsection.8.12.4}%
\contentsline {paragraph}{Ventajas y Desventajas}{117}{section*.26}%
\contentsline {section}{\numberline {8.13}MEMMs}{117}{section.8.13}%
\contentsline {section}{\numberline {8.14}Ejemplo de características utilizadas en el etiquetado de partes del habla}{119}{section.8.14}%
\contentsline {section}{\numberline {8.15}Plantillas de características}{119}{section.8.15}%
\contentsline {paragraph}{Ejemplo}{119}{section*.27}%
\contentsline {section}{\numberline {8.16}MEMMs y Softmax Multiclase}{119}{section.8.16}%
\contentsline {section}{\numberline {8.17}Entrenamiento de los MEMMs}{120}{section.8.17}%
\contentsline {section}{\numberline {8.18}Decodificación con MEMMs}{120}{section.8.18}%
\contentsline {section}{\numberline {8.19}Comparación entre MEMMs y HMMs}{122}{section.8.19}%
\contentsline {section}{\numberline {8.20}Campos Aleatorios Condicionales (CRFs)}{122}{section.8.20}%
\contentsline {paragraph}{Ejemplo}{123}{section*.28}%
\contentsline {section}{\numberline {8.21}Decodificación con CRFs}{124}{section.8.21}%
\contentsline {section}{\numberline {8.22}Estimación de Parámetros en CRFs (Entrenamiento)}{125}{section.8.22}%
\contentsline {section}{\numberline {8.23}CRFs y MEMMs}{125}{section.8.23}%
\contentsline {subsection}{\numberline {8.23.1}CRFs y MEMMs: el problema del sesgo de etiqueta}{125}{subsection.8.23.1}%
\contentsline {section}{\numberline {8.24}Enlaces}{126}{section.8.24}%
\contentsline {chapter}{\numberline {9}Redes Neuronales Convolucionales}{127}{chapter.9}%
\ttl@stoptoc {default@8}
\ttl@starttoc {default@9}
\contentsline {section}{\numberline {9.1}Redes Neuronales Convolucionales (CNN) en Procesamiento del Lenguaje Natural (PLN)}{127}{section.9.1}%
\contentsline {section}{\numberline {9.2}Convolución Básica + Agrupamiento}{127}{section.9.2}%
\contentsline {section}{\numberline {9.3}Convoluciones 1D sobre Texto}{128}{section.9.3}%
\contentsline {section}{\numberline {9.4}Convoluciones Angostas vs. Amplias}{128}{section.9.4}%
\contentsline {section}{\numberline {9.5}Agrupamiento Vectorial}{129}{section.9.5}%
\contentsline {section}{\numberline {9.6}Clasificación de Sentimientos en Twitter con CNN}{129}{section.9.6}%
\contentsline {section}{\numberline {9.7}Redes Neuronales Convolucionales Muy Profundas para la Clasificación de Texto}{129}{section.9.7}%
\contentsline {chapter}{\numberline {10}Redes Neuronales Recurrentes}{131}{chapter.10}%
\ttl@stoptoc {default@9}
\ttl@starttoc {default@10}
\contentsline {section}{\numberline {10.1}La Abstracción de las RNN}{131}{section.10.1}%
\contentsline {section}{\numberline {10.2}Red Elman o Simple-RNN}{133}{section.10.2}%
\contentsline {section}{\numberline {10.3}Entrenamiento de las RNN}{133}{section.10.3}%
\contentsline {section}{\numberline {10.4}Patrones de uso de las RNN: Aceptador}{134}{section.10.4}%
\contentsline {section}{\numberline {10.5}Patrones de uso de las RNN: Transductor}{135}{section.10.5}%
\contentsline {section}{\numberline {10.6}Redes neuronales recurrentes bidireccionales (BIRNN)}{135}{section.10.6}%
\contentsline {section}{\numberline {10.7}Redes neuronales recurrentes multi-capa (apiladas)}{136}{section.10.7}%
\contentsline {section}{\numberline {10.8}Arquitecturas con compuertas}{137}{section.10.8}%
\contentsline {section}{\numberline {10.9}GRU}{140}{section.10.9}%
\contentsline {section}{\numberline {10.10}Clasificación de sentimientos con RNN}{141}{section.10.10}%
\contentsline {section}{\numberline {10.11}Clasificación de sentimientos en Twitter con LSTMS y Emojis}{141}{section.10.11}%
\contentsline {section}{\numberline {10.12}Bi-LSTM CRF}{142}{section.10.12}%
\contentsline {chapter}{\numberline {11}Modelos Secuencia a Secuencia}{145}{chapter.11}%
\ttl@stoptoc {default@10}
\ttl@starttoc {default@11}
\contentsline {section}{\numberline {11.1}Modelos de lenguaje y generación de lenguaje}{145}{section.11.1}%
\contentsline {section}{\numberline {11.2}Problemas de secuencia a secuencia}{146}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Generación condicionada}{146}{subsection.11.2.1}%
\contentsline {paragraph}{Gráfico de entrenamiento de secuencia a secuencia}{146}{section*.29}%
\contentsline {paragraph}{Traducción automática neuronal}{146}{section*.30}%
\contentsline {paragraph}{Progreso de BLEU en traducción automática}{146}{section*.31}%
\contentsline {section}{\numberline {11.3}Enfoques de decodificación}{147}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Búsqueda Beam}{147}{subsection.11.3.1}%
\contentsline {section}{\numberline {11.4}Generación condicionada con atención}{148}{section.11.4}%
\contentsline {section}{\numberline {11.5}Atención y alineaciones de palabras}{151}{section.11.5}%
\contentsline {section}{\numberline {11.6}Otros tipos de atención}{151}{section.11.6}%
\contentsline {chapter}{\numberline {12}Arquitectura de Transformer}{153}{chapter.12}%
\ttl@stoptoc {default@11}
\ttl@starttoc {default@12}
\contentsline {subsection}{\numberline {12.0.1}Dependencias en la traducción automática neuronal}{153}{subsection.12.0.1}%
\contentsline {section}{\numberline {12.1}El Transformer}{153}{section.12.1}%
\contentsline {section}{\numberline {12.2}Mecanismo de atención en el Transformer}{154}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Consultas}{155}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Claves}{155}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Valores}{156}{subsection.12.2.3}%
\contentsline {subsection}{\numberline {12.2.4}Atención de producto puntual escalado}{156}{subsection.12.2.4}%
\contentsline {section}{\numberline {12.3}El Codificador}{157}{section.12.3}%
\contentsline {paragraph}{Introduciendo los tensores en la imagen}{157}{section*.32}%
\contentsline {section}{\numberline {12.4}Autoatención a alto nivel}{159}{section.12.4}%
\contentsline {section}{\numberline {12.5}Autoatención en detalle}{159}{section.12.5}%
\contentsline {paragraph}{Paso 1}{159}{section*.33}%
\contentsline {paragraph}{Paso 2}{160}{section*.34}%
\contentsline {paragraph}{Pasos 3 y 4}{161}{section*.35}%
\contentsline {paragraph}{Autoatención en detalle: pasos 5 y 6}{161}{section*.36}%
\contentsline {section}{\numberline {12.6}Cálculo matricial de la autoatención}{162}{section.12.6}%
\contentsline {section}{\numberline {12.7}Atención multi-head}{163}{section.12.7}%
\contentsline {section}{\numberline {12.8}Conexiones residuales}{165}{section.12.8}%
\contentsline {section}{\numberline {12.9}El codificador: resumen}{165}{section.12.9}%
\contentsline {section}{\numberline {12.10}El Decodificador}{166}{section.12.10}%
\contentsline {section}{\numberline {12.11}La Capa Lineal Final y el Entrenamiento}{167}{section.12.11}%
\contentsline {section}{\numberline {12.12}Codificaciones posicionales}{168}{section.12.12}%
\contentsline {section}{\numberline {12.13}Conclusiones}{169}{section.12.13}%
\contentsline {chapter}{\numberline {13}Grandes Modelos de Lenguaje}{171}{chapter.13}%
\ttl@stoptoc {default@12}
\ttl@starttoc {default@13}
\contentsline {section}{\numberline {13.1}Representaciones para una palabra}{171}{section.13.1}%
\contentsline {section}{\numberline {13.2}Los Modelos de Lenguaje Neurales pueden producir Incrustaciones Contextualizadas}{171}{section.13.2}%
\contentsline {section}{\numberline {13.3}ELMo: Incrustaciones de Modelos de Lenguaje}{171}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}ELMo: Uso con una tarea}{172}{subsection.13.3.1}%
\contentsline {paragraph}{ELMo: Resultados}{172}{section*.37}%
\contentsline {section}{\numberline {13.4}ULMfit}{172}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Énfasis de ULMfit}{173}{subsection.13.4.1}%
\contentsline {subsection}{\numberline {13.4.2}Transferencia de aprendizaje con ULMfit}{173}{subsection.13.4.2}%
\contentsline {section}{\numberline {13.5}¡Aumentemos la escala!}{173}{section.13.5}%
\contentsline {section}{\numberline {13.6}BERT (Bidirectional Encoder Representations from Transformers)}{173}{section.13.6}%
\contentsline {section}{\numberline {13.7}Modelado de Lenguaje Mascarado y Predicción de la Siguiente Oración}{174}{section.13.7}%
\contentsline {section}{\numberline {13.8}Codificación de pares de oraciones en BERT}{174}{section.13.8}%
\contentsline {section}{\numberline {13.9}Arquitectura y entrenamiento del modelo BERT}{175}{section.13.9}%
\contentsline {section}{\numberline {13.10}Ajuste fino del modelo BERT}{175}{section.13.10}%
\contentsline {subsection}{\numberline {13.10.1}Resultados de BERT en tareas GLUE}{175}{subsection.13.10.1}%
\contentsline {paragraph}{Tareas de GLUE}{176}{section*.38}%
\contentsline {subsection}{\numberline {13.10.2}Efecto de la tarea de preentrenamiento en BERT}{176}{subsection.13.10.2}%
\contentsline {section}{\numberline {13.11}Decodificadores de preentrenamiento GPT y GPT-2}{176}{section.13.11}%
\contentsline {paragraph}{Salida del modelo GPT-2 (seleccionada)}{177}{section*.39}%
\contentsline {section}{\numberline {13.12}¿Qué tipos de cosas aprende el preentrenamiento?}{177}{section.13.12}%
\contentsline {section}{\numberline {13.13}Cambio de fase: GPT-3 (2020)}{178}{section.13.13}%
\contentsline {section}{\numberline {13.14}Aprendizaje sin ejemplos, con un solo ejemplo y con pocos ejemplos con GPT-3}{179}{section.13.14}%
\contentsline {section}{\numberline {13.15}Chain-of-thought Prompting}{180}{section.13.15}%
\contentsline {section}{\numberline {13.16}Modelos de Lenguaje como Asistentes de Usuario (o Chatbots)}{180}{section.13.16}%
\contentsline {subsection}{\numberline {13.16.1}LaMDA: Modelos de Lenguaje para Aplicaciones de Diálogo}{181}{subsection.13.16.1}%
\contentsline {paragraph}{Criterios de Optimización de LaMDA}{182}{section*.40}%
\contentsline {subsection}{\numberline {13.16.2}ChatGPT y RLHF}{182}{subsection.13.16.2}%
\contentsline {subsection}{\numberline {13.16.3}GPT-4 (2023)}{183}{subsection.13.16.3}%
\contentsline {section}{\numberline {13.17}Ajuste Fino de Instrucciones}{184}{section.13.17}%
\contentsline {section}{\numberline {13.18}Línea de tiempo de los Modelos de Lenguaje Grandes}{185}{section.13.18}%
\contentsline {section}{\numberline {13.19}Prompt Engineering}{185}{section.13.19}%
\contentsline {section}{\numberline {13.20}Peligros de los Grandes Modelos de Lenguaje}{186}{section.13.20}%
\contentsline {section}{\numberline {13.21}Conclusiones}{187}{section.13.21}%
\contentsfinish 
