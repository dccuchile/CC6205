\babel@toc {spanish}{}\relax 
\contentsline {chapter}{Prefacio}{11}{chapter*.2}%
\contentsline {chapter}{\numberline {1}Introducción}{13}{chapter.1}%
\ttl@starttoc {default@1}
\contentsline {section}{\numberline {1.1}Desafíos del Procesamiento del Lenguaje Natural (PLN)}{13}{section.1.1}%
\contentsline {paragraph}{Ambigüedad}{13}{section*.3}%
\contentsline {paragraph}{Dinamismo}{14}{section*.4}%
\contentsline {paragraph}{Discretitud}{14}{section*.5}%
\contentsline {paragraph}{Composicionalidad}{14}{section*.6}%
\contentsline {paragraph}{Dispersión (sparseness)}{14}{section*.7}%
\contentsline {section}{\numberline {1.2}PLN y Lingüística Computacional}{14}{section.1.2}%
\contentsline {section}{\numberline {1.3}Tareas en Procesamiento del Lenguaje Natural (PLN)}{15}{section.1.3}%
\contentsline {section}{\numberline {1.4}Niveles de descripción lingüística}{16}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Fonética}{16}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Fonología}{16}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Morfología}{16}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Sintaxis}{17}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}Semántica}{17}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}Pragmática}{18}{subsection.1.4.6}%
\contentsline {section}{\numberline {1.5}Aprendizaje Automático en PLN}{18}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Ejemplo 1: Clasificación de Tópicos}{18}{subsection.1.5.1}%
\contentsline {subsection}{\numberline {1.5.2}Ejemplo 2: Análisis de Sentimiento}{19}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Lingüística y Procesamiento del Lenguaje Natural (PNL)}{19}{subsection.1.5.3}%
\contentsline {subsection}{\numberline {1.5.4}Limitaciones del Aprendizaje Supervisado}{20}{subsection.1.5.4}%
\contentsline {section}{\numberline {1.6}Etiquetado de Datos en PLN}{21}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Supervisión a Distancia}{22}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}Crowdsourcing}{22}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Paradigmas de Aprendizaje Automático}{22}{section.1.7}%
\contentsline {paragraph}{Aprendizaje Profundo y Conceptos Lingüísticos}{24}{section*.8}%
\contentsline {section}{\numberline {1.8}Historia de PLN}{24}{section.1.8}%
\contentsline {section}{\numberline {1.9}Conclusiones y Estructura del Apunte}{25}{section.1.9}%
\contentsline {chapter}{\numberline {2}Modelo de Espacio Vectorial}{27}{chapter.2}%
\ttl@stoptoc {default@1}
\ttl@starttoc {default@2}
\contentsline {section}{\numberline {2.1}Tokens y Tipos}{27}{section.2.1}%
\contentsline {paragraph}{Ejemplo}{27}{section*.9}%
\contentsline {subsection}{\numberline {2.1.1}Tipos}{27}{subsection.2.1.1}%
\contentsline {paragraph}{Extracción de Vocabulario}{28}{section*.10}%
\contentsline {section}{\numberline {2.2}Eliminación de Stopwords}{28}{section.2.2}%
\contentsline {section}{\numberline {2.3}Stemming}{28}{section.2.3}%
\contentsline {section}{\numberline {2.4}Lematización}{29}{section.2.4}%
\contentsline {section}{\numberline {2.5}Ley de Zipf}{29}{section.2.5}%
\contentsline {section}{\numberline {2.6}Listas de posteo y el índice invertido}{30}{section.2.6}%
\contentsline {section}{\numberline {2.7}Motores de búsqueda web}{30}{section.2.7}%
\contentsline {section}{\numberline {2.8}El modelo de espacio vectorial}{31}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}El modelo TF-IDF}{32}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Similitud entre vectores}{32}{subsection.2.8.2}%
\contentsline {paragraph}{Ejercicio}{33}{section*.11}%
\contentsline {section}{\numberline {2.9}Clustering de Documentos}{33}{section.2.9}%
\contentsline {section}{\numberline {2.10}Modelos de Tópicos}{34}{section.2.10}%
\contentsline {section}{\numberline {2.11}Conclusiones y Conceptos Adicionales}{36}{section.2.11}%
\contentsline {chapter}{\numberline {3}Modelos de Lenguaje Probabilísticos}{37}{chapter.3}%
\ttl@stoptoc {default@2}
\ttl@starttoc {default@3}
\contentsline {section}{\numberline {3.1}El Problema del Modelado del Lenguaje}{37}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}¿Por qué querríamos hacer esto?}{38}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2}Regla de la Cadena en Modelos de Lenguaje}{38}{section.3.2}%
\contentsline {section}{\numberline {3.3}Mirada Predictiva}{39}{section.3.3}%
\contentsline {section}{\numberline {3.4}Mirada Generativa}{40}{section.3.4}%
\contentsline {section}{\numberline {3.5}Un Método Ingenuo}{40}{section.3.5}%
\contentsline {section}{\numberline {3.6}Procesos de Markov}{41}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Modelado de secuencias de longitud variable}{41}{subsection.3.6.1}%
\contentsline {section}{\numberline {3.7}Modelos de lenguaje de Trigramas}{41}{section.3.7}%
\contentsline {section}{\numberline {3.8}Evaluación de un modelo de lenguaje: Perplejidad}{43}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Intuición sobre la perplejidad}{43}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}El trade-off entre sesgo y varianza}{44}{subsection.3.8.2}%
\contentsline {section}{\numberline {3.9}Interpolación Lineal}{45}{section.3.9}%
\contentsline {section}{\numberline {3.10}Estimación de los Valores $\lambda $}{45}{section.3.10}%
\contentsline {section}{\numberline {3.11}Métodos de Descuento}{46}{section.3.11}%
\contentsline {subsection}{\numberline {3.11.1}Modelos de Katz Back-Off (Bigramas)}{47}{subsection.3.11.1}%
\contentsline {section}{\numberline {3.12}Historia}{48}{section.3.12}%
\contentsline {section}{\numberline {3.13}Conclusiones}{49}{section.3.13}%
\contentsline {chapter}{\numberline {4}Modelo Naïve Bayes para Clasificación de Texto}{51}{chapter.4}%
\ttl@stoptoc {default@3}
\ttl@starttoc {default@4}
\contentsline {paragraph}{¿Por qué el análisis de sentimientos?}{52}{section*.12}%
\contentsline {paragraph}{Clasificación básica de sentimientos}{52}{section*.13}%
\contentsline {paragraph}{Resumen: Clasificación de texto}{53}{section*.14}%
\contentsline {section}{\numberline {4.1}Clasificación de texto: Definición}{53}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Métodos de clasificación: Reglas codificadas a mano}{53}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Métodos de clasificación: Aprendizaje automático supervisado}{53}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Problemas de aprendizaje supervisado}{53}{subsection.4.1.3}%
\contentsline {subsection}{\numberline {4.1.4}Modelos generativos}{54}{subsection.4.1.4}%
\contentsline {subsection}{\numberline {4.1.5}Clasificación con Modelos Generativos}{54}{subsection.4.1.5}%
\contentsline {section}{\numberline {4.2}Intuición del Bayes Ingenuo}{54}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Aplicación de la Regla de Bayes a Documentos y Clases}{54}{subsection.4.2.1}%
\contentsline {section}{\numberline {4.3}Clasificador Bayes Ingenuo}{54}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Suposiciones de Independencia del Bayes Ingenuo Multinomial}{55}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Clasificador Bayes Ingenuo Multinomial}{55}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {4.3.3}Aplicación de los clasificadores Naive Bayes multinomiales a la clasificación de texto}{56}{subsection.4.3.3}%
\contentsline {subsection}{\numberline {4.3.4}Problemas al multiplicar muchas probabilidades}{56}{subsection.4.3.4}%
\contentsline {subsection}{\numberline {4.3.5}Aprendizaje del modelo Naive Bayes multinomial}{56}{subsection.4.3.5}%
\contentsline {subsection}{\numberline {4.3.6}Estimación de parámetros}{57}{subsection.4.3.6}%
\contentsline {subsection}{\numberline {4.3.7}Probabilidades cero y el problema de las palabras no vistas}{57}{subsection.4.3.7}%
\contentsline {subsection}{\numberline {4.3.8}Suavizado Laplaciano (Add-1) para Naïve Bayes}{57}{subsection.4.3.8}%
\contentsline {subsection}{\numberline {4.3.9}Naïve Bayes multinomial: aprendizaje}{58}{subsection.4.3.9}%
\contentsline {subsection}{\numberline {4.3.10}Palabras desconocidas}{58}{subsection.4.3.10}%
\contentsline {section}{\numberline {4.4}Ejemplo}{58}{section.4.4}%
\contentsline {section}{\numberline {4.5}Naive Bayes como modelo de lenguaje}{59}{section.4.5}%
\contentsline {section}{\numberline {4.6}Evaluación}{60}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}La Matriz de Confusión 2x2}{60}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Evaluación: Exactitud}{60}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}Evaluación: Precisión y Recall}{61}{subsection.4.6.3}%
\contentsline {subsection}{\numberline {4.6.4}¿Por qué Precisión y Recall?}{61}{subsection.4.6.4}%
\contentsline {subsection}{\numberline {4.6.5}Una Medida Combinada: Medida F}{61}{subsection.4.6.5}%
\contentsline {subsection}{\numberline {4.6.6}Conjuntos de Prueba de Desarrollo ("Devsets")}{62}{subsection.4.6.6}%
\contentsline {subsection}{\numberline {4.6.7}Validación Cruzada: Múltiples Divisiones}{62}{subsection.4.6.7}%
\contentsline {subsection}{\numberline {4.6.8}Matriz de Confusión para clasificación de 3 clases}{63}{subsection.4.6.8}%
\contentsline {chapter}{\numberline {5}Modelos Lineales}{65}{chapter.5}%
\ttl@stoptoc {default@4}
\ttl@starttoc {default@5}
\contentsline {section}{\numberline {5.1}Ejemplo: Detección de Idiomas}{65}{section.5.1}%
\contentsline {section}{\numberline {5.2}Clasificación binaria log-lineal}{67}{section.5.2}%
\contentsline {section}{\numberline {5.3}Clasificación multiclase}{68}{section.5.3}%
\contentsline {section}{\numberline {5.4}Representaciones}{68}{section.5.4}%
\contentsline {section}{\numberline {5.5}Representación de Vectores One-Hot}{69}{section.5.5}%
\contentsline {section}{\numberline {5.6}Entrenamiento}{70}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Optimización basada en Gradiente}{71}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Descenso de Gradiente Estocástico en Línea}{71}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Descenso de Gradiente Estocástico en Mini-batch}{72}{subsection.5.6.3}%
\contentsline {subsection}{\numberline {5.6.4}Funciones de Pérdida}{73}{subsection.5.6.4}%
\contentsline {section}{\numberline {5.7}Regularización}{74}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Regularización L$_2$}{74}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Regularización L$_1$}{75}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Elastic-Net}{75}{subsection.5.7.3}%
\contentsline {section}{\numberline {5.8}Más allá del SGD}{75}{section.5.8}%
\contentsline {section}{\numberline {5.9}Conjuntos de entrenamiento, prueba y validación}{75}{section.5.9}%
\contentsline {section}{\numberline {5.10}Una limitación de los modelos lineales: el problema XOR}{76}{section.5.10}%
\contentsline {subsection}{\numberline {5.10.1}Transformaciones no lineales de las entradas}{77}{subsection.5.10.1}%
\contentsline {chapter}{\numberline {6}Redes Neuronales}{79}{chapter.6}%
\ttl@stoptoc {default@5}
\ttl@starttoc {default@6}
\contentsline {section}{\numberline {6.1}Redes neuronales feedforward}{79}{section.6.1}%
\contentsline {paragraph}{Capas completamente conectadas como multiplicaciones de vectores y matrices}{80}{section*.15}%
\contentsline {subsection}{\numberline {6.1.1}Redes neuronales como funciones matemáticas}{80}{subsection.6.1.1}%
\contentsline {section}{\numberline {6.2}Capacidad de representación}{81}{section.6.2}%
\contentsline {section}{\numberline {6.3}Funciones de activación}{81}{section.6.3}%
\contentsline {paragraph}{Sigmoide}{81}{section*.16}%
\contentsline {paragraph}{Tangente hiperbólica (tanh)}{82}{section*.17}%
\contentsline {paragraph}{Hard tanh}{82}{section*.18}%
\contentsline {paragraph}{ReLU}{83}{section*.19}%
\contentsline {paragraph}{Leaky ReLU}{83}{section*.20}%
\contentsline {paragraph}{ELU}{83}{section*.21}%
\contentsline {subsection}{\numberline {6.3.1}Problemas Prácticos}{83}{subsection.6.3.1}%
\contentsline {section}{\numberline {6.4}Capas de Embedding}{83}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Vectores Densos vs Representaciones One-hot}{84}{subsection.6.4.1}%
\contentsline {paragraph}{Ejemplo: Vectores Densos vs Representaciones One-hot}{85}{section*.22}%
\contentsline {section}{\numberline {6.5}Entrenamiento de Redes Neuronales}{86}{section.6.5}%
\contentsline {section}{\numberline {6.6}Recordatorio de la Regla de la Cadena en Derivadas}{86}{section.6.6}%
\contentsline {section}{\numberline {6.7}Retropropagación}{87}{section.6.7}%
\contentsline {section}{\numberline {6.8}La Abstracción del Grafo de Cómputo}{90}{section.6.8}%
\contentsline {subsection}{\numberline {6.8.1}Cómputo hacia Adelante}{91}{subsection.6.8.1}%
\contentsline {subsection}{\numberline {6.8.2}Cómputo hacia Atrás (Retropropagación)}{92}{subsection.6.8.2}%
\contentsline {subsection}{\numberline {6.8.3}Resumen de la Abstracción del Grafo de Cómputo}{92}{subsection.6.8.3}%
\contentsline {subsection}{\numberline {6.8.4}Derivadas de funciones no matemáticas}{93}{subsection.6.8.4}%
\contentsline {section}{\numberline {6.9}Regularización y Dropout}{93}{section.6.9}%
\contentsline {section}{\numberline {6.10}Frameworks de Aprendizaje Profundo}{94}{section.6.10}%
\contentsline {chapter}{\numberline {7}Vectores de Palabra}{95}{chapter.7}%
\ttl@stoptoc {default@6}
\ttl@starttoc {default@7}
\contentsline {section}{\numberline {7.1}Hipótesis Distribucional y Matrices Palabra Contexto}{95}{section.7.1}%
\contentsline {section}{\numberline {7.2}PPMI}{96}{section.7.2}%
\contentsline {section}{\numberline {7.3}Word2Vec}{97}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Parametrización del modelo Skip-gram}{98}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}Skip-gram con Negative Sampling}{99}{subsection.7.3.2}%
\contentsline {subsection}{\numberline {7.3.3}Continuous Bag of Words: CBOW}{100}{subsection.7.3.3}%
\contentsline {subsection}{\numberline {7.3.4}GloVe}{101}{subsection.7.3.4}%
\contentsline {section}{\numberline {7.4}Analogías de palabras}{101}{section.7.4}%
\contentsline {section}{\numberline {7.5}Evaluación}{101}{section.7.5}%
\contentsline {section}{\numberline {7.6}Correspondencia entre modelos distribuidos y distribucionales}{102}{section.7.6}%
\contentsline {section}{\numberline {7.7}FastText}{102}{section.7.7}%
\contentsline {section}{\numberline {7.8}Embbedings de frases específicas de sentimiento}{102}{section.7.8}%
\contentsline {section}{\numberline {7.9}Gensim}{103}{section.7.9}%
\contentsline {chapter}{\numberline {8}Etiquetado de Secuencias}{105}{chapter.8}%
\ttl@stoptoc {default@7}
\ttl@starttoc {default@8}
\contentsline {section}{\numberline {8.1}Tareas de Etiquetado de Secuencias o Etiquetado}{105}{section.8.1}%
\contentsline {section}{\numberline {8.2}Etiquetado de Partes del Discurso}{105}{section.8.2}%
\contentsline {section}{\numberline {8.3}Reconocimiento de Entidades Nombradas (NER)}{106}{section.8.3}%
\contentsline {paragraph}{Etiquetado BIO: NER como Etiquetado de Secuencias}{106}{section*.23}%
\contentsline {subsection}{\numberline {8.3.1}Etiquetado de secuencias como aprendizaje supervisado}{107}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Enfoque generativo para el etiquetado de secuencias}{107}{subsection.8.3.2}%
\contentsline {section}{\numberline {8.4}Modelos Ocultos de Markov}{107}{section.8.4}%
\contentsline {section}{\numberline {8.5}Modelos Ocultos de Markov Trigramas (Trigram HMM)}{107}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Parámetros del modelo}{108}{subsection.8.5.1}%
\contentsline {paragraph}{Un ejemplo}{108}{section*.24}%
\contentsline {section}{\numberline {8.6}Supuestos de independencia en los HMM trigramas}{108}{section.8.6}%
\contentsline {section}{\numberline {8.7}¿Por qué el nombre?}{109}{section.8.7}%
\contentsline {section}{\numberline {8.8}Estimación Suavizada}{109}{section.8.8}%
\contentsline {section}{\numberline {8.9}Tratando con Palabras de Baja Frecuencia}{110}{section.8.9}%
\contentsline {section}{\numberline {8.10}Problema de Decodificación}{110}{section.8.10}%
\contentsline {subsection}{\numberline {8.10.1}Método Bruto Ingenuo}{111}{subsection.8.10.1}%
\contentsline {section}{\numberline {8.11}Decodificación de Viterbi con Programación Dinámica}{111}{section.8.11}%
\contentsline {paragraph}{Factorial}{111}{section*.25}%
\contentsline {paragraph}{Fibonacci}{112}{section*.26}%
\contentsline {paragraph}{Complejidad}{112}{section*.27}%
\contentsline {section}{\numberline {8.12}El Algoritmo de Viterbi}{112}{section.8.12}%
\contentsline {paragraph}{Un Ejemplo}{113}{section*.28}%
\contentsline {subsection}{\numberline {8.12.1}Una Definición Recursiva}{113}{subsection.8.12.1}%
\contentsline {paragraph}{Justificación de la Definición Recursiva}{113}{section*.29}%
\contentsline {subsection}{\numberline {8.12.2}El Algoritmo de Viterbi}{114}{subsection.8.12.2}%
\contentsline {subsection}{\numberline {8.12.3}El Algoritmo de Viterbi con Punteros de Retroceso}{114}{subsection.8.12.3}%
\contentsline {subsection}{\numberline {8.12.4}El Algoritmo de Viterbi: Tiempo de Ejecución}{115}{subsection.8.12.4}%
\contentsline {paragraph}{Ventajas y Desventajas}{115}{section*.30}%
\contentsline {section}{\numberline {8.13}MEMMs}{115}{section.8.13}%
\contentsline {section}{\numberline {8.14}Ejemplo de características utilizadas en el etiquetado de partes del habla}{117}{section.8.14}%
\contentsline {section}{\numberline {8.15}Plantillas de características}{117}{section.8.15}%
\contentsline {paragraph}{Ejemplo}{117}{section*.31}%
\contentsline {section}{\numberline {8.16}MEMMs y Softmax Multiclase}{117}{section.8.16}%
\contentsline {section}{\numberline {8.17}Entrenamiento de los MEMMs}{118}{section.8.17}%
\contentsline {section}{\numberline {8.18}Decodificación con MEMMs}{118}{section.8.18}%
\contentsline {section}{\numberline {8.19}Comparación entre MEMMs y HMMs}{120}{section.8.19}%
\contentsline {section}{\numberline {8.20}Campos Aleatorios Condicionales (CRFs)}{120}{section.8.20}%
\contentsline {paragraph}{Ejemplo}{121}{section*.32}%
\contentsline {section}{\numberline {8.21}Decodificación con CRFs}{122}{section.8.21}%
\contentsline {section}{\numberline {8.22}Estimación de Parámetros en CRFs (Entrenamiento)}{123}{section.8.22}%
\contentsline {section}{\numberline {8.23}CRFs y MEMMs}{123}{section.8.23}%
\contentsline {subsection}{\numberline {8.23.1}CRFs y MEMMs: el problema del sesgo de etiqueta}{123}{subsection.8.23.1}%
\contentsline {section}{\numberline {8.24}Enlaces}{124}{section.8.24}%
\contentsline {chapter}{\numberline {9}Redes Neuronales Convolucionales}{125}{chapter.9}%
\ttl@stoptoc {default@8}
\ttl@starttoc {default@9}
\contentsline {section}{\numberline {9.1}Redes Neuronales Convolucionales (CNN) en Procesamiento del Lenguaje Natural (PLN)}{125}{section.9.1}%
\contentsline {section}{\numberline {9.2}Convolución Básica + Agrupamiento}{125}{section.9.2}%
\contentsline {section}{\numberline {9.3}Convoluciones 1D sobre Texto}{126}{section.9.3}%
\contentsline {section}{\numberline {9.4}Convoluciones Angostas vs. Amplias}{126}{section.9.4}%
\contentsline {section}{\numberline {9.5}Agrupamiento Vectorial}{127}{section.9.5}%
\contentsline {section}{\numberline {9.6}Clasificación de Sentimientos en Twitter con CNN}{127}{section.9.6}%
\contentsline {section}{\numberline {9.7}Redes Neuronales Convolucionales Muy Profundas para la Clasificación de Texto}{127}{section.9.7}%
\contentsline {chapter}{\numberline {10}Redes Neuronales Recurrentes}{129}{chapter.10}%
\ttl@stoptoc {default@9}
\ttl@starttoc {default@10}
\contentsline {section}{\numberline {10.1}La Abstracción de las RNN}{129}{section.10.1}%
\contentsline {section}{\numberline {10.2}Red Elman o Simple-RNN}{131}{section.10.2}%
\contentsline {section}{\numberline {10.3}Entrenamiento de las RNN}{131}{section.10.3}%
\contentsline {section}{\numberline {10.4}Patrones de uso de las RNN: Aceptador}{132}{section.10.4}%
\contentsline {section}{\numberline {10.5}Patrones de uso de las RNN: Transductor}{133}{section.10.5}%
\contentsline {section}{\numberline {10.6}Redes neuronales recurrentes bidireccionales (BIRNN)}{133}{section.10.6}%
\contentsline {section}{\numberline {10.7}Redes neuronales recurrentes multi-capa (apiladas)}{134}{section.10.7}%
\contentsline {section}{\numberline {10.8}Arquitecturas con compuertas}{135}{section.10.8}%
\contentsline {section}{\numberline {10.9}GRU}{138}{section.10.9}%
\contentsline {section}{\numberline {10.10}Clasificación de sentimientos con RNN}{139}{section.10.10}%
\contentsline {section}{\numberline {10.11}Clasificación de sentimientos en Twitter con LSTMS y Emojis}{139}{section.10.11}%
\contentsline {section}{\numberline {10.12}Bi-LSTM CRF}{140}{section.10.12}%
\contentsline {chapter}{\numberline {11}Modelos Secuencia a Secuencia y Atención Neuronal}{143}{chapter.11}%
\ttl@stoptoc {default@10}
\ttl@starttoc {default@11}
\contentsline {section}{\numberline {11.1}Modelos de lenguaje y generación de lenguaje}{143}{section.11.1}%
\contentsline {section}{\numberline {11.2}Problemas de secuencia a secuencia}{144}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Generación condicionada}{144}{subsection.11.2.1}%
\contentsline {paragraph}{Gráfico de entrenamiento de secuencia a secuencia}{144}{section*.33}%
\contentsline {paragraph}{Traducción automática neuronal}{144}{section*.34}%
\contentsline {paragraph}{Progreso de BLEU en traducción automática}{144}{section*.35}%
\contentsline {section}{\numberline {11.3}Enfoques de decodificación}{145}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Búsqueda Beam}{145}{subsection.11.3.1}%
\contentsline {section}{\numberline {11.4}Generación condicionada con atención}{146}{section.11.4}%
\contentsline {section}{\numberline {11.5}Atención y alineaciones de palabras}{149}{section.11.5}%
\contentsline {section}{\numberline {11.6}Otros tipos de atención}{149}{section.11.6}%
\contentsline {chapter}{\numberline {12}Arquitectura de Transformer}{151}{chapter.12}%
\ttl@stoptoc {default@11}
\ttl@starttoc {default@12}
\contentsline {subsection}{\numberline {12.0.1}Dependencias en la traducción automática neuronal}{151}{subsection.12.0.1}%
\contentsline {section}{\numberline {12.1}El Transformer}{151}{section.12.1}%
\contentsline {section}{\numberline {12.2}Mecanismo de atención en el Transformer}{153}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Consultas}{153}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Claves}{154}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Valores}{155}{subsection.12.2.3}%
\contentsline {subsection}{\numberline {12.2.4}Atención de producto puntual escalado}{155}{subsection.12.2.4}%
\contentsline {section}{\numberline {12.3}El Codificador}{155}{section.12.3}%
\contentsline {paragraph}{Introduciendo los tensores en la imagen}{156}{section*.36}%
\contentsline {section}{\numberline {12.4}Autoatención a alto nivel}{157}{section.12.4}%
\contentsline {section}{\numberline {12.5}Autoatención en detalle}{157}{section.12.5}%
\contentsline {paragraph}{Paso 1}{157}{section*.37}%
\contentsline {paragraph}{Paso 2}{158}{section*.38}%
\contentsline {paragraph}{Pasos 3 y 4}{159}{section*.39}%
\contentsline {paragraph}{Autoatención en detalle: pasos 5 y 6}{159}{section*.40}%
\contentsline {section}{\numberline {12.6}Cálculo matricial de la autoatención}{160}{section.12.6}%
\contentsline {section}{\numberline {12.7}Atención multi-head}{161}{section.12.7}%
\contentsline {section}{\numberline {12.8}Conexiones residuales}{162}{section.12.8}%
\contentsline {section}{\numberline {12.9}El codificador: resumen}{163}{section.12.9}%
\contentsline {section}{\numberline {12.10}El Decodificador}{164}{section.12.10}%
\contentsline {section}{\numberline {12.11}La Capa Lineal Final y el Entrenamiento}{165}{section.12.11}%
\contentsline {section}{\numberline {12.12}Codificaciones posicionales}{166}{section.12.12}%
\contentsline {section}{\numberline {12.13}Conclusiones}{167}{section.12.13}%
\contentsline {chapter}{\numberline {13}Grandes Modelos de Lenguaje}{169}{chapter.13}%
\ttl@stoptoc {default@12}
\ttl@starttoc {default@13}
\contentsline {section}{\numberline {13.1}Representaciones para una palabra}{169}{section.13.1}%
\contentsline {section}{\numberline {13.2}Los Modelos de Lenguaje Neurales pueden producir Incrustaciones Contextualizadas}{169}{section.13.2}%
\contentsline {section}{\numberline {13.3}ELMo: Incrustaciones de Modelos de Lenguaje}{169}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}ELMo: Uso con una tarea}{170}{subsection.13.3.1}%
\contentsline {paragraph}{ELMo: Resultados}{170}{section*.41}%
\contentsline {section}{\numberline {13.4}ULMfit}{170}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Énfasis de ULMfit}{171}{subsection.13.4.1}%
\contentsline {subsection}{\numberline {13.4.2}Transferencia de aprendizaje con ULMfit}{171}{subsection.13.4.2}%
\contentsline {section}{\numberline {13.5}¡Aumentemos la escala!}{171}{section.13.5}%
\contentsline {section}{\numberline {13.6}BERT (Bidirectional Encoder Representations from Transformers)}{171}{section.13.6}%
\contentsline {section}{\numberline {13.7}Modelado de Lenguaje Mascarado y Predicción de la Siguiente Oración}{172}{section.13.7}%
\contentsline {section}{\numberline {13.8}Codificación de pares de oraciones en BERT}{172}{section.13.8}%
\contentsline {section}{\numberline {13.9}Arquitectura y entrenamiento del modelo BERT}{173}{section.13.9}%
\contentsline {section}{\numberline {13.10}Ajuste fino del modelo BERT}{173}{section.13.10}%
\contentsline {subsection}{\numberline {13.10.1}Resultados de BERT en tareas GLUE}{173}{subsection.13.10.1}%
\contentsline {paragraph}{Tareas de GLUE}{174}{section*.42}%
\contentsline {subsection}{\numberline {13.10.2}Efecto de la tarea de preentrenamiento en BERT}{174}{subsection.13.10.2}%
\contentsline {section}{\numberline {13.11}Decodificadores de preentrenamiento GPT y GPT-2}{174}{section.13.11}%
\contentsline {paragraph}{Salida del modelo GPT-2 (seleccionada)}{175}{section*.43}%
\contentsline {section}{\numberline {13.12}¿Qué tipos de cosas aprende el preentrenamiento?}{176}{section.13.12}%
\contentsline {section}{\numberline {13.13}Cambio de fase: GPT-3 (2020)}{177}{section.13.13}%
\contentsline {section}{\numberline {13.14}Aprendizaje sin ejemplos, con un solo ejemplo y con pocos ejemplos con GPT-3}{177}{section.13.14}%
\contentsline {section}{\numberline {13.15}Chain-of-thought Prompting}{178}{section.13.15}%
\contentsline {section}{\numberline {13.16}Modelos de Lenguaje como Asistentes de Usuario (o Chatbots)}{178}{section.13.16}%
\contentsline {subsection}{\numberline {13.16.1}LaMDA: Modelos de Lenguaje para Aplicaciones de Diálogo}{179}{subsection.13.16.1}%
\contentsline {paragraph}{Criterios de Optimización de LaMDA}{180}{section*.44}%
\contentsline {subsection}{\numberline {13.16.2}ChatGPT y RLHF}{180}{subsection.13.16.2}%
\contentsline {subsection}{\numberline {13.16.3}GPT-4 (2023)}{181}{subsection.13.16.3}%
\contentsline {section}{\numberline {13.17}Ajuste Fino de Instrucciones}{182}{section.13.17}%
\contentsline {section}{\numberline {13.18}Línea de tiempo de los Modelos de Lenguaje Grandes}{183}{section.13.18}%
\contentsline {section}{\numberline {13.19}Prompt Engineering}{183}{section.13.19}%
\contentsline {section}{\numberline {13.20}Peligros de los Grandes Modelos de Lenguaje}{184}{section.13.20}%
\contentsline {section}{\numberline {13.21}Conclusiones}{185}{section.13.21}%
\contentsfinish 
