%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  My documentation report
%  Objetive: Explain what I did and how, so someone can continue with the investigation
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations


\usepackage[top=3cm,bottom=3cm,left=3.2cm,right=3.2cm,headsep=10pt,letterpaper]{geometry} % Page margins

\usepackage{xcolor} % Required for specifying colors by name
\definecolor{ocre}{RGB}{52,177,201} % Define the orange color used for highlighting throughout the book

% Font Settings
\usepackage{avant} % Use the Avantgarde font for headings
%\usepackage{times} % Use the Times font for headings
\usepackage{mathptmx} % Use the Adobe Times Roman as the default text font together with math symbols from the Sym­bol, Chancery and Com­puter Modern fonts
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{amsthm}

% Bibliography
%\usepackage[style=alphabetic,sorting=nyt,sortcites=true,autopunct=true,babel=hyphen,hyperref=true,abbreviate=false,backref=true,backend=biber]{biblatex}
%\addbibresource{bio.bib} % BibTeX bibliography file
%\defbibheading{bibempty}{}

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

%----------------------------------------------------------------------------------------
%	Definitions of new commands
%----------------------------------------------------------------------------------------

\def\R{\mathbb{R}}
\newcommand{\cvx}{convex}
\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\AddToShipoutPicture*{\put(0,0){\includegraphics[scale=1.25]{portada5}}} % Image background
\centering
\vspace*{5cm}
\par\normalfont\fontsize{35}{35}\sffamily\selectfont
\textbf{\textcolor{white}{Procesamiento de Lenguaje Natural}}\\
{\LARGE \textcolor{white}{Apunte de Clases (Borrador)}}\par % Book title
\vspace*{1cm}
{\Huge \textcolor{white}{Felipe Bravo Márquez}}\par % Author name
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Felipe Bravo Márquez\\

\noindent Ilustración Portada por Paulette Filla\\

\noindent \textsc{Departamento de Ciencias de la Computación, Universidad de Chile}\\

\noindent \textsc{github.com/dccuchile/CC6205}\\ % URL

\noindent Apuntes de clases del curso de Procesamiento de Lenguaje Natural de la Universidad de Chile.\\ % License information

\noindent El formato del apunte fue tomado del template de Jasmine Hao. \\

\noindent \textit{Borrador, \today} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{head3.png} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

%\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again




\section{Evaluación}

La evaluación juega un papel crucial en la construcción de sistemas de PLN basados en aprendizaje automático. Al construir estos sistemas, es común llevar a cabo comparaciones y experimentos con diversos modelos durante el proceso de entrenamiento, con el fin de elegir el modelo más idóneo para su posterior implementación en producción. Generalmente, adoptamos un enfoque cuantitativo para evaluar modelos, contrastando las predicciones realizadas con las predicciones esperadas, y resumiendo estos resultados mediante métricas de evaluación. Este enfoque cuantitativo se prefiere debido a que un enfoque cualitativo o de inspección manual no resulta escalable. Ilustremos este proceso de evaluación con un ejemplo.


\begin{example}
Supongamos un problema de clasificación binaria, para la cual queremos comparar dos modelos de clasificación $m_1$ y $m_2$. Probamos los modelos sobre un conjunto de datos de prueba y obtenemos las predicciones señaladas en la Tabla~\ref{tab:ej_sentimiento}.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Texto} & \textbf{Clase} & \textbf{m1} & \textbf{m2} \\
\hline
no estoy bien & negativo & negativo & positivo \\
al fin! & positivo & positivo & negativo \\
que pena & negativo & negativo & negativo \\
feliz & positivo & positivo & positivo \\
no quiero & negativo & positivo & negativo \\
jamás! & negativo & negativo & negativo \\
buena idea & positivo & positivo & negativo \\
chao contigo & negativo & positivo & negativo \\
\hline
\end{tabular}
\caption{Sentimiento de texto y predicciones realizadas por dos modelos distintos.}
\label{tab:ej_sentimiento}
\end{table}

Se puede observar que tanto el modelo $m_1$ como el modelo $m_2$ presentan errores en sus predicciones al compararse con las categorías reales de los ejemplos (columna clase), también conocidas como ``gold labels'' o ``ground truth''. Sin embargo, ¿cómo determinamos cuál modelo es mejor? Para abordar esta pregunta, resulta conveniente calcular métricas de desempeño, las cuales se derivan al contrastar las predicciones de un modelo con sus salidas esperadas.
\end{example}

\subsection{Matriz de Confusión}

La mayoría de las métricas de evaluación para problemas de clasificación se derivan de una matriz de confusión (véase Tabla~\ref{tab:matriz_confusion}), la cual representa una tabla de contingencia entre las predicciones de un modelo y las salidas esperadas en un conjunto de prueba. Para problemas de clasificación binaria, esta matriz consta de 4 componentes:

\begin{enumerate}
\item \textbf{Verdadero Positivo (VP)}: cuando el modelo predice correctamente la clase positiva, coincidiendo con la salida esperada.
\item \textbf{Verdadero Negativo (VN)}: cuando el modelo predice correctamente la clase negativa, coincidiendo con la salida esperada.
\item \textbf{Falso Negativo (FN)}: cuando el modelo predice incorrectamente la clase negativa, siendo la salida esperada positiva.
\item \textbf{Falso Positivo (FP)}: cuando el modelo predice incorrectamente la clase positiva, siendo la salida esperada negativa.
\end{enumerate}

Es importante destacar que la definición de qué constituye la clase positiva es arbitraria en un problema de clasificación; generalmente se refiere a la clase que se desea detectar. En el caso de la clasificación de sentimientos, es una coincidencia que la clase positiva se asocie con el sentimiento ``positivo''. Esta consideración es fundamental, ya que el valor de algunas métricas de desempeño (como precisión, recall y $F_1$) depende de la elección de la clase positiva.

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{} & \textbf{Modelo Positivo} & \textbf{Modelo Negativo} \\
\hline
\textbf{Real Positivo} & Verdadero Positivo (VP) & Falso Negativo (FN) \\
\hline
\textbf{Real Negativo} & Falso Positivo (FP) & Verdadero Negativo (VN) \\
\hline
\end{tabular}
\caption{Matriz de confusión para un problema de clasificación binaria.}
\label{tab:matriz_confusion}
\end{table}

\begin{example}
Veamos como serían las matrices de confusión para los modelos $m_1$ y $m_2$ del ejemplo anterior. Primero procedemos a categorizar cada predicción de ambos modelos según una de las cuatro categorías señaladas (VP,VN,FP,FN). 

\begin{table}[h]
\centering
\begin{tabular}{|l||l|l||l|l|}
\hline
\textbf{Clase esperada} & \textbf{m1} & \textbf{Resultado m1} & \textbf{m2} & \textbf{Resultado m2} \\ \hline
negativo & negativo & VN & positivo & FP \\ \hline
positivo & positivo & VP & negativo & FN \\ \hline
negativo & negativo & VN & negativo & VN \\ \hline
positivo & positivo & VP & positivo & VP \\ \hline
negativo & positivo & FP & negativo & VN \\ \hline
negativo & negativo & VN & negativo & VN \\ \hline
positivo & positivo & VP & negativo & FN \\ \hline
negativo & positivo & FP & negativo & VN \\ \hline
\end{tabular}
\caption{Tipos de predicciones de $m_1$ y $m_2$.}
\label{tab:matriz_confusion}
\end{table}

Luego, agregamos dichos conteos en una matriz de confusión para cada modelo.

\end{example}


\subsection{Métricas de Desempeño}

\textbf{Recall} (exhaustividad) (también conocido como \textbf{Sensibilidad} o \textbf{Tasa de Verdaderos Positivos}):
\[ \text{Recall} = \frac{VP}{VP + FN} \]

\textbf{Precisión}:
\[ \text{Precisión} = \frac{VP}{VP + FP} \]

\textbf{Accuracy} (exactitud):
\[ \text{Accuracy} = \frac{VP + VN}{VP + FP + VN + FN} \]


¿Por qué no usamos la exactitud como nuestra métrica?

Imagina que vimos 1 millón de tweets:
\begin{itemize}
\item 100 de ellos hablaban sobre Delicious Pie Co.
\item 999,900 hablaban de otra cosa.
\end{itemize}

Podríamos construir un clasificador tonto que simplemente etiquete todos los tweets como "no sobre pasteles":
\begin{itemize}
\item ¡¡¡Obtendría una exactitud del 99.99\%!!! ¡¡¡Wow!!!
\item ¡Pero sería inútil! ¡No devuelve los comentarios que estamos buscando!
\end{itemize}

Por eso usamos precisión y recall en su lugar.


\textbf{Precisión} mide el porcentaje de elementos que el sistema detectó (es decir, los elementos que el sistema etiquetó como positivos) que son realmente positivos (según las etiquetas de oro humanas).

\[
\text{Precisión} = \frac{\text{Verdaderos Positivos}}{\text{Verdaderos Positivos + Falsos Positivos}}
\]

\textbf{Recall} mide el porcentaje de elementos que el sistema identificó correctamente de todos los elementos que deberían haber sido identificados.

\[
\text{Recall} = \frac{\text{Verdaderos Positivos}}{\text{Verdaderos Positivos + Falsos Negativos}}
\]


Considera nuestro clasificador tonto de pasteles que simplemente etiqueta nada como "sobre pasteles".

\begin{itemize}
  \item Exactitud = 99.99\% (etiqueta correctamente la mayoría de los tweets como no relacionados con pasteles)
  \item Recall = 0 (no detecta ninguno de los 100 tweets relacionados con pasteles)
\end{itemize}

La precisión y el recall, a diferencia de la exactitud, enfatizan los verdaderos positivos:
\begin{itemize}
  \item Se centran en encontrar las cosas que se supone que debemos buscar.
\end{itemize}

\subsection{Una Medida Combinada: Medida F}
La medida F es un número único que combina la precisión (P) y el recall (R), definida como:
\[
F_\beta = \frac{(\beta^2+1)PR}{\beta^2P + R}
\]

La medida F, definida con el parámetro $\beta$, pondera diferencialmente la importancia del recall y la precisión.
\begin{itemize}
  \item $\beta > 1$ favorece al recall
  \item $\beta < 1$ favorece a la precisión
\end{itemize}

Cuando $\beta = 1$, la precisión y el recall son iguales, y tenemos la medida $F_1$ equilibrada:
\[
F_1 = \frac{2PR}{P + R}
\]

\subsection{Conjuntos de Prueba de Desarrollo ("Devsets")}

\begin{itemize}
 \item Para evitar el sobreajuste y proporcionar una estimación más conservadora del rendimiento, comúnmente utilizamos un enfoque de tres conjuntos: conjunto de entrenamiento, conjunto de desarrollo y conjunto de prueba.
\begin{figure}[h]
\includegraphics[scale = 0.23]{pics/devsets.png}
\end{figure}

\begin{itemize}
\item \textbf{Conjunto de entrenamiento}: Se utiliza para entrenar el modelo.
\item \textbf{Conjunto de desarrollo}: Se utiliza para ajustar el modelo y seleccionar los mejores hiperparámetros.
\item \textbf{Conjunto de prueba}: Se utiliza para informar el rendimiento final del modelo.
\end{itemize}

\item Este enfoque garantiza que el modelo no esté ajustado específicamente al conjunto de prueba, evitando el sobreajuste.
\item Sin embargo, crea una paradoja: queremos la mayor cantidad de datos posible para el entrenamiento, pero también para el conjunto de desarrollo.
\item ¿Cómo dividimos los datos?

\end{itemize}





\subsection{Validación Cruzada: Múltiples Divisiones}

La validación cruzada nos permite utilizar todos nuestros datos para el entrenamiento y la prueba sin tener un conjunto de entrenamiento, conjunto de desarrollo y conjunto de prueba fijos.  Elegimos un número $k$ y dividimos nuestros datos en $k$ subconjuntos disjuntos llamados pliegues.  En cada iteración, uno de los pliegues se selecciona como conjunto de prueba mientras que los $k-1$ pliegues restantes se utilizan para entrenar el clasificador.

Calculamos la tasa de error en el conjunto de prueba y repetimos este proceso $k$ veces.  Finalmente, promediamos las tasas de error de estas $k$ ejecuciones para obtener una tasa de error promedio.

Por ejemplo, la validación cruzada de 10 pliegues implica entrenar 10 modelos con el 90\% de los datos y probar cada modelo por separado.  Las tasas de error resultantes se promedian para obtener la estimación final del rendimiento. Sin embargo, la validación cruzada requiere que todo el corpus sea ciego, lo que impide examinar los datos para sugerir características o comprender el comportamiento del sistema.  Para abordar esto, se crea un conjunto de entrenamiento y un conjunto de prueba fijos, y se realiza la validación cruzada de 10 pliegues dentro del conjunto de entrenamiento.  La tasa de error se calcula convencionalmente en el conjunto de prueba.


\begin{center}
\includegraphics[scale=0.28]{pics/cv.png}
\end{center}

\section{Conjuntos de entrenamiento, prueba y validación}
Cuando entrenamos un modelo, nuestro objetivo es producir una función $f(\vec{x})$ que mapee correctamente las entradas $\vec{x}$ a las salidas $\hat{y}$ según lo evidenciado por el conjunto de entrenamiento. La evaluación del rendimiento en los datos de entrenamiento puede ser engañosa, ya que nuestro objetivo es entrenar una función capaz de generalizar a ejemplos no vistos. Una forma común de abordar esto es dividir el conjunto de entrenamiento en subconjuntos de entrenamiento y prueba (80\% y 20\% respectivamente). Se entrena el modelo en el subconjunto de entrenamiento y se calcula la precisión en el subconjunto de prueba.

Sin embargo, este enfoque tiene una limitación. En la práctica, a menudo se entrenan varios modelos, se comparan sus calidades y se selecciona el mejor. Si se selecciona el mejor modelo en función de la precisión en el subconjunto de prueba, se obtendrá una estimación excesivamente optimista de la calidad del modelo. No se sabe si la configuración elegida del clasificador final es buena en general o simplemente es buena para los ejemplos particulares en los subconjuntos de prueba.

La metodología aceptada es utilizar una división de tres vías de los datos en conjuntos de entrenamiento, validación (también llamado desarrollo) y prueba\footnote{Un enfoque alternativo es la validación cruzada, pero no se escala bien para entrenar redes neuronales profundas.}. Esto proporciona dos conjuntos apartados: un conjunto de validación (también llamado conjunto de desarrollo) y un conjunto de prueba. Todos los experimentos, ajustes, análisis de errores y selección de modelos deben realizarse

basados en el conjunto de validación. Luego, una única ejecución del modelo final sobre el conjunto de prueba proporcionará una buena estimación de su calidad esperada en ejemplos no vistos. Es importante mantener el conjunto de prueba lo más limpio posible, realizando la menor cantidad de experimentos posible en él. Incluso algunos defienden que no se deben mirar siquiera los ejemplos en el conjunto de prueba, para evitar sesgar el diseño del modelo.

\begin{figure}[htb]
	\centering
	 \includegraphics[scale=0.55]{pics/validation.png}
\end{figure}

\footnotetext{Fuente: \url{https://www.codeproject.com/KB/AI/1146582/validation.PNG}}


\subsection{Matriz de Confusión para clasificación de 3 clases}


\begin{center}
\includegraphics[scale=0.23]{pics/confmatrix.png}
\end{center}

Cómo combinar métricas binarias (Precisión, Recall, $F_1$) de más de 2 clases para obtener una métrica única:
\begin{itemize}
 \item Macro-promedio:
 \begin{itemize}
    \item Calcular las métricas de rendimiento (Precisión, Recall, $F_1$) para cada clase individualmente.
    \item Promediar las métricas en todas las clases.
 \end{itemize}
 \item Micro-promedio:
 \begin{itemize}
    \item Recopilar las decisiones para todas las clases en una matriz de confusión.
    \item Calcular la Precisión y el Recall a partir de la matriz de confusión.
 \end{itemize}
\end{itemize}

\begin{center}
\includegraphics[scale=0.23]{pics/confmatrixmulti.png}
\end{center}



\bibliography{bio}
\bibliographystyle{apalike}

\end{document}
