{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introducci贸n\" data-toc-modified-id=\"Introducci贸n-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introducci贸n</a></span></li><li><span><a href=\"#Representaciones\" data-toc-modified-id=\"Representaciones-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Representaciones</a></span></li><li><span><a href=\"#Algoritmos\" data-toc-modified-id=\"Algoritmos-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Algoritmos</a></span></li><li><span><a href=\"#M茅tricas-de-Evaluaci贸n\" data-toc-modified-id=\"M茅tricas-de-Evaluaci贸n-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>M茅tricas de Evaluaci贸n</a></span></li><li><span><a href=\"#Experimentos\" data-toc-modified-id=\"Experimentos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Experimentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importar-librer铆as-y-utiles\" data-toc-modified-id=\"Importar-librer铆as-y-utiles-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Importar librer铆as y utiles</a></span></li><li><span><a href=\"#Definir-m茅todos-de-evaluaci贸n\" data-toc-modified-id=\"Definir-m茅todos-de-evaluaci贸n-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Definir m茅todos de evaluaci贸n</a></span></li><li><span><a href=\"#Datos\" data-toc-modified-id=\"Datos-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Datos</a></span></li><li><span><a href=\"#Analizar-los-datos\" data-toc-modified-id=\"Analizar-los-datos-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Analizar los datos</a></span></li><li><span><a href=\"#Custom-Features\" data-toc-modified-id=\"Custom-Features-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Custom Features</a></span></li><li><span><a href=\"#Definir-la-representaci贸n-y-el-clasificador\" data-toc-modified-id=\"Definir-la-representaci贸n-y-el-clasificador-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Definir la representaci贸n y el clasificador</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-para-alg煤n-dataset\" data-toc-modified-id=\"Ejecutar-el-pipeline-para-alg煤n-dataset-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Ejecutar el pipeline para alg煤n dataset</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-por-cada-train-set\" data-toc-modified-id=\"Ejecutar-el-pipeline-por-cada-train-set-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Ejecutar el pipeline por cada train set</a></span></li><li><span><a href=\"#Predecir-target-set\" data-toc-modified-id=\"Predecir-target-set-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Predecir target set</a></span></li></ul></li><li><span><a href=\"#Conclusiones\" data-toc-modified-id=\"Conclusiones-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusiones</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    }
   },
   "source": [
    "# Tarea 1 NLP\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Nombre:**\n",
    "\n",
    "- **Usuario o nombre de equipo en Codalab:** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:47.464702Z",
     "start_time": "2020-03-31T13:49:47.444379Z"
    }
   },
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    }
   },
   "source": [
    "Este es el baseline de la tarea 1. Contiene la estructura del reporte mas un c贸digo b谩sico que:\n",
    "\n",
    "- Obtiene los dataset.\n",
    "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizadas.\n",
    "    - Transforma los dataset a bag of words(BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se est谩 ejecutando el notebook. \n",
    "\n",
    "La idea es que a partir de este notebook puedan empezar la tarea. Obviamente, no es requisito usarlo, pero puede facilitarles mucho al comienzo.\n",
    "\n",
    "Para ver las instrucciones, descarga el enunciado de la tarea en ucursos. **隆Recuerda ingresar tu nombre, el de tu equipo y sus usuarios en codalab!**\n",
    "\n",
    "(Pueden eliminar esta y cualquier celda con instrucciones...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    }
   },
   "source": [
    "## Introducci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    }
   },
   "source": [
    "## Representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    }
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    }
   },
   "source": [
    "## M茅tricas de Evaluaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC: ...\n",
    "- Kappa: ...\n",
    "- Accuracy: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    }
   },
   "source": [
    "### Importar librer铆as y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.166420Z",
     "start_time": "2020-04-01T15:32:27.826255Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir m茅todos de evaluaci贸n\n",
    "\n",
    "Estas funciones est谩n a cargo de evaluar los resultados de la tarea. No deber铆an cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.186328Z",
     "start_time": "2020-04-01T15:32:29.166420Z"
    }
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "\n",
    "    # Classification Report\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    \n",
    "    # Auc\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    \n",
    "    # Kappa\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos\n",
    "\n",
    "Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.686681Z",
     "start_time": "2020-04-01T15:32:29.186328Z"
    }
   },
   "outputs": [],
   "source": [
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.726401Z",
     "start_time": "2020-04-01T15:32:29.686681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>10704</td>\n",
       "      <td>@MeghanEMurphy Oh gosh, if you get 800+ raging...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>10422</td>\n",
       "      <td>Drop Snapchat names #bored #snap #swap #pics</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>10056</td>\n",
       "      <td>Absolutely fuming that some woman jumped into ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>10291</td>\n",
       "      <td>#Anger or #wrath is an intense emotional respo...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>10474</td>\n",
       "      <td>Puzzle investing opening portland feodal popul...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>10142</td>\n",
       "      <td>i had an hour of football practice under the b...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>10612</td>\n",
       "      <td>Because you offended by the lies of Alexandria...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>10716</td>\n",
       "      <td>Praying for the #Lord to keep #anger #hate #je...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>10303</td>\n",
       "      <td>Yo there's a kid on my snap chat from LA &amp;amp;...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10047</td>\n",
       "      <td>Still can't log into my fucking Snapchat #Snap...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  class  \\\n",
       "704  10704  @MeghanEMurphy Oh gosh, if you get 800+ raging...  anger   \n",
       "422  10422       Drop Snapchat names #bored #snap #swap #pics  anger   \n",
       "56   10056  Absolutely fuming that some woman jumped into ...  anger   \n",
       "291  10291  #Anger or #wrath is an intense emotional respo...  anger   \n",
       "474  10474  Puzzle investing opening portland feodal popul...  anger   \n",
       "142  10142  i had an hour of football practice under the b...  anger   \n",
       "612  10612  Because you offended by the lies of Alexandria...  anger   \n",
       "716  10716  Praying for the #Lord to keep #anger #hate #je...  anger   \n",
       "303  10303  Yo there's a kid on my snap chat from LA &amp;...  anger   \n",
       "47   10047  Still can't log into my fucking Snapchat #Snap...  anger   \n",
       "\n",
       "    sentiment_intensity  \n",
       "704              medium  \n",
       "422              medium  \n",
       "56                 high  \n",
       "291              medium  \n",
       "474              medium  \n",
       "142                high  \n",
       "612              medium  \n",
       "716              medium  \n",
       "303              medium  \n",
       "47                 high  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias:\n",
    "train['anger'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, seg煤n su intensidad de sentimiento. Noten que las clases est谩n desbalanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.766213Z",
     "start_time": "2020-04-01T15:32:29.726401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count(),\n",
    "          '\\n---------------------------------------\\n')\n",
    "\n",
    "\n",
    "for dataset_name in train:\n",
    "    get_group_dist(dataset_name, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta car谩cteres relevantes ('!', '?', '#') en los tweets.\n",
    "2. Definios una funci贸n c贸mo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la funci贸n `transform` en donde iteramos por cada tweet, llamamos a la funci贸n que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitar谩 el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aqu铆](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.786237Z",
     "start_time": "2020-04-01T15:32:29.766213Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count('#')\n",
    "        num_exclamations = tweet.count('!')\n",
    "        num_interrogations = tweet.count('?')\n",
    "        return [num_hashtags, num_exclamations, num_interrogations]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        for tweet in X:\n",
    "            chars.append(self.get_relevant_chars(tweet))\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.806148Z",
     "start_time": "2020-04-01T15:32:29.786237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just joined #pottermore and was sorted into HU...</td>\n",
       "      <td>[2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I get so angry at people that don't know that ...</td>\n",
       "      <td>[2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kingcharles9th i Lowkey forgot you had twitte...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@canada4trumpnow @donlemon he has BAD TEMPERAM...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im so angry </td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Is it me, or is Ding wearing the look of a man...</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Petition for non-Maghrebi PoC to stop buying '...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@Idubbbz @LeafyIsHere  I am offended</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ananya just grabbed a bible, opened it, starte...</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@johnaugust @clmazin I was so looking forward ...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          1\n",
       "0  Just joined #pottermore and was sorted into HU...  [2, 0, 0]\n",
       "1  I get so angry at people that don't know that ...  [2, 0, 0]\n",
       "2  @kingcharles9th i Lowkey forgot you had twitte...  [1, 0, 0]\n",
       "3  @canada4trumpnow @donlemon he has BAD TEMPERAM...  [1, 0, 0]\n",
       "4                                     Im so angry   [0, 0, 0]\n",
       "5  Is it me, or is Ding wearing the look of a man...  [1, 0, 1]\n",
       "6  Petition for non-Maghrebi PoC to stop buying '...  [0, 0, 0]\n",
       "7               @Idubbbz @LeafyIsHere  I am offended  [0, 0, 0]\n",
       "8  Ananya just grabbed a bible, opened it, starte...  [0, 0, 1]\n",
       "9  @johnaugust @clmazin I was so looking forward ...  [1, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos que sucede si ejecutamos el transformer\n",
    "sample = train['anger'].sample(10).tweet\n",
    "pd.DataFrame(zip(sample, CharsCountTransformer().transform(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir la representaci贸n y el clasificador\n",
    "\n",
    "Para definir el flujo de los datos sobre nuestras transformaciones y el clasificador usaremos un `Pipeline`.\n",
    "\n",
    "Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el proceso que seguiran nuestros datos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando as铆 nuestra programaci贸n.\n",
    "\n",
    "El pipeline m谩s b谩sico que podemos hacer es transformar el dataset a Bag of Words y despu茅s usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "\n",
    "        Dataset -> BagOfWords (llamado CountVectorizer) -> NaiveBayes\n",
    "\n",
    "En c贸digo queda c贸mo:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformaci贸n para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenar谩 los vectores resultantes de ejecutar los Transformer en un solo vector.\n",
    "\n",
    "\n",
    "        Dataset -> FeatureUnion [CountVectorizer, CharsCountTransformer] -> NaiveBayes\n",
    "\n",
    "\n",
    "En c贸digo queda como: \n",
    "\n",
    "```python\n",
    "Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count',CharsCountTransformer())])),\n",
    "            ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas pistas sobre como mejorar el rendimiento del vectorizador y del clasificador:\n",
    "\n",
    "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. Tambi茅n, el par谩metro `ngram_range` (Ojo que el clf naive bayes no deber铆a usarse con n-gramas, ya que rompe el supuesto de independencia). Adem谩s, implementar los atributos que crean 煤tiles desde el listado del el enunciado. Investigar tambi茅n el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la funci贸n `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Pueden guiarse (pero no limitarse) por las que dice el enunciado.\n",
    "\n",
    "- **Reducci贸n de dimensionalidad**: Tambi茅n puede serles de ayuda. Referencias [aqu铆](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por 煤ltimo, pueden encontrar mas referencias de c贸mo mejorar sus features, el vectorizador y el clasificador [aqu铆](https://affectivetweets.cms.waikato.ac.nz/benchmark/).\n",
    "\n",
    "Recuerden que cada vez que ejecuten pipeline, deben hacerlo con una instancia distinta. De lo contrario, podr铆an solapar resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el pipeline para alg煤n dataset\n",
    "\n",
    "Ejecuta el pipeline sobre un dataset. Retorna el modelo ya entrenado mas sus labels asociadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.826406Z",
     "start_time": "2020-04-01T15:32:29.806148Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_experiment_0_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.846382Z",
     "start_time": "2020-04-01T15:32:29.826406Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name):\n",
    "\n",
    "    # Dividimos el dataset en train y test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "\n",
    "    pipeline = get_experiment_0_pipeline()\n",
    "\n",
    "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Obtenemos el orden de las clases aprendidas.\n",
    "    learned_labels = pipeline.classes_\n",
    "\n",
    "    # Evaluamos:\n",
    "    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
    "    return pipeline, learned_labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el pipeline por cada train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:30.066135Z",
     "start_time": "2020-04-01T15:32:29.846382Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  7  47   0]\n",
      " [  5 193   8]\n",
      " [  0  43   8]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.58      0.13      0.21        54\n",
      "      medium       0.68      0.94      0.79       206\n",
      "        high       0.50      0.16      0.24        51\n",
      "\n",
      "    accuracy                           0.67       311\n",
      "   macro avg       0.59      0.41      0.41       311\n",
      "weighted avg       0.64      0.67      0.60       311\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.615\tKappa: 0.133\tAccuracy: 0.669\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 22  69   2]\n",
      " [  8 211  14]\n",
      " [  2  70  17]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.69      0.24      0.35        93\n",
      "      medium       0.60      0.91      0.72       233\n",
      "        high       0.52      0.19      0.28        89\n",
      "\n",
      "    accuracy                           0.60       415\n",
      "   macro avg       0.60      0.44      0.45       415\n",
      "weighted avg       0.60      0.60      0.55       415\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.672\tKappa: 0.192\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[ 14  68   2]\n",
      " [  9 141  13]\n",
      " [  0  24  27]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.61      0.17      0.26        84\n",
      "      medium       0.61      0.87      0.71       163\n",
      "        high       0.64      0.53      0.58        51\n",
      "\n",
      "    accuracy                           0.61       298\n",
      "   macro avg       0.62      0.52      0.52       298\n",
      "weighted avg       0.61      0.61      0.56       298\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.729\tKappa: 0.261\tAccuracy: 0.611\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 14  52   1]\n",
      " [ 14 144   7]\n",
      " [  0  39  13]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.50      0.21      0.29        67\n",
      "      medium       0.61      0.87      0.72       165\n",
      "        high       0.62      0.25      0.36        52\n",
      "\n",
      "    accuracy                           0.60       284\n",
      "   macro avg       0.58      0.44      0.46       284\n",
      "weighted avg       0.59      0.60      0.55       284\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.633\tKappa: 0.175\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "Average scores:\n",
      "\n",
      " Average AUC: 0.662\t Average Kappa: 0.19\t Average Accuracy: 0.621\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "scores_array = []\n",
    "\n",
    "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "for dataset_name, dataset in train.items():\n",
    "    # ejecutamos el pipeline sobre el dataset\n",
    "    classifier, learned_labels, scores = run(dataset, dataset_name)\n",
    "\n",
    "    # guardamos el clasificador entrenado\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "    # guardamos las labels aprendidas por el clasificador\n",
    "    learned_labels_array.append(learned_labels)\n",
    "\n",
    "    # guardamos los scores obtenidos\n",
    "    scores_array.append(scores)\n",
    "\n",
    "# print avg scores\n",
    "print(\n",
    "    \"Average scores:\\n\\n\",\n",
    "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "    .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    }
   },
   "source": [
    "### Predecir target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:30.086102Z",
     "start_time": "2020-04-01T15:32:30.066135Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:30.296588Z",
     "start_time": "2020-04-01T15:32:30.086102Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.isdir('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivo separado\n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
