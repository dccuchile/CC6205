
%\documentclass[mathserif]{beamer}
\documentclass[handout]{beamer}
%\usetheme{Goettingen}
%\usetheme{Warsaw}
\usetheme{Singapore}



%\usetheme{Frankfurt}
%\usetheme{Copenhagen}
%\usetheme{Szeged}
%\usetheme{Montpellier}
%\usetheme{CambridgeUS}
%\usecolortheme{}
%\setbeamercovered{transparent}
\usepackage[english, activeacute]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{dsfont}
\usepackage{graphics}
\usepackage{cases}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{multirow}	
\usepackage{amstext}
\usepackage[ruled,vlined,lined]{algorithm2e}
\usepackage{amsmath}
\usepackage{epic}
\usepackage{epsfig}
\usepackage{fontenc}
\usepackage{framed,color}
\usepackage{palatino, url, multicol}
%\algsetup{indent=2em}
\newcommand{\factorial}{\ensuremath{\mbox{\sc Factorial}}}
\newcommand{\BIGOP}[1]{\mathop{\mathchoice%
{\raise-0.22em\hbox{\huge $#1$}}%
{\raise-0.05em\hbox{\Large $#1$}}{\hbox{\large $#1$}}{#1}}}
\newcommand{\bigtimes}{\BIGOP{\times}}
\vspace{-0.5cm}
\title{Natural Language Processing \\ Large Language Models Usage and Evaluation Patterns}
\vspace{-0.5cm}
\author[Felipe Bravo MÃ¡rquez]{\footnotesize
%\author{\footnotesize  
 \textcolor[rgb]{0.00,0.00,1.00}{Felipe Bravo-Marquez}} 
  
 

\date{\today}

\begin{document}
\begin{frame}
\titlepage


\end{frame}


% Large Language Models (LLMs) have changed the way computers understand and use human language. They're used in many different areas, and in this talk we'll look at how people use and evaluate them. We'll start by looking at the different ways people use LLMs. First, when they are used as general assistants for tasks like writing, summarizing, coding, etc. Then, when they are adapted to address more domain-specific tasks using two approaches: 1) retrieval-assisted generation, and 2) fine-tuning. We'll also see how LLMs are integrated into software applications, such as when they are invoked by computer code (API calls) or used by autonomous agents to make decisions on their own. On the evaluation side, we'll talk about a method called MTBench, a multi-turn question set, and Chatbot Arena, a crowdsourced battle platform between LLMs.





\section{Introduction}
\begin{frame}{Introduction}
\begin{scriptsize}
\begin{itemize}
\item Since the inception of Large Language Models, various patterns of use and evaluation of this technology have emerged.
\item In this talk, we will try to organize these patterns and give a general overview of them.
\end{itemize}

 \begin{figure}[h]
        	\includegraphics[scale = 0.2]{pics/Large-Language-Models.jpg}
        \end{figure}
Source: \url{https://www.masayume.it/img/masayume/Large-Language-Models.jpg}
\end{scriptsize}
\end{frame}



\begin{frame}{Recap: What is an LLM}
\begin{scriptsize}
\begin{itemize}
\item  Large Language Model: An autoregressive language model trained with a Transformer neural network on a large corpus (hundreds of bullions of tokens) and a large parameter space (billions) to predict the next word.
\item It is usually later aligned to work as a user assistant using techniques such as Reinforcement Learning From Human Feedback  \cite{ouyang2022training} or supervised fine-tuning.
\item Some are private (access via API or web browser): Google Bard, ChatGPT, etc.
\item Others are open (model's weights can be downloaded): Llama, LLama2, Falcon, etc.
\end{itemize}

 \begin{figure}[h]
        	\includegraphics[scale = 0.4]{pics/gptllama.png}
        \end{figure}

\end{scriptsize}
\end{frame}

\begin{frame}{Zero-shot, One-shot, and Few-shot Learning}
\scriptsize
The most remarkable feature of these models is their few-shot, one-shot, zero-shot learning capabilities (also known as ``in-context-learning'').
 \begin{figure}[h]
        	\includegraphics[scale = 0.12]{pics/zeroonefew.png}
        \end{figure}  

This means that they can learn new tasks without large amounts of human-annotated data.


\end{frame}


\begin{frame}{Talk Overview}
\begin{scriptsize}
\begin{itemize}
 \item Despite the recency of this technology, its adoption has been tremendous in many areas. 
 \item Below, we propose a simple categorization of the ways in which LLMs are used and evaluated.
  \item These patterns will serve as the narrative backbone of this presentation.
\end{itemize}



\begin{columns}[t]
\column{0.5\textwidth}
\begin{block}{Usage Patterns}
\begin{enumerate}
\item General-domain Assistant
\item Domain-specific Assistant
\begin{enumerate}\scriptsize
\item Retrieval-augmented generation
\item Fine-Tuning
\end{enumerate}
\item LLM-based Applications
\begin{enumerate}\scriptsize
\item API calls
\item Autonomous Agents
\end{enumerate}
\end{enumerate}
\end{block}

\column{0.5\textwidth}
\begin{block}{Evaluation Patterns}
\begin{itemize}
\item MTBench
\item LLM Arena
\end{itemize}
\end{block}

\end{columns}

\end{scriptsize}
\end{frame}









% \url{https://ai.meta.com/llama/get-started/?trk=feed_main-feed-card_reshare_feed-article-content}.

\section{General-domain Assistant}

\begin{frame}{Usage Pattern 1: General-domain Assistant}
\begin{scriptsize}
\begin{itemize}
\item In this pattern a user interacts with the LLM proving prompts as input and receiving a text as answer.
\item The knowledge the LLM has access is limited to the corpus on which it was trained and the context given in the prompt.
\end{itemize}

 \begin{figure}[h]
        	\includegraphics[scale = 0.6]{pics/assistantpattern.pdf}
        \end{figure}  


\end{scriptsize}
\end{frame}


\begin{frame}{Tasks}
\begin{scriptsize}
LLMs can solve many tasks with this pattern:

\begin{itemize}
\item Textual: Language understanding and common sense (e.g., rewriting, summarizing, translating, answering questions)
\item Arithmetic: Mathematical reasoning  (it can fail in many cases though)
\item Visual: Multimodal reasoning involving pictures (GPT-4, Llava)
\item Symbolic: Structured input such as programming languages
\end{itemize}
Source: \url{https://twitter.com/IntuitMachine/status/1727079666001870877}.
\end{scriptsize}
\end{frame}


\begin{frame}{Prompt Engineering: Guiding the Language Model}
\begin{scriptsize}
Prompt engineering, often referred to as ``Prompting,'' is the discipline or ``art'' of crafting effective prompts to guide the Language Model (LM) towards generating accurate responses. Some common prompting guidelines: \vspace{0.5cm}

\begin{itemize}
\item \textbf{Clarity and Conciseness}: Clearly articulate the prompt to minimize ambiguity and ensure the LM understands the task at hand.
\item \textbf{Use of Specific Examples}: Provide concrete examples within the prompt to offer the LM context.
\item \textbf{Role-based Prompts}: incorporating roles into the prompts (e.g., a tour guide, a teacher, a doctor).
\item \textbf{Desired Output Specification}: Clearly define the desired format of the output (e.g, JSON, HTML, csv). 
\end{itemize}
\end{scriptsize}
\end{frame}


\begin{frame}{Chain-of-thought Prompting}

\begin{scriptsize}
\begin{itemize}
\item Chain-of-thought prompting is a simple mechanism for eliciting multi-step reasoning behavior in large language models.
\item  This method involves augmenting each exemplar in a few-shot prompt with a connected sequence of thoughts, creating a structured chain of logical steps. \cite{wei2022chain}
\end{itemize}
\end{scriptsize}



 \begin{figure}[h]
        	\includegraphics[scale = 0.3]{pics/chainoftought.png}
        \end{figure}



\end{frame}


\section{Domain-specific Assistant}

\begin{frame}{Usage Pattern 2: Domain-specific Assistant}
\begin{scriptsize}
\begin{itemize}
\item Idea: Incorporate domain-specific knowledge not covered in training (e.g., recent news, private documents).
\item This is very common for companies developing chatbots with private documents or for creating more domain-specific chatbots.
\item There are two main patterns to achieve this:
\begin{enumerate}\scriptsize
 \item Retrieval-Augmented Generation (Vector Databases)
 \item Fine-Tuning
\end{enumerate}
\end{itemize}


\end{scriptsize}
% https://platform.openai.com/docs/guides/fine-tuning/common-use-cases
\end{frame}


\begin{frame}{Retrieval-Augmented Generation}
\begin{scriptsize}
Idea: Incorporate domain-specific knowledge into the query using information retrieval and document embeddings (i.e., densely encoded vectors that capture the semantic information of the document). \cite{lewis2021retrievalaugmented}.

\end{scriptsize}

    \begin{figure}[h]
        	\includegraphics[scale = 0.4]{pics/retrievalaugmented.pdf}
        \end{figure}  


\end{frame}


\begin{frame}{Retrieval-Augmented Generation Process}
\begin{scriptsize}
\begin{enumerate}
 \item Encode all domain-specific documents using document embeddings and store them in a vector database.  
  \begin{itemize}\scriptsize
    \item OpenAI provides a vectorizer called (text2vec-openai), but there are many open source alternatives.
 \item There are also many vector databases available, a popular one is \textbf{weaviate}.
  \end{itemize}
 \item Encode the prompt with the same vectorizer used to encode documents.
 \item Use prompt embedding and the vecto database to retrieve relevant documents based on similarity.
 \item Create a refined prompt that includes a domain-specific role, the user prompt, and the retrieved documents as contexts.
 \item Send the refined prompt to the LLM and return the response to the user.
\end{enumerate}

\end{scriptsize}



\end{frame}

 

%\item \url{https://www.infoworld.com/article/3709912/vector-databases-in-llms-and-search.html}
%\item \url{https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/1/introduction}
%\item \url{https://stackoverflow.blog/2023/10/09/from-prototype-to-production-vector-databases-in-generative-ai-applications/}


\begin{frame}{Fine-Tuning}
\begin{scriptsize}
Idea: Incorporate domain-specific knowledge by fine-tuning a pre-trained LLM with the next-token prediction task over a domain-specific corpus and interact with the resulting LLM. 



    \begin{figure}[h]
        	\includegraphics[scale = 0.35]{pics/llmfinetuning.pdf}
        \end{figure}  

This can be computationally expensive unless some tricks are used.
\end{scriptsize}
\end{frame}




\begin{frame}{Instruction Fine-Tuning}

%Setting the style, tone, format, or other qualitative aspects, Improving reliability at producing a desired output Correcting failures to follow complex prompts   Handling many edge cases in specific ways     Performing a new skill or task thatâs hard to articulate in a prompt


\begin{scriptsize}
\begin{itemize}
\item Idea: instead of training the LM with raw text with next token prediction, train it with pairs of prompts and user-aligned answers.
\item Paid Fine-Tuning (GPT-4??)  
\item OpenAI offers many more specific gpts: \url{https://openai.com/blog/introducing-gpts}
\item Alpaca, Vicuna, Llama, Llama2
\item https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{Datasets for Instruction Fine-Tuning}
\begin{scriptsize}
\begin{itemize}
\item Standford Alpaca Dataset (Vicuna)
\item ShareGPT (Alpaca)
\item Dolly-15K
\item Orca Dataset
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{Parameter Efficient Fine Tuning}
\begin{scriptsize}
\begin{itemize}
\item Lora, QLora
\item https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{Token-Incrementation}
\begin{scriptsize}
\begin{itemize}
\item Lora, QLora
\item https://blog.gopenai.com/paper-review-qlora-efficient-finetuning-of-quantized-llms-a3c857cd0cca
\end{itemize}
\end{scriptsize}
\end{frame}




\section{Applications}

\begin{frame}{Applications}
\begin{scriptsize}
\begin{itemize}
\item LLMs can be embedded into any software via API calls. For example, a search engine (you.com)
\item https://gptstore.ai/
\item For example, PDF summarization software. You write software that first converts the PDF to raw text and then sends it to an LLM for summarization.
\item Or a software for summarizing videoconferences. First the audio is transcribed and then summarized with an LLM.
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{Autonomous Agents}
%https://arxiv.org/pdf/2304.03442.pdf
% https://arxiv.org/abs/2210.03629
%https://bootcamp.uxdesign.cc/a-comprehensive-and-hands-on-guide-to-autonomous-agents-with-gpt-b58d54724d50
% https://leftasexercise.com/2023/06/17/autonomous-agents-and-llms-autogpt-langchain-and-all-that/
\begin{scriptsize}
\begin{itemize}
\item Agents are a special kind of LLMs application in which the LLM serves as the reasoning and planning component of the software.
\item agent in the sense of perceiving an environment and taking actions to achieve goals. 
\end{itemize}
\end{scriptsize}
\end{frame}




\section{Evaluation Patterns}
\begin{frame}{LLMBench and LLm Arena}
\begin{scriptsize}
\begin{itemize}
\item Standard NLP evaluation: human annotated gold-labels and metrics.
\item LLMS are intrinsically multi-task and not easily evaluated with this approach.
\item Machines evaluating machines??
\item MT-bench (categories)
\item HuggingFace Open LLM Leaderboard
\item LLM Arena
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}
\frametitle{Questions?}
%\vspace{1.5cm}
\begin{center}\LARGE Thanks for your Attention!\\ \end{center}



\end{frame}

\begin{frame}[allowframebreaks]\scriptsize
\frametitle{References}
\bibliography{bio}
\bibliographystyle{apalike}
%\bibliographystyle{flexbib}
\end{frame}  


%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
