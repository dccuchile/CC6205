
%\documentclass[mathserif]{beamer}
\documentclass[handout]{beamer}
%\usetheme{Goettingen}
%\usetheme{Warsaw}
\usetheme{Singapore}



%\usetheme{Frankfurt}
%\usetheme{Copenhagen}
%\usetheme{Szeged}
%\usetheme{Montpellier}
%\usetheme{CambridgeUS}
%\usecolortheme{}
%\setbeamercovered{transparent}
\usepackage[english, activeacute]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{dsfont}
\usepackage{graphics}
\usepackage{cases}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{multirow}	
\usepackage{amstext}
\usepackage[ruled,vlined,lined]{algorithm2e}
\usepackage{amsmath}
\usepackage{epic}
\usepackage{epsfig}
\usepackage{fontenc}
\usepackage{framed,color}
\usepackage{palatino, url, multicol}
%\algsetup{indent=2em}
\newcommand{\factorial}{\ensuremath{\mbox{\sc Factorial}}}
\newcommand{\BIGOP}[1]{\mathop{\mathchoice%
{\raise-0.22em\hbox{\huge $#1$}}%
{\raise-0.05em\hbox{\Large $#1$}}{\hbox{\large $#1$}}{#1}}}
\newcommand{\bigtimes}{\BIGOP{\times}}
\vspace{-0.5cm}
\title{Natural Language Processing \\ Transformer Architecture}
\vspace{-0.5cm}
\author[Felipe Bravo Márquez]{\footnotesize
%\author{\footnotesize  
 \textcolor[rgb]{0.00,0.00,1.00}{Felipe Bravo-Marquez}} 
  
 

\date{\today}

\begin{document}
\begin{frame}
\titlepage


\end{frame}



\begin{frame}{What's Wrong with RNNs?}
\begin{scriptsize}
\begin{itemize}
\item Given what we just learned on previous lecture, it would seem like attention solves all the problems with RNNs and encoder-decoder architectures\footnote{The following material is based on: \url{http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/} and \url{http://jalammar.github.io/illustrated-transformer/}}.

\item There are a few shortcomings of RNNs that another architecture called the \textbf{Transformer} tries to address.

\item The Transformer discards the recursive component of the Encoder-Decoder architecture and purely relies on attention mechanisms \cite{vaswani2017attention}.

\item When we process a sequence using RNNs, each hidden state depends on the previous hidden state. 

\item This becomes a major pain point on GPUs: GPUs have a lot of computational capability and they hate having to wait for data to become available.

\item Even with technologies like CuDNN, RNNs are painfully inefficient and slow on the GPU.


\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{Dependencies in neural machine translations}
\begin{scriptsize}
In essence, there are three kinds of dependencies in neural machine translations: 
\begin{enumerate}
\item Dependencies between the input and output tokens.

\item Dependencies between the input tokens themselves.

\item Dependencies between the output tokens themselves.
\end{enumerate}


The traditional attention mechanism largely solved the first dependency by giving the decoder access to the entire input sequence. The second and third dependencies were addressed by the RNNs.

\end{scriptsize}

\end{frame}


\begin{frame}{The Transformer}
\begin{scriptsize}
\begin{itemize}
 \item The novel idea of the Transformer is to extend this mechanism to the processing input and output sentences as well.
 \item The RNN processes input sequences sequentially. 
 \item The Transformer, on the other hand, allows the encoder and decoder to see the entire input sequence all at once.
 \item This is done using attention.
\end{itemize}



\end{scriptsize}

\end{frame}




\begin{frame}{The Key Component: Multi-Head Attention}
\begin{scriptsize}
\begin{itemize}
 \item The attention mechanism in the Transformer is interpreted as a way of computing the relevance of a set of \textbf{values} (information) based on some \textbf{keys} and \textbf{queries}. 
 
 \item The attention mechanism is used as a way for the model to \textbf{focus on relevant information} based on what it is currently processing.
 
 \item In the RNN encoder-decoder architecture with attention:
 

 \begin{enumerate}
  \begin{scriptsize}
 \item Attention weights were the relevance of the encoder hidden states (values) in processing the decoder state (query).
 
 \item These values were calculated based on the encoder hidden states (keys) and the decoder hidden state (query).
 \end{scriptsize}

 \end{enumerate}

 
\end{itemize}



\end{scriptsize}

\end{frame}


\begin{frame}{The Key Component: Multi-Head Attention}

    \begin{figure}[h]
        	\includegraphics[scale = 0.28]{pics/attention_concept.png}
        	\caption{In this example, the query is the word being decoded (which means dog) and both the keys and values are the source sentence. The attention score represents the relevance, and in this case is large for the word “dog” and small for others.}
        \end{figure}  

\end{frame}




\begin{frame}{The Key Component: Multi-Head Attention}
\begin{scriptsize}
\begin{itemize}
 \item When we think of attention this way, we can see that the keys, values, and queries could be anything. 
 \item They could even be the same. 
 \item For instance, both values and queries could be input embeddings (self attention). 
 
 \item If we only computed a single attention weighted sum of the values, it would be difficult to capture various different aspects of the input.
 

\end{itemize}

\end{scriptsize}

\end{frame}



\begin{frame}{The Key Component: Multi-Head Attention}
\begin{scriptsize}
\begin{itemize}
 \item For instance, in the sentence ``I like cats more than dogs'', you might want to capture the fact that the sentence compares two entities, while also retaining the actual entities being compared. 
 
 \item To solve this problem the Transformer uses the Multi-Head Attention block. 
 
 \item This block computes multiple attention weighted sums instead of a single attention pass over the values.
 
 \item Hence the name ``Multi-Head'' Attention.
 
 
 
\end{itemize}

\end{scriptsize}

\end{frame}


\begin{frame}{The Key Component: Multi-Head Attention}
\begin{scriptsize}
\begin{itemize}
 \item To learn diverse representations, the Multi-Head Attention applies different linear transformations to the values, keys, and queries for each “head” of attention.
 
 
     \begin{figure}[h]
        	\includegraphics[scale = 0.48]{pics/multi_head_attention.png}
        \end{figure}  
 

\end{itemize}

\end{scriptsize}

\end{frame}



\begin{frame}{The Key Component: Multi-Head Attention}
\begin{scriptsize}
\begin{itemize}

 \item A single attention applies a unique linear transformation to its input queries, keys, and values.
 
 \item Then computes the attention score between each query and key.
 
 \item The attention score is used to weight the values and sum them up.


\item The Multi-Head Attention block just applies multiple blocks in parallel, concatenates their outputs, then applies one single linear transformation. 
 
\end{itemize}

\end{scriptsize}

\end{frame}


\begin{frame}{Scaled Dot Product Attention}
\begin{scriptsize}
\begin{itemize}

 \item  Transformer uses a particular form of attention called the “Scaled Dot-Product Attention”:
 \begin{displaymath}
  \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
 \end{displaymath}

 \item where $Q$ is the matrix of queries packed together and $K$ and $V$ are the matrices of keys and values packed together. 
 
 \item $d_k$ represents the dimensionality of the queries and keys.
 
  \item The normalization over $\sqrt{d_k}$ is used to rescale the dot products between queries and keys (dot products tend to grow with the dimensionality).
 
\end{itemize}

\end{scriptsize}

\end{frame}


\begin{frame}{The Transformer}

     \begin{figure}[h]
        	\includegraphics[scale = 0.29]{pics/transformer.png}
        \end{figure}  


\end{frame}


\begin{frame}{The Transformer}
\begin{scriptsize}
\begin{itemize}

 \item  The Transformer still uses the basic encoder-decoder design of RNN neural machine translation systems.
 
 \item The left-hand side is the encoder, and the right-hand side is the decoder. 
 
 \item The initial inputs to the encoder are the embeddings of the input sequence.
 \item The initial inputs to the decoder are the embeddings of the outputs up to that point. 
 
 \item The encoder and decoder are composed of $N$ blocks (where $N = 6$ for both networks).
 
 \item These blocks are composed of smaller blocks as well. 
 
 \item Let's look at each block in further detail.
 
\end{itemize}

\end{scriptsize}

\end{frame}



\begin{frame}{The Encoder}
\begin{scriptsize}
\begin{itemize}

\item The encoder contains self-attention layers. 

\item In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. 

\item Each position in the encoder can attend to all positions in the previous layer of the encoder.

 \item  The encoder is composed of two blocks (which we will call sub-layers to distinguish from the $N$ blocks composing the encoder and decoder).
\item  One is the Multi-Head Attention sub-layer over the inputs, mentioned above. 
\item The other is a simple feed-forward network. 

\end{itemize}

\end{scriptsize}


    


\end{frame}



\begin{frame}{The Encoder}



     \begin{figure}[h]
        	\includegraphics[scale = 0.35]{pics/Transformer_encoder.png}
        \end{figure}  





\end{frame}


\begin{frame}{Bringing The Tensors Into The Picture}
\begin{scriptsize}
\begin{itemize}



\item We begin by turning each input word into a vector using an embedding layer  of the size 512 in the bottom-most encoder.

\begin{figure}[h]
        	\includegraphics[scale = 0.35]{pics/embeddings_enc.png}
        \end{figure}  




\item In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that's directly below.

\item The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.


\end{itemize}

\end{scriptsize}

    

\end{frame}





\begin{frame}{Bringing The Tensors Into The Picture}
\begin{scriptsize}

\begin{figure}[h]
        	\includegraphics[scale = 0.2]{pics/encoder_with_tensors.png}
        \end{figure}  

\begin{itemize}


\item The word in each position flows through its own path in the encoder. 

\item There are dependencies between these paths in the self-attention layer. 

\item The feed-forward layer does not have those dependencies.

\item However, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.


\end{itemize}

\end{scriptsize}


\end{frame}


\begin{frame}{Now We’re Encoding!}
\begin{scriptsize}


\begin{itemize}


\item As we've mentioned already, an encoder receives a list of vectors as input. 

\item It processes this list by passing these vectors into a 'self-attention' layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.

\end{itemize}

\begin{figure}[h]
        	\includegraphics[scale = 0.2]{pics/encoder_with_tensors_2.png}
        \end{figure}  


\end{scriptsize}


\end{frame}


\begin{frame}{Self-Attention at a High Level}
\begin{scriptsize}



\begin{itemize}


\item Say the following sentence is an input sentence we want to translate: 
\\ \textcolor{red}{``The animal didn't cross the street because it was too tired''}

\item What does ``it'' in this sentence refer to? 

\item Is it referring to the street or to the animal? 
\item It's a simple question to a human, but not as simple to an algorithm.

\item When the model is processing the word ``it'', self-attention allows it to associate ``it'' with ``animal''.

\item As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.


\end{itemize}

\end{scriptsize}


\end{frame}


\begin{frame}{Self-Attention at a High Level}
\begin{scriptsize}



\begin{itemize}


\item Think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it's processing. 

\item Self-attention is the method the Transformer uses to bake the ``understanding'' of other relevant words into the one we're currently processing.


\end{itemize}

\end{scriptsize}

\begin{figure}[h]
        	\includegraphics[scale = 0.4]{pics/transformer_self-attention_visualization.png}
        \end{figure}  


\end{frame}



\begin{frame}{Self-Attention in Detail: step 1}
\begin{scriptsize}



\begin{itemize}


\item The \textbf{first step} in calculating self-attention is to create three vectors from each of the encoder's input vectors (in this case, the embedding of each word).

\item So for each word, we create a Query vector, a Key vector, and a Value vector.

\item These vectors are created by multiplying the embedding by three matrices that we trained during the training process.

\item Notice that these new vectors are smaller in dimension than the embedding vector. 

\item Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. 

\item They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.

\end{itemize}

\end{scriptsize}


\end{frame}



\begin{frame}{Self-Attention in Detail: step 1}


\begin{figure}[h]
        	\includegraphics[scale = 0.3]{pics/transformer_self_attention_vectors.png}
        \end{figure}  

\begin{scriptsize}


\begin{itemize}        
\item Example sentence: ``Thinking Machines''.
\item Multiplying \textcolor{green}{$x_1$} by the \textcolor{purple}{WQ} weight matrix produces \textcolor{purple}{$q_1$}, the ``query'' vector associated with that word.
\item We end up creating a ``query'', a ``key'', and a ``value'' projection of each word in the input sentence.

\end{itemize}

\end{scriptsize}


        

\end{frame}


\begin{frame}{Self-Attention in Detail: step 2}
\begin{scriptsize}



\begin{itemize}


\item The \textbf{second step} in calculating self-attention is to calculate a score. 

\item Say we’re calculating the self-attention for the first word in this example, ``Thinking''.

\item We need to score each word of the input sentence against this word. 

\item The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.

\item The score is calculated by taking the dot product of the \textcolor{purple}{query vector} with the \textcolor{orange}{key vector} of the respective word we're scoring. 
\item So if we're processing the self-attention for the word in position \textcolor{green}{\#1}, the first score would be the dot product of \textcolor{purple}{q1} and \textcolor{orange}{k1}.
\item The second score would be the dot product of \textcolor{purple}{q1} and \textcolor{orange}{k2}.

\end{itemize}

\end{scriptsize}


\end{frame}


\begin{frame}{The Encoder}
\begin{scriptsize}
\begin{itemize}

\item Between each sub-layer, there is a residual connection followed by a layer normalization. 

\item A residual connection is basically just taking the input and adding it to the output of the sub-network, and is a way of making training deep networks easier. 

\item Layer normalization is a normalization method in deep learning that is similar to batch normalization.
 
\end{itemize}

\end{scriptsize}

\end{frame}



\begin{frame}{The Encoder}

     \begin{figure}[h]
        	\includegraphics[scale = 0.39]{pics/transformerencoder.png}
        \end{figure}  


\end{frame}

\begin{frame}{The Encoder}
\begin{scriptsize}
\begin{itemize}

 \item  What each encoder block is doing is actually just a bunch of matrix multiplications followed by a couple of element-wise transformations.
 
 \item This is why the Transformer is so fast: everything is just parallelizable matrix multiplications.
 
 \item The point is that by stacking these transformations on top of each other, we can create a very powerful network. 
 
 \item The core of this is the attention mechanism which modifies and attends over a wide range of information.
 
\end{itemize}

\end{scriptsize}

\end{frame}



\begin{frame}{The Decoder}
\begin{scriptsize}
\begin{itemize}



\item The decoder has ``encoder-decoder attention'' layers in which the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.

\item This allows every position in the decoder to attend over all positions in the input sequence.

\item This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.

\item Self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.

 \item This plays a similar role to the decoder hidden state in RNN machine translation architectures.


 
\end{itemize}

\end{scriptsize}

\end{frame}





\begin{frame}{The Decoder}
\begin{scriptsize}
\begin{itemize}
 \item When we train the Transformer, we want to process all the sentences at the same time. 
 
 \item When we do this, if we give the decoder access to the entire target sentence, the model can just repeat the target sentence (in other words, it doesn't need to learn anything).
 
 \item To prevent this from happening, the decoder masks the ``future'' tokens when decoding a certain word.

  \item The masking is done by setting to $- \infty$ all values in the input of the softmax which correspond to illegal connections.
 \item This is why ``multi-head attention blocks'' in the decoder are referred to as ``masked'': the inputs to the decoder from future time-steps are masked.

\end{itemize}

\end{scriptsize}

\end{frame}


\begin{frame}{The Decoder}

     \begin{figure}[h]
        	\includegraphics[scale = 0.29]{pics/transformerdecoder.png}
        \end{figure}  


\end{frame}



\begin{frame}{Positional Encodings}
\begin{scriptsize}
\begin{itemize}

 \item  Unlike recurrent networks, the multi-head attention network cannot naturally make use of the position of the words in the input sequence.
 
 \item Without positional encodings, the output of the multi-head attention network would be the same for the sentences ``I like cats more than dogs'' and ``I like dogs more than cats''.
 
 \item Positional encodings explicitly encode the relative/absolute positions of the inputs as vectors and are then added to the input embeddings.

\end{itemize}

\end{scriptsize}


\end{frame}

\begin{frame}{Positional Encodings}
\begin{scriptsize}
\begin{itemize}

 \item The paper uses the following equation to compute the positional encodings: \\
 $PE(pos,2i) = \sin(pos/10000^{2i/d_{model}})$ 
  $PE(pos,2i+1) = \cos(pos/10000^{2i/d_{model}})$
 
\item Where $pos$ represents the position, and $i$ is the dimension.

\item Basically, each dimension of the positional encoding is a wave with a different frequency. 


\end{itemize}

\end{scriptsize}


\end{frame}


\begin{frame}{Conclusions}
\begin{scriptsize}
\begin{itemize}

 \item The Transformer achieves better BLUE scores than previous state-of-the-art models for English-to-German translation and English-to-French translation at a fraction of the training cost.

      \begin{figure}[h]
        	\includegraphics[scale = 0.29]{pics/transformerresults.png}
        \end{figure}  

\item The Transformer is a powerful and efficient alternative to recurrent neural networks to model dependencies using only attention mechanisms.

\item A very illustrative blog post about the Transformer: \url{http://jalammar.github.io/illustrated-transformer/}.
 
\end{itemize}

\end{scriptsize}


\end{frame}

\begin{frame}
\frametitle{Questions?}
%\vspace{1.5cm}
\begin{center}\LARGE Thanks for your Attention!\\ \end{center}



\end{frame}

\begin{frame}[allowframebreaks]\scriptsize
\frametitle{References}
\bibliography{bio}
\bibliographystyle{apalike}
%\bibliographystyle{flexbib}
\end{frame}  


%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
