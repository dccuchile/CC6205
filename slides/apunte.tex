\documentclass{book}
\usepackage[spanish]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{dsfont}
\usepackage{graphics}
\usepackage{cases}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{amstext}
\usepackage[ruled,vlined,lined]{algorithm2e}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{epic}
\usepackage{epsfig}
\usepackage{fontenc}
\usepackage{framed,color}
\usepackage{palatino, url, multicol}
\usepackage{hyperref}


\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

\begin{document}


\thispagestyle{empty}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        \includegraphics[width=2cm]{pics/escudocolor.png} % Replace "pics/escudocolor.png" with the filename and path of your logo image
        \vspace{1cm}
        
        {\Huge\textbf{Universidad de Chile}} \\
        \vspace{0.5cm}
        {\Large Departamento de Ciencias de la Computación} \\
        \vspace{2cm}
        
        {\Huge\textbf{Procesamiento de Lenguaje Natural}} \\
        \vspace{0.5cm}
        {\Large Apuntes de Clases} \\
        \vspace{2cm}
        
        {\Large Felipe Bravo-Márquez} \\
        \vspace{2cm}
        
        {\large \today}
    \end{center}
\end{titlepage}

\newpage

\thispagestyle{empty}

\newpage
\pagenumbering{roman}

\tableofcontents 
\newpage

\listoftables
\newpage
\listoffigures
\newpage


\thispagestyle{empty}


\pagenumbering{arabic}

%..................................................

\chapter*{Prefacio}
\addcontentsline{toc}{chapter}{Preface}
Este apunte es el resultado de cinco años de experiencia en la enseñanza del curso de Procesamiento de Lenguaje Natural en la Universidad de Chile\footnote{El repositorio del curso se encuentra en: \url{https://github.com/dccuchile/CC6205}}. El objetivo es proporcionar una introducción a esta disciplina, centrándose en las técnicas y conceptos fundamentales. Se ha realizado un esfuerzo por lograr un equilibrio entre las técnicas tradicionales, como los modelos de lenguaje de N-gramas, Naive Bayes y los modelos ocultos de Markov (HMM), y los enfoques modernos basados en redes neuronales profundas, como los vectores de palabras, las redes neuronales recurrentes (RNN), los transformers y los grandes modelos de lenguaje. Sin embargo, es importante mencionar que existen temas relevantes que no se incluyen en este material, como el etiquetado de datos, las gramáticas, las técnicas de parsing, así como discusiones sobre tareas más específicas como el question answering.

El contenido se ha recopilado de diversas fuentes. Los temas relacionados con las redes neuronales se basan principalmente en el libro "Neural Network Methods for Natural Language Processing" de Goldberg \cite{goldberg2017neural}. Los temas no relacionados con las redes neuronales, como los modelos probabilísticos del lenguaje, Naive Bayes y HMM, se han tomado del curso de Michael Collins de Columbia \cite{collins2013language} y del borrador de la tercera edición del libro de Dan Jurafsky y James H. Martin \cite{JurafskyBook}. Además, algunos capítulos se han adaptado de tutoriales en línea y otros cursos, como el curso de Stanford de Christopher Manning\footnote{\url{http://web.stanford.edu/class/cs224n/}}. Las imágenes utilizadas se han extraído intencionalmente directamente de sus fuentes correspondientes. Algún día se diseñarán imágenes propias.

\chapter{Introducción}
\label{cap:intro}
\input{cap_intro}



\chapter{Modelo de Espacio Vectorial y Recuperación de Información}
\label{cap_ir}
\input{cap_ir}


\chapter{Modelos de Lenguaje Probabilísticos}
\label{cap_plm}
\input{cap_plm}


\chapter{Text Classification and Naïve Bayes}
\label{cap_nb}
\input{cap_nb}


\chapter{Modelos Lineales}
\label{cap_lineales}
\input{cap_lineales}


\chapter{Redes Neuronales}
\label{cap_redes}
\input{cap_redes}

\chapter{Vectores de Palabra}
\label{cap_embeddings}
\input{cap_embeddings}

\chapter{Etiquetado de Secuencias}
\label{cap_etisec}
\input{cap_etisec}



\chapter{Redes Neuronales Convolucionales}
\label{cap_cnn}
\input{cap_cnn}


\chapter{Redes Neuronales Recurrentes}
\label{cap_rnn}
%\input{cap_rnn}

%Translate this Latex book chapter of an NLP book on Recurrent Neural Networks to Spanish. Output in Latex format. Rearrange bullet points (\items) into full paragraphs. Make sure that sentences are connected in a more fluid way as they come. Elaborate on incomplete ideas where appropriate. Add caption and center images.


\section{La Abstracción de las RNN}

Si bien las representaciones derivadas de las redes convolucionales ofrecen cierta sensibilidad al orden de las palabras, su sensibilidad al orden se limita principalmente a patrones locales y no tiene en cuenta el orden de patrones que están lejos en la secuencia. Las redes neuronales recurrentes (RNN) permiten representar entradas secuenciales de longitud arbitraria en vectores de tamaño fijo, prestando atención a las propiedades estructuradas de las entradas \cite{goldberg2016primer}. Las RNN, especialmente aquellas con arquitecturas con compuertas como LSTM y GRU, son muy eficaces para capturar regularidades estadísticas en entradas secuenciales.

Utilizamos $\vec{x}_{i:j}$ para denotar la secuencia de vectores $\vec{x}_i, \dots, \vec{x}_j$. A alto nivel, la RNN es una función que toma como entrada una secuencia ordenada de longitud arbitraria de $n$ vectores de dimensión $d_{in}$, $\vec{x}_{1 :n}=\vec{x}_1,\vec{x}_2, \dots, \vec{x}_n$ ($\vec{x}_i \in \mathbb{R}^{d_{in}}$), y devuelve como salida un solo vector de dimensión $d_{out}$, $\vec{y}_n \in \mathbb{R}^{d_{out}}$:

\begin{equation}
\begin{split}
\vec{y}_n & = \text{RNN}(\vec{x}_{1:n}) \\
\vec{x}_i \in \mathbb{R}^{d_{in}}, & \quad \vec{y}_n \in \mathbb{R}^{d_{out}}
\end{split}
\end{equation}

Esto define implícitamente un vector de salida $\vec{y}_i$ para cada prefijo $\vec{x}_{1:i}$ de la secuencia $\vec{x}_{i:n}$. Denotamos por $RNN^{*}$ a la función que devuelve esta secuencia:

\begin{equation}
\begin{split}
\vec{y}_{1:n} & = RNN^{*}(\vec{x}_{1:n}) \\
\vec{y}_i & = \text{RNN}(\vec{x}_{1:i}) \\
\vec{x}_i \in \mathbb{R}^{d_{in}}, & \quad \vec{y}_n \in \mathbb{R}^{d_{out}}
\end{split}
\end{equation}

Luego, se utiliza el vector de salida $\vec{y}_n$ para realizar predicciones adicionales. Por ejemplo, un modelo para predecir la probabilidad condicional de un evento $e$ dado la secuencia $\vec{x}_{1:n}$ se puede definir como el $j$-ésimo elemento del vector de salida resultante de la operación softmax sobre una transformación lineal de la codificación RNN:

\begin{displaymath}
p(e = j|\vec{x}_{1:n}) = \text{softmax}(\text{RNN}(\vec{x}_{1:n})\cdot W +\vec{b})_{[j]}
\end{displaymath}

La función RNN proporciona un marco para condicionar toda la historia sin recurrir a la suposición de Markov que se utiliza tradicionalmente para modelar secuencias. La RNN se define de forma recursiva mediante una función $R$ que toma como entrada un vector de estado $\vec{s}_{i-1}$ y un vector de entrada $\vec{x}_{i}$, y devuelve un nuevo vector de estado $\vec{s}_i$. Luego, el vector de estado $\vec{s}_i$ se asigna a un vector de salida $\vec{y}_i$ mediante una función determinista simple $O(\cdot)$. La base de la recursión es un vector de estado inicial $\vec{s}_{0}$, que también es una entrada de la RNN. Por brevedad, a menudo omitimos el vector inicial $\vec{s}_{0}$ o asumimos que es el vector cero. Al construir una RNN, al igual que al construir una red de alimentación directa, es necesario especificar la dimensión de las entradas $\vec{x}_i$, así como las dimensiones de las salidas $\vec{y}_i$:

\begin{equation}
\begin{split}
RNN^{*}(\vec{x}_{1:n};\vec{s}_0) & = \vec{y}_{1:n} \\
\vec{y}_i & = O(\vec{s}_i) \\
\vec{s}_i & = R(\vec{s}_{i-1},\vec{x}_i) \\
\vec{x}_i \in \mathbb{R}^{d_{in}}, & \quad \vec{y}_i \in \mathbb{R}^{d_{out}}, \quad \vec{s}_i \in \mathbb{R}^{f(d_{out})}
\end{split}
\end{equation}

Las funciones $R$ y $O$ son las mismas en todas las posiciones de la secuencia. La RNN realiza un seguimiento de los estados de la computación a través del vector de estado $\vec{s}_i$ que se guarda y se pasa en las invocaciones de $R$.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{pics/RNN.png}
  \caption{Representación gráfica de una RNN.}
\end{figure}

Esta presentación sigue la definición recursiva y es válida para secuencias de longitud arbitraria. Sin embargo, para una secuencia de entrada de tamaño finito (y todas las secuencias de entrada con las que trabajamos son finitas), se puede desenrollar la recursión.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{pics/RNN-unrolled.png}
  \caption{Representación gráfica de una RNN desenrollada para una secuencia de longitud finita.}
\end{figure}

Los parámetros $\theta$ resaltan el hecho de que los mismos parámetros se comparten en todos los pasos de tiempo. Diferentes instancias de $R$ y $O$ darán como resultado estructuras de red diferentes. Observamos que el valor de $\vec{s}_i$ (y, por lo tanto, $\vec{y}_i$) se basa en toda la entrada $\vec{x}_1,\dots, \vec{x}_i$. Por ejemplo, al expandir la recursión para $i = 4$ obtenemos:

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{pics/RNN-recursion.png}
  \caption{Representación gráfica de la RNN después de expandir la recurs

ión para $i = 4$. Así, $\vec{s}_n$ y $\vec{y}_n$ pueden considerarse como la codificación de toda la secuencia de entrada. El objetivo del entrenamiento de la red es establecer los parámetros de $R$ y $O$ de manera que el estado transmita información útil para la tarea que estamos tratando de resolver.

\section{Red Elman o Simple-RNN}

Después de describir la abstracción de las RNN, ahora podemos discutir las instancias más simples de estas. Recordemos que estamos interesados en una función recursiva $\vec{s}_i = R(\vec{x}_i, \vec{s}_{i-1})$ tal que $\vec{s}_i$ codifique la secuencia $\vec{x}_{1:n}$. La formulación más sencilla de una RNN se conoce como Red Elman o Simple-RNN (S-RNN).

\begin{equation}
\begin{split}
\vec{s}_i & = R_{SRNN}(\vec{x}_{i},\vec{s}_{i-1}) = g(\vec{s}_{i-1}W^{s}+\vec{x}_{i}W^{x}+\vec{b}) \\
\vec{y}_i & = O_{SRNN}(\vec{s}_i) = \vec{s}_i \\
\vec{s}_i, \vec{y}_i \in \mathbb{R}^{d_{s}}, & \quad \vec{x}_i \in \mathbb{R}^{d_{x}}, \quad W^{x} \in \mathbb{R}^{d_{x}\times d_{s}}, \quad W^{s} \in \mathbb{R}^{d_{s}\times d_{s}}, \vec{b} \in \mathbb{R}^{d_{s}}
\end{split}
\end{equation}

El estado $\vec{s}_i$ y la entrada $\vec{x}_i$ se transforman linealmente. Los resultados se suman (junto con un término de sesgo) y luego se pasan a través de una función de activación no lineal $g$ (comúnmente tangente hiperbólica o ReLU). El Simple-RNN ofrece buenos resultados para etiquetado de secuencias y modelado del lenguaje.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.55]{pics/elman.pdf}
  \caption{Red Elman o Simple-RNN.}
\end{figure}

\section{Entrenamiento de las RNN}

Una RNN desenrollada es simplemente una red neuronal muy profunda. Los mismos parámetros se comparten en muchas partes de la computación y se agrega una entrada adicional en varias capas. Para entrenar una red RNN, se necesita crear el grafo computacional desenrollado para una secuencia de entrada dada, agregar un nodo de pérdida al grafo desenrollado y luego utilizar el algoritmo de retropropagación para calcular los gradientes con respecto a esa pérdida. Este procedimiento se conoce en la literatura de las RNN como retropropagación a través del tiempo (BPTT). La RNN por sí sola no hace mucho, sino que sirve como un componente entrenable en una red más grande. La predicción final y el cálculo de pérdida se realizan en esa red más grande y el error se retropropaga a través de la RNN. De esta manera, la RNN aprende a codificar propiedades de las secuencias de entrada que son útiles para la tarea de predicción. La señal de supervisión no se aplica directamente a la RNN, sino a través de la red más grande.

\section{Patrones de uso de las RNN: Aceptador}

Un patrón común de uso de las RNN es el patrón del aceptador. Este patrón se utiliza para la clasificación de texto, donde la señal de supervisión se basa únicamente en el vector de salida final $\vec{y}_n$. El vector de salida de la RNN $\vec{y}_n$ se alimenta a una capa completamente conectada o un MLP, que produce una predicción. Los gradientes de error luego se retropropagan a través del resto de la secuencia. La pérdida puede tener cualquier forma conocida, como entropía cruzada o hinge, etc.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{pics/acceptor.png}
  \caption{Gráfico de entrenamiento de una RNN del tipo aceptador.}
\end{figure}

Por ejemplo, se puede tener una RNN que lee los caracteres de una palabra uno por uno y luego utiliza el estado final para predecir la categoría gramatical de esa palabra. Otro ejemplo es una RNN que lee una oración y, en función del estado final, decide si transmite un sentimiento positivo o negativo.

\section{Patrones de uso de las RNN: Transductor}

Otra opción es tratar la RNN como un transductor, produciendo una salida $\hat{t}_i$ para cada entrada que lee. Este patrón es muy útil para tareas de etiquetado de secuencias (por ejemplo, etiquetado de partes del discurso, reconocimiento de entidades nombradas, etc.). Se calcula una señal de pérdida local (por ejemplo, entropía cruzada) $L_{\text

{local}}(\hat{t}_{i},{t}_{i})$ para cada una de las salidas $\hat{t}_{i}$ basada en una etiqueta verdadera ${t}_{i}$. La pérdida para la secuencia desenrollada será entonces: $L(\hat{t}_{i:n},{t}_{i:n}) = \sum_{i} L_{\text{{local}}}(\hat{t}_{i},{t}_{i})$, o utilizando otra combinación en lugar de la suma, como el promedio o un promedio ponderado.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.25]{pics/transducer.png}
  \caption{Gráfico de entrenamiento de una RNN del tipo transductor.}
\end{figure}

Por ejemplo, un etiquetador de secuencias (por ejemplo, NER, POS), en el que $\vec{x}_{i:n}$ representa las representaciones de características de las $n$ palabras de una oración, y $t_i$ se utiliza para predecir la asignación de etiquetas de la palabra $i$ en función de las palabras $1:i$. Otro ejemplo es el modelado de lenguaje, en el que la secuencia de palabras $x_{1:n}$ se utiliza para predecir una distribución sobre la palabra $(i+1)$-ésima. Los modelos de lenguaje basados en RNN han demostrado tener una perplejidad mucho mejor que los modelos de lenguaje tradicionales.

El uso de las RNN como transductores nos permite relajar la suposición de Markov que se hace tradicionalmente en los modelos de lenguaje y los etiquetadores HMM, y condicionar en todo el historial de predicciones.

\section{Redes neuronales recurrentes bidireccionales (BIRNN)}

Una elaboración útil de una RNN es una red neuronal recurrente bidireccional (también conocida como biRNN). Consideremos la tarea de etiquetado de secuencias en una oración. Una RNN nos permite calcular una función de la $i$-ésima palabra $x_i$ en función de las palabras anteriores $x_{1:i}$, incluyendo la palabra actual. Sin embargo, las palabras siguientes $x_{i+1:n}$ también pueden ser útiles para la predicción. La biRNN nos permite mirar arbitrariamente lejos tanto al pasado como al futuro dentro de la secuencia. Consideremos una secuencia de entrada $\vec{x}_{1:n}$. La biRNN funciona manteniendo dos estados separados, $s_{i}^{f}$ y $s_{i}^{b}$ para cada posición de entrada $i$. El estado hacia adelante $s_{i}^{f}$ se basa en $\vec{x}_1, \vec{x}_2, \dots ,\vec{x}_i$, mientras que el estado hacia atrás $s_{i}^{b}$ se basa en $\vec{x}_n, \vec{x}_{n-1}, \dots ,\vec{x}_i$. Los estados hacia adelante y hacia atrás se generan mediante dos RNN diferentes. La primera RNN $(R^f, O^f)$ recibe la secuencia de entrada $\vec{x}_{1:n}$ tal como está, mientras que la segunda RNN $(R^b , O^b)$ recibe la secuencia de entrada en orden inverso. La representación del estado $\vec{s}_i$ está compuesta tanto por los estados hacia adelante como por los estados hacia atrás. La salida en la posición $i$ se basa en la concatenación de los dos vectores de salida:

\begin{displaymath}
\vec{y}_i = [\vec{y}_{i}^{f};\vec{y}_{i}^{b}]=[O^{f}(s_{i}^{f});O^{b}(s_{i}^{b})]
\end{displaymath}

La salida tiene en cuenta tanto el pasado como el futuro. La codificación biRNN de la palabra $i$ en una secuencia es la concatenación de dos RNN, una que lee la secuencia desde el principio y otra que la lee desde el final. Definimos $biRNN(\vec{x}_{1:n}, i)$ como el vector de salida correspondiente a la $i$-ésima posición de la secuencia:

\begin{displaymath}
biRNN(\vec{x}_{1:n}, i) = \vec{y}_i = [RNN^{f}(\vec{x}_{1:i});RNN^{b}(\vec{x}_{n:i})]
\end{displaymath}

El vector $\vec{y}_i$ se puede utilizar directamente para la predicción o se puede alimentar como parte de la entrada a una red más compleja. Mientras que las dos RNN se ejecutan de forma independiente, los gradientes de error en la posición $i$ fluirán tanto hacia adelante como hacia atrás a través de las dos RNN. Al alimentar el vector $\vec{y}_i$ a través de un MLP antes de la predicción, se mezclarán aún más las señales hacia adelante y hacia atrás.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{pics/biRNN.png}
  \caption{Red neuronal recurrente bidireccional (biRNN).}
\end{figure}

Observa cómo el vector $\vec{y}_4$, correspondiente a la palabra \textbf{saltó}, codifica una ventana infinita alrededor (e incluyendo) el vector de enfoque $\vec{x}_{saltó}$. La biRNN es muy efectiva para tareas de etiquetado, en las que cada vector de entrada corresponde a un vector de salida. También es útil como componente de extracción de características entrenable de propósito general, que se puede utilizar siempre que se requiera una ventana alrededor de una palabra determinada.

\section{Redes neuronales recurrentes multi-capa (apiladas)}

Las RNN se pueden apilar en capas, formando una estructura en forma de cuadrícula. Consideremos $k$ RNN, $RNN_{1},\dots, RNN_{k}$, donde la $j$-ésima RNN tiene estados $\vec{s}_{1:n}^{j}$ y salidas $\vec{y}_{1:n}^{j}$. La entrada para la primera RNN son $\vec{x}_{1:n}$. La entrada de la $j$-ésima RNN ($j\geq 2$) son las salidas de la RNN debajo de ella, $\vec{y}_{1:n}^{j-1}$. La salida de toda la formación es la salida de la última RNN, $\vec{y}_{1:n}^k$. Estas arquitecturas en capas se conocen comúnmente como RNN profundas. No

existe una restricción en la cantidad de capas que se pueden apilar, pero es importante tener en cuenta que agregar más capas puede aumentar la complejidad del modelo y requerir más datos y tiempo de entrenamiento.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{pics/stackedRNN.png}
  \caption{Redes neuronales recurrentes multi-capa (apiladas).}
\end{figure}

La ventaja de las RNN apiladas es que cada capa puede aprender representaciones de características de nivel superior en la secuencia de entrada. Las capas superiores pueden capturar patrones de más largo alcance y dependencias más complejas. Cada capa procesa la secuencia de entrada de manera incremental, y las capas superiores reciben información de las capas inferiores para ayudar a hacer predicciones más precisas. Las RNN apiladas son particularmente efectivas para tareas de modelado del lenguaje y traducción automática.

\section{Long Short-Term Memory (LSTM)}

Una de las limitaciones de las RNN tradicionales es que pueden tener dificultades para capturar dependencias a largo plazo en una secuencia. Las LSTM (Long Short-Term Memory) son una variante de las RNN que se introdujeron para abordar este problema. Las LSTM son capaces de mantener y actualizar información a lo largo de secuencias más largas, lo que las hace especialmente efectivas para tareas en las que las dependencias a largo plazo son importantes.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{pics/LSTM.png}
  \caption{Estructura de una celda LSTM.}
\end{figure}

La estructura central de una celda LSTM es la celda de memoria, que permite a la red almacenar y acceder a información a largo plazo. La celda de memoria está compuesta por una puerta de olvido, una puerta de entrada y una puerta de salida. La puerta de olvido decide qué información de la celda de memoria anterior debe olvidarse. La puerta de entrada determina qué nueva información debe agregarse a la celda de memoria. La puerta de salida controla qué información se extrae de la celda de memoria y se pasa a la siguiente capa de la red. Las operaciones en una celda LSTM son no lineales y se calculan mediante una combinación de multiplicaciones matriciales y funciones de activación no lineales.

Las LSTM han demostrado ser muy efectivas en una amplia gama de tareas, incluido el modelado del lenguaje, la traducción automática y el reconocimiento de voz.

\section{Gated Recurrent Unit (GRU)}

Otra variante popular de las RNN es la Gated Recurrent Unit (GRU). La GRU también aborda el problema de capturar dependencias a largo plazo en una secuencia y se considera una alternativa más simple a las LSTM. La GRU reemplaza la celda de memoria en la LSTM con una sola puerta de actualización y una puerta de reinicio.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{pics/GRU.png}
  \caption{Estructura de una celda GRU.}
\end{figure}

La puerta de actualización en una celda GRU decide cuánta información se debe actualizar en la celda de estado oculta actual. La puerta de reinicio determina cuánta información de la celda de estado oculta anterior debe olvidarse. Estas puertas permiten a la GRU controlar el flujo de información a través del tiempo y actualizar selectivamente la información relevante. Al igual que las LSTM, las GRU también utilizan operaciones no lineales y combinan multiplicaciones matriciales con funciones de activación para calcular los estados ocultos.

La GRU ha demostrado ser efectiva en muchas tareas de procesamiento del lenguaje natural, incluido el modelado del lenguaje, la traducción automática y el reconocimiento de voz.

\section{Conclusiones}

Las redes neuronales recurrentes (RNN) son un tipo de modelo de aprendizaje automático especialmente diseñado para trabajar con datos secuenciales. A diferencia de las redes neuronales convolucionales (CNN), las RNN pueden procesar entradas de longitud variable y capturar dependencias a largo plazo en una secuencia. Las RNN son particularmente útiles en tareas de procesamiento del lenguaje natural, como el etiquetado de secuencias, el modelado del lenguaje y la traducción automática.

Existen varias variantes de las RNN, incluidas las redes Elman o Simple-RNN, las redes neuronales recurrentes bidireccionales (biRNN), las redes neuronales recurrentes multi-capa y las LSTM y GRU. Cada variante tiene sus propias ventajas y se adapta mejor a diferentes tipos de tareas y conjuntos de datos.

La elección de la arquitectura de la RNN depende de la tarea específica que se esté abordando y de las características de los datos. En general, las RNN más complejas, como las LSTM y las GRU, tienden a funcionar mejor en tareas que involucran dependencias a largo plazo, mientras que las RNN más simples, como las redes Elman, pueden ser suficientes para tareas más simples.

El entrenamiento de las RNN implica el despliegue de la recursión a lo largo de la secuencia de entrada, la retropropagación a través del tiempo (BPTT) y la actualización de los parámetros mediante algoritmos de optimización como el descenso de gradiente estocástico (SGD).

En resumen, las RNN son una herramienta poderosa para el procesamiento de datos secuenciales y han demostrado su eficacia en una amplia gama de tareas de procesamiento del lenguaje natural. La elección de la arquitectura de la RNN adecuada y el entrenamiento adecuado de la red son aspectos clave para obtener un buen rendimiento en una tarea específica.

\chapter{Modelos Secuencia a Secuencia y Atención Neuronal}
\label{cap_sec}
\input{cap_sec}


\chapter{Arquitectura de Transformer}
\label{cap_trans}
\input{cap_trans}



        
        
\chapter{Grandes Modelos de Lenguaje}
\label{cap_llm}
\input{cap_llm}






\bibliography{bio}
\bibliographystyle{apalike}

\end{document}
