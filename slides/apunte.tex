\documentclass{book}
\usepackage[spanish]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{dsfont}
\usepackage{graphics}
\usepackage{cases}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{amstext}
\usepackage[ruled,vlined,lined]{algorithm2e}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{epic}
\usepackage{epsfig}
\usepackage{fontenc}
\usepackage{framed,color}
\usepackage{palatino, url, multicol}
\usepackage{hyperref}


\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

\begin{document}


\thispagestyle{empty}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        \includegraphics[width=2cm]{pics/escudocolor.png} % Replace "pics/escudocolor.png" with the filename and path of your logo image
        \vspace{1cm}
        
        {\Huge\textbf{Universidad de Chile}} \\
        \vspace{0.5cm}
        {\Large Departamento de Ciencias de la Computación} \\
        \vspace{2cm}
        
        {\Huge\textbf{Procesamiento de Lenguaje Natural}} \\
        \vspace{0.5cm}
        {\Large Apuntes de Clases} \\
        \vspace{2cm}
        
        {\Large Felipe Bravo-Márquez} \\
        \vspace{2cm}
        
        {\large \today}
    \end{center}
\end{titlepage}

\newpage

\thispagestyle{empty}

\newpage
\pagenumbering{roman}

\tableofcontents 
\newpage

\listoftables
\newpage
\listoffigures
\newpage


\thispagestyle{empty}


\pagenumbering{arabic}

%..................................................

\chapter*{Prefacio}
\addcontentsline{toc}{chapter}{Preface}
Este apunte es el resultado de cinco años de experiencia en la enseñanza del curso de Procesamiento de Lenguaje Natural en la Universidad de Chile\footnote{El repositorio del curso se encuentra en: \url{https://github.com/dccuchile/CC6205}}. El objetivo es proporcionar una introducción a esta disciplina, centrándose en las técnicas y conceptos fundamentales. Se ha realizado un esfuerzo por lograr un equilibrio entre las técnicas tradicionales, como los modelos de lenguaje de N-gramas, Naive Bayes y los modelos ocultos de Markov (HMM), y los enfoques modernos basados en redes neuronales profundas, como los vectores de palabras, las redes neuronales recurrentes (RNN), los transformers y los grandes modelos de lenguaje. Sin embargo, es importante mencionar que existen temas relevantes que no se incluyen en este material, como el etiquetado de datos, las gramáticas, las técnicas de parsing, así como discusiones sobre tareas más específicas como el question answering.

El contenido se ha recopilado de diversas fuentes. Los temas relacionados con las redes neuronales se basan principalmente en el libro "Neural Network Methods for Natural Language Processing" de Goldberg \cite{goldberg2017neural}. Los temas no relacionados con las redes neuronales, como los modelos probabilísticos del lenguaje, Naive Bayes y HMM, se han tomado del curso de Michael Collins de Columbia \cite{collins2013language} y del borrador de la tercera edición del libro de Dan Jurafsky y James H. Martin \cite{JurafskyBook}. Además, algunos capítulos se han adaptado de tutoriales en línea y otros cursos, como el curso de Stanford de Christopher Manning\footnote{\url{http://web.stanford.edu/class/cs224n/}}. Las imágenes utilizadas se han extraído intencionalmente directamente de sus fuentes correspondientes. Algún día se diseñarán imágenes propias.

\chapter{Introducción}
\label{cap:intro}
\input{cap_intro}



\chapter{Modelo de Espacio Vectorial y Recuperación de Información}
\label{cap_ir}
\input{cap_ir}


\chapter{Modelos de Lenguaje Probabilísticos}
\label{cap_plm}
\input{cap_plm}


\chapter{Text Classification and Naïve Bayes}
\label{cap_nb}
\input{cap_nb}


\chapter{Modelos Lineales}
\label{cap_lineales}
\input{cap_lineales}


\chapter{Redes Neuronales}
\label{cap_redes}
\input{cap_redes}

\chapter{Vectores de Palabra}
\label{cap_embeddings}
\input{cap_embeddings}

\chapter{Etiquetado de Secuencias}
\label{cap_etisec}
\input{cap_etisec}



\chapter{Redes Neuronales Convolucionales}
\label{cap_cnn}
\input{cap_cnn}


\chapter{Redes Neuronales Recurrentes}
\label{cap_rnn}
\input{cap_rnn}



\chapter{Modelos Secuencia a Secuencia y Atención Neuronal}
\label{cap_sec}
%\input{cap_sec}

%Translate this Latex book chapter of an NLP book on sequence to sequence models to Spanish. Output in Latex format. Rearrange bullet points (\items) into full paragraphs. Make sure that sentences are connected in a more fluid way as they come. Elaborate on incomplete ideas where appropriate. Add caption and center images.


\section{Modelos de lenguaje y generación de lenguaje}
\begin{itemize}
\item La modelización del lenguaje es la tarea de asignar una probabilidad a las oraciones en un lenguaje.
\item Por ejemplo, ¿cuál es la probabilidad de ver la oración "el perro perezoso ladró ruidosamente"?
\item La tarea se puede formular como la tarea de predecir la probabilidad de ver una palabra condicionada a las palabras anteriores:
\begin{displaymath}
P(w_i | w_1, w_2, \cdots, w_{i-1}) = \frac{P(w_1, w_2, \cdots, w_{i-1}, w_i)}{P(w_1, w_2, \cdots, w_{i-1})}
\end{displaymath}
\item Las RNN se pueden utilizar para entrenar modelos de lenguaje vinculando la salida en el tiempo $i$ con su entrada en el tiempo $i + 1$.
\item Esta red se puede utilizar para generar secuencias de palabras o frases aleatorias.
\item Proceso de generación: predecir una distribución de probabilidad sobre la primera palabra condicionada al símbolo de inicio y seleccionar una palabra aleatoria de acuerdo con la distribución predicha.
\item Luego, predecir una distribución de probabilidad sobre la segunda palabra condicionada a la primera, y así sucesivamente, hasta predecir el símbolo de fin de secuencia $</$s$>$.
\item Después de predecir una distribución sobre los siguientes símbolos de salida $P(t_i = k | t_{1:i-1})$, se elige un token $t_i$ y su vector de incrustación correspondiente se alimenta como entrada al siguiente paso.
         \begin{figure}[h]
        	\centering
        	\includegraphics[scale = 0.25]{pics/generator.png}
        	\caption{Arquitectura de generación de lenguaje con una RNN.}
        \end{figure}
\item Teacher-forcing: durante el \textbf{entrenamiento}, se alimenta al generador con la palabra anterior verdadera, incluso si su propia predicción le asignó una pequeña probabilidad.
\item Es probable que el generador haya generado una palabra diferente en este estado durante la \textbf{prueba}.
\end{itemize}





\section{Problemas de secuencia a secuencia}
Casi cualquier tarea en NLP se puede formular como un problema de secuencia a secuencia (o generación condicionada), es decir, generar secuencias de salida a partir de secuencias de entrada. Las secuencias de entrada y salida pueden tener longitudes diferentes.
\begin{itemize}
\item Traducción automática: de un idioma fuente a un idioma objetivo.
\item Resumen: de texto largo a texto corto.
\item Diálogo (chatbots): de las intervenciones anteriores a la siguiente intervención.
\end{itemize}

\subsection{Generación condicionada}
\begin{itemize}
\item Si bien utilizar la RNN como generador es un ejercicio interesante para demostrar su fortaleza, el verdadero poder del generador de RNN se revela cuando se pasa a una generación condicionada o un marco de codificador-decodificador.
\item Idea central: usar dos RNN.
\item Codificador: se utiliza una RNN para codificar la entrada fuente en un vector $\overrightarrow{c}$.
\item Decodificador: se utiliza otra RNN para decodificar la salida del codificador y generar la salida objetivo.
\item En cada etapa del proceso de generación, el vector de contexto $\overrightarrow{c}$ se concatena a la entrada $\hat{t}_j$ y la concatenación se alimenta a la RNN.
\begin{frame}{Marco codificador-decodificador}
         \begin{figure}[h]
        	\includegraphics[scale = 0.32]{pics/seqseq.png}
        \end{figure}
\end{frame}
\item Esta configuración es útil para mapear secuencias de longitud $n$ a secuencias de longitud $m$.
\item El codificador resume la oración fuente en un vector $\vec{c}$.
\item Luego, la RNN del decodificador se utiliza para predecir (utilizando un objetivo de modelado del lenguaje) las palabras de la secuencia objetivo condicionadas tanto a las palabras predichas anteriormente como a la oración codificada $\vec{c}$.
\item Las RNN del codificador y del decodificador se entrenan conjuntamente.
\item La supervisión solo ocurre para la RNN del decodificador, pero los gradientes se propagan hasta la RNN del codificador.
\end{itemize}


\paragraph{Gráfico de entrenamiento de secuencia a secuencia}
         \begin{figure}[h]
        	\includegraphics[scale = 0.35]{pics/seq2se2train.png}
        \end{figure}





\paragraph{Traducción automática neuronal}
         \begin{figure}[h]
        	\includegraphics[scale = 0.25]{pics/mt.png}
        \end{figure}

\paragraph{Progreso de BLEU en traducción automática}
         \begin{figure}[h]
        	\includegraphics[scale = 0.35]{pics/nmt_progress.png}
        \end{figure}
[Edinburgh En-De WMT]
\footnotetext{fuente: \url{http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf}}


\section{Enfoques de decodificación}
\begin{itemize}
\item El decodificador tiene como objetivo generar la secuencia de salida con la puntuación máxima (o probabilidad máxima), es decir, de modo que se maximice $\sum_{i=1}^{n}P(\hat{t}_i | \hat{t}_{1:i-1})$.
\item La naturaleza no markoviana de la RNN significa que la función de probabilidad no se puede descomponer en factores que permitan una búsqueda exacta utilizando la programación dinámica estándar.
\item Búsqueda exacta: encontrar la secuencia óptima requiere evaluar todas las secuencias posibles (computacionalmente prohibitivo).
\item Por lo tanto, solo tiene sentido resolver aproximadamente el problema de optimización anterior.
\item Búsqueda por greedy: elegir la predicción (palabra) de mayor puntuación en cada paso.
\item Esto puede dar como resultado una probabilidad global subóptima y llevar a prefijos seguidos de eventos de baja probabilidad.
\end{itemize}


\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{pics/greedysearch.png}
  \caption{Búsqueda greedy en la decodificación de secuencia a secuencia.}
\end{figure}



\subsection{Búsqueda Beam}
\begin{itemize}
\item La búsqueda Beam interpolan entre la búsqueda exacta y la búsqueda greedy al cambiar el tamaño $K$ de las hipótesis mantenidas durante todo el procedimiento de búsqueda \cite{cho2015natural}.
\item El algoritmo de búsqueda Beam funciona en etapas.
\item Primero se eligen las $K$ palabras de inicio con la probabilidad más alta.
\item En cada paso, cada secuencia candidata se expande con todas las posibles siguientes etapas.
\item Cada paso candidato se puntúa.
\item Se conservan las $K$ secuencias con las probabilidades más probables y se eliminan todas las demás candidatas.
\item El proceso de búsqueda puede detenerse para cada candidato por separado ya sea alcanzando una longitud máxima, alcanzando un token de fin de secuencia o alcanzando una probabilidad umbral.
\item Se selecciona la oración con la probabilidad global más alta.
\end{itemize}

\footnotetext{Más información en: \url{https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/}}

\section{Generación condicionada con atención}
\begin{itemize}
\item En las redes codificador-decodificador, la oración de entrada se codifica en un solo vector, que luego se utiliza como contexto de condicionamiento para un generador RNN.
\item Esta arquitectura obliga al vector codificado $\vec{c}$ a contener toda la información requerida para la generación.
\item ¡No funciona bien para oraciones largas!
\item También requiere que el generador pueda extraer esta información del vector de longitud fija.
\item "¡No puedes meter el significado de una maldita oración en un solo maldito vector!" -Raymond Mooney
\item Esta arquitectura se puede mejorar sustancialmente (en muchos casos) mediante la adición de un mecanismo de atención.
\item El mecanismo de atención intenta resolver este problema permitiendo que el decodificador "mire hacia atrás" los estados ocultos del codificador según su estado actual.
\item La oración de entrada (una secuencia de entrada de longitud $n$, $\vec{x}_{1:n}$) se codifica utilizando una biRNN como una secuencia de vectores $\vec{c}_{1:n}$.
\item El decodificador utiliza un mecanismo de atención suave para decidir en qué partes de la entrada codificada debe enfocarse.
\item En cada etapa $j$, el decodificador ve un promedio ponderado de los vectores $\vec{c}_{1:n}$, donde los pesos de atención ($\vec{\alpha}^j$) son elegidos por el mecanismo de atención.
\begin{displaymath}
\vec{c}^j = \sum_{i=1}^{n} \vec{\alpha}_{[i]}^{j}\cdot \vec{c}_i
\end{displaymath}
\item Los elementos de $\vec{\alpha}^j$ son todos positivos y suman uno.
\item Se producen pesos de atención no normalizados ($\bar{\alpha}_{[i]}^j$) teniendo en cuenta el estado del decodificador en el tiempo $j$ ($\vec{s}_j$) y cada uno de los vectores $\vec{c}_i$.
\item Se pueden obtener de varias maneras, básicamente cualquier función diferenciable que devuelva un escalar de dos vectores $\vec{s}_j$ y $\vec{c}_i$ se puede utilizar.
\item El enfoque más simple es un producto escalar: $\bar{\alpha}_{[i]}^j = \vec{s}_j \cdot \vec{c}_i$.
\item El que utilizaremos en estas diapositivas es la atención aditiva, que utiliza un perceptrón multicapa: $\bar{\alpha}_{[i]}^j = MLP^{att}([\vec{s}_j;\vec{c}_i]) = \vec{v} \cdot \operatorname{tanh}([\vec{s}_j;\vec{c}_i]U +\vec{b})$
\item Luego, estos pesos no normalizados se normalizan en una distribución de probabilidad utilizando la función softmax.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{pics/atten_formula.png}
\end{figure}

\item El codificador, el decodificador y el mecanismo de atención se entrenan conjuntamente para interactuar bien entre sí.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{pics/encdecattention.png}
  \caption{Arquitectura de un modelo codificador-decodificador con atención.}
\end{figure}

La generación de toda la secuencia a secuencia con atención se define como:

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{pics/attentionformula.png}
\end{figure}

\begin{itemize}
\item ¿Por qué usar el codificador biRNN para traducir la secuencia de condicionamiento $\vec{x}_{1:n}$ en los vectores de contexto $\vec{c}_{1:n}$?
\item ¿Por qué no simplemente centrarse directamente en las entradas (incrustaciones de palabras) $MLP^{att}([\vec{s}_j;\vec{x}_i])$?
\item Podríamos hacerlo, pero obtenemos beneficios importantes del proceso de codificación.
\item En primer lugar, los vectores biRNN $\vec{c}_i$ representan los elementos $\vec{x}_i$ en su contexto oracional.
\item Contexto oracional: una ventana enfocada alrededor del elemento de entrada $\vec{x}_i$ y no el elemento en sí.
\item En segundo lugar, al tener un componente de codificación entrenable que se entrena conjuntamente con el decodificador, el codificador y el decodificador evolucionan juntos.
\item Por lo tanto, la red puede aprender a codificar propiedades relevantes de la entrada que son útiles para la decodificación y que pueden no estar presentes en la secuencia fuente $\vec{x}_{1:n}$ directamente.
\end{itemize}



\section{Atención y alineaciones de palabras}
En el contexto de la traducción automática, se puede pensar que $MLP^{att}$ calcula una alineación suave entre el estado actual del decodificador $\vec{s}_j$ (capturando las palabras extranjeras recién producidas) y cada uno de los componentes de la oración fuente $\vec{c}_i$.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.28]{pics/attention-alignment.png}
  \caption{Fuente: \cite{cho2015describing}}
\end{figure}


\section{Otros tipos de atención}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.32]{pics/types_of_attention.png}
  \caption{Fuente: \url{https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html}}
\end{figure}

\chapter{Arquitectura de Transformer}
\label{cap_trans}
\input{cap_trans}



        
        
\chapter{Grandes Modelos de Lenguaje}
\label{cap_llm}
\input{cap_llm}






\bibliography{bio}
\bibliographystyle{apalike}

\end{document}
