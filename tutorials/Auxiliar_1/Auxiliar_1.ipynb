{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 1 \n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:59:04.618627Z",
     "start_time": "2019-07-24T20:59:04.602978Z"
    }
   },
   "source": [
    "## üìö Objetivos de la clase üìö\n",
    "\n",
    "El objetivo principal de esta clase es introducirlos a la clasificaci√≥n de texto en NLP. \n",
    "Para esto, implementaremos varios modelos de clasificaci√≥n destinados a **predecir la categor√≠a de noticias de la radio biobio**.\n",
    "\n",
    "Los modelos y m√©todos que usaremos ser√°n las vistas en las clases anteriores: \n",
    "\n",
    "- Preprocesamiento: Tokenizaci√≥n, Stemming, Lematizaci√≥n y eliminaci√≥n de Stop Words.\n",
    "- Bag of Words.\n",
    "- Claisifcador de Bayes .\n",
    "- Logistic regression.\n",
    "\n",
    "La clase estar√° enfocada en utilizar las siguientes librer√≠as (muy utilizadas en NLP):\n",
    "\n",
    "- Pandas\n",
    "- Scikit-Learn\n",
    "- Spacy\n",
    "- NLTK\n",
    "\n",
    "Una vez resuelto, pueden utilizar cualquier parte del c√≥digo que les parezca prudente para la tarea 1 (que tambi√©n es de clasificaci√≥n de texto! üòä).\n",
    "\n",
    "\n",
    "El notebook del auxiliar ya ejecutado se encuentra en el [github](https://github.com/dccuchile/CC6205/tree/master/tutorials) del curso (Recuerden dejar su Star ‚≠êüòâ!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las librer√≠as\n",
    "\n",
    "### En local: Python y Conda\n",
    "\n",
    "Primero, si es que no tienen aun las librer√≠as, hay que instalarlas.\n",
    "Recuerden que usaremos `python 3.7` junto a `conda` como gestor de paquetes para el curso.\n",
    "\n",
    "Este pueden descargarlo e instalenlo desde aqu√≠ : [üêç Anaconda üêç](https://www.anaconda.com/distribution/).\n",
    "\n",
    "Para instalar las librer√≠as, ejecutar en una consola üíª:\n",
    "\n",
    "```cmd\n",
    "conda install pandas scikit-learn ntkl spacy \n",
    "```\n",
    "Y luego descargar el modelo de spacy en espa√±ol: \n",
    "\n",
    "```cmd\n",
    "python -m spacy download es_core_news_sm\n",
    "```\n",
    "\n",
    "Si saben un poco mas de anaconda, pueden instalar sus paquetes en un ambiente exlcusivo para el curso. Pero no es necesario!! Mas informaci√≥n [aqu√≠](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "\n",
    "### Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:27:57.336521Z",
     "start_time": "2020-04-23T13:27:55.063682Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:27:57.451614Z",
     "start_time": "2020-04-23T13:27:57.416126Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:02:54.853968Z",
     "start_time": "2020-04-23T14:02:53.389134Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=['ner', 'parser', 'tagger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:34:52.757813Z",
     "start_time": "2020-04-22T22:34:52.747748Z"
    }
   },
   "source": [
    "## Clasificaci√≥n de Texto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øCu√°l de estos emails es SPAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![respuesta spam](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/spam.PNG \"Email: Respuesta de cuales son spam y cuales no\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T23:05:11.304836Z",
     "start_time": "2020-04-22T23:05:11.285260Z"
    }
   },
   "source": [
    "La respuesta es..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![respuesta spam](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/spam_2.PNG \"Email: Respuesta de cuales son spam y cuales no\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T23:05:28.615278Z",
     "start_time": "2020-04-22T23:05:28.585246Z"
    }
   },
   "source": [
    "Pero, estos tambi√©n pueden representar otros tipos de categor√≠as..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![categor√≠as de comunicados](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/emails_clases.PNG \"Email: Cual es la clase del email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "¬øC√≥mo habr√°n encontrado la pel√≠cula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![¬øC√≥mo encontraron la pel√≠cula?](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/limpiapiscinas.PNG \"Email: ¬øC√≥mo encontraron la pel√≠cula?\")\n",
    "\n",
    "\n",
    "Cr√©ditos a [OndaMedia](https://ondamedia.cl/). Pel√≠culas y mater√≠al audiovisual chileno gratis, muy recomendado!\n",
    "\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, \n",
    "\n",
    "### ¬øQu√© es la clasificaci√≥n de texto?\n",
    "\n",
    "La clasificaci√≥n de texto consiste en tomar distintos textos y asignarles alguna clase. Dichas clases var√≠an seg√∫n la task que queramos resolver. Por ejemplo:\n",
    "\n",
    "    - Detectar emails SPAM -> SPAM, NO SPAM\n",
    "    - Reviews de Peliculas -> Buena, Mas o menos, Mala, Mal√≠sima, Brutalmente mala.\n",
    "    - An√°lisis de sentimientos de tweets: Felicidad, Tristeza, Enojo, Ira,...\n",
    "    - Detectar Fake News -> Es, No es\n",
    "    - Lenguaje del texto -> Espa√±ol, Ingl√©s, Chino,...\n",
    "    - Categor√≠a de una noticia -> Nacional, Internacional, Econom√≠a, Sociedad, Opini√≥n...\n",
    "    - Autor de un texto -> Cada autor es una clase distinta.\n",
    "Se define formalmente como:\n",
    "\n",
    "- Input: \n",
    "\n",
    "    - Un documento $d$\n",
    "    - Un conjunto fijo de clases $c_1, c_2, ..., c_j$\n",
    "\n",
    "- Output: \n",
    "    \n",
    "    - Una clase $c \\in C$ para el documento \n",
    "    \n",
    " \n",
    "Hay dos clases de m√©todos para resolver estos problemas: \n",
    "\n",
    "1. **Hand-coded Rules ü§ô**: \n",
    "\n",
    "    Establecemos a mano las reglas que permiten detectar las clases.\n",
    "    \n",
    "\n",
    "2. **Supervised Machine Learning üíª**:\n",
    "   \n",
    "    Entrenamos clasificadores a partir de muchos ejemplos de documentos etiquetados a mano. \n",
    "    \n",
    "\n",
    "----------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øQu√© haremos a continuaci√≥n?\n",
    "\n",
    "\n",
    "C√≥mo dijimos al comienzo, en este auxiliar crearemos un sistema que nos permita clasificar noticias de la radio biobio en 20 categor√≠as o t√≥picos:\n",
    "\n",
    "```python\n",
    "[\n",
    "    'america-latina', 'eeuu', 'europa', 'chile', 'region-metropolitana',\n",
    "    'region-del-bio-bio', 'negocios-y-empresas', 'region-de-los-lagos',\n",
    "    'actualidad-economica', 'region-de-valparaiso', 'region-de-la-araucania',\n",
    "    'curiosidades', 'asia', 'region-de-los-rios', 'entrevistas', 'debates',\n",
    "    'mediooriente', 'viral', 'animales', 'tu-bolsillo'\n",
    "]\n",
    "```\n",
    "\n",
    "Los pasos a seguir ser√°n: \n",
    "\n",
    "1. Primero que nada, descargaremos los datos con los que trabajaremos.\n",
    "\n",
    "2. Luego, crearemos el sistema mas b√°sico. Este consiste en transformar nuestro texto a `Bag of Words (BoW)` y luego, usar esos vectores para entrenar un clasificador. Este sistema nos puede entregar un muy buen baseline para comenzar a mejorar.\n",
    "\n",
    "3. Evaluaremos nuestro clasificador seg√∫n las m√©tricas.\n",
    "\n",
    "4. A continuaci√≥n, veremos como mejorar aun mas nuestros resultados. Para esto agregaremos muchas mas t√©cnicas vistas en c√°tedra, tales como el preprocesamiento de texto y probar con clasificadores a√∫n mas sofisticados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datasets \n",
    "\n",
    "\n",
    "Los datos que usaremos son 5000 documentos con noticias dividas en 20 categor√≠as. Las noticias fueron obtenidas desde la p√°gina de la radio biobio.\n",
    "Cada categor√≠a contiene 250 documentos (noticias). \n",
    "\n",
    "Los cargaremos directamente desde el github del curso utilizando la librer√≠a `pandas` üêº: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:51.139822Z",
     "start_time": "2020-04-23T13:55:07.564431Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_json('https://github.com/dccuchile/CC6205/releases/download/Data/biobio_clean.bz2')\n",
    "dataset_r = dataset.copy(deep=True) # lo dejaremos ah√≠ por si acaso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:51.464346Z",
     "start_time": "2020-04-23T13:55:51.319447Z"
    }
   },
   "outputs": [],
   "source": [
    "# creamos una nueva columna titulo y contenido.\n",
    "content = dataset['title'] + '. ' + dataset['content'] \n",
    "# obtenemos las clases\n",
    "subcategory = dataset.subcategory\n",
    "\n",
    "# dejamos en el dataset solo contenido de la noticia y categoria\n",
    "dataset = pd.DataFrame({'content': content, 'category': subcategory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:51.669793Z",
     "start_time": "2020-04-23T13:55:51.614588Z"
    }
   },
   "outputs": [],
   "source": [
    "# El n√∫mero de noticias por clase lo pueden cambiar despues modificando la constante NUM_SAMPLES.\n",
    "# noten que el n√∫mero de noticias en el dataset original por categor√≠a est√° desbalanceada.\n",
    "# sample intentar√° sacar la mayor cantidad de ejemplos y retornar√° siempre, incluso si devuelve \n",
    "# menos de los que le pidieron.\n",
    "\n",
    "NUM_SAMPLES = 250\n",
    "\n",
    "categorias_seleccionadas = [\n",
    "    'america-latina', 'eeuu', 'europa', 'chile', 'region-metropolitana',\n",
    "    'region-del-bio-bio', 'negocios-y-empresas', 'region-de-los-lagos',\n",
    "    'actualidad-economica', 'region-de-valparaiso', 'region-de-la-araucania',\n",
    "    'curiosidades', 'asia', 'region-de-los-rios', 'entrevistas', 'debates',\n",
    "    'mediooriente', 'viral', 'animales', 'tu-bolsillo'\n",
    "]\n",
    "\n",
    "# filtrar solo categorias seleccionadas\n",
    "dataset = dataset[dataset['category'].isin(categorias_seleccionadas)]\n",
    "\n",
    "# balancear clases\n",
    "g = dataset.groupby('category')\n",
    "dataset = pd.DataFrame(\n",
    "    g.apply(lambda x: x.sample(NUM_SAMPLES).reset_index(drop=True))).reset_index(\n",
    "        drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:51.859287Z",
     "start_time": "2020-04-23T13:55:51.840573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region-de-valparaiso      250\n",
       "europa                    250\n",
       "region-metropolitana      250\n",
       "animales                  250\n",
       "region-de-la-araucania    250\n",
       "curiosidades              250\n",
       "negocios-y-empresas       250\n",
       "america-latina            250\n",
       "region-de-los-lagos       250\n",
       "tu-bolsillo               250\n",
       "region-de-los-rios        250\n",
       "eeuu                      250\n",
       "mediooriente              250\n",
       "chile                     250\n",
       "actualidad-economica      250\n",
       "entrevistas               250\n",
       "region-del-bio-bio        250\n",
       "asia                      250\n",
       "viral                     250\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as√≠ qued√≥ nuestro dataset:\n",
    "dataset.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos unos cuantos ejemplos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:52.039309Z",
     "start_time": "2020-04-23T13:55:52.019568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>Exdirector del Sename llama a mejorar Ley de a...</td>\n",
       "      <td>entrevistas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>Cuidador de ni√±o asegura que no lo devolver√° a...</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>Casinos tributaron m√°s de $10 mil millones en ...</td>\n",
       "      <td>negocios-y-empresas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>Barco con 233 migrantes rescatados atraca en M...</td>\n",
       "      <td>europa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>Liberan ranking mundial de aerol√≠neas 2018: un...</td>\n",
       "      <td>negocios-y-empresas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>Anciana abandonada en Valdivia: vecinas denunc...</td>\n",
       "      <td>region-de-los-rios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>Vecinos de Corral aseguran que la bah√≠a estar√≠...</td>\n",
       "      <td>region-de-los-rios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>7 tips que necesitas saber para que tus mascot...</td>\n",
       "      <td>animales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>Enfrentamientos en la UdeC dejan un carabinero...</td>\n",
       "      <td>region-del-bio-bio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>Audio 8D: descubre el sonido que har√° volar tu...</td>\n",
       "      <td>viral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content              category\n",
       "1859  Exdirector del Sename llama a mejorar Ley de a...           entrevistas\n",
       "3586  Cuidador de ni√±o asegura que no lo devolver√° a...  region-de-valparaiso\n",
       "2601  Casinos tributaron m√°s de $10 mil millones en ...   negocios-y-empresas\n",
       "2163  Barco con 233 migrantes rescatados atraca en M...                europa\n",
       "2728  Liberan ranking mundial de aerol√≠neas 2018: un...   negocios-y-empresas\n",
       "3325  Anciana abandonada en Valdivia: vecinas denunc...    region-de-los-rios\n",
       "3353  Vecinos de Corral aseguran que la bah√≠a estar√≠...    region-de-los-rios\n",
       "512   7 tips que necesitas saber para que tus mascot...              animales\n",
       "3815  Enfrentamientos en la UdeC dejan un carabinero...    region-del-bio-bio\n",
       "4569  Audio 8D: descubre el sonido que har√° volar tu...                 viral"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, el procedimiento estandar de introducci√≥n a la programaci√≥n: **Dividir nuestros conjuntos en train y test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:52.234309Z",
     "start_time": "2020-04-23T13:55:52.214245Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
    "                                                    dataset.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:32:10.797379Z",
     "start_time": "2020-04-23T00:32:10.777275Z"
    }
   },
   "source": [
    "### Nuestro primer sistema de clasificaci√≥n\n",
    "\n",
    "\n",
    "Ahora que tenemos cargado el dataset, podemos implementar nuestro clasificador!\n",
    "\n",
    "Para esto, usaremos 3 herramientas fundamentales de scikit-learn: un `pipeline`, `CountVectorizer` y `MultinomialNB`.\n",
    "\n",
    "#### Pipeline \n",
    "\n",
    "\n",
    "Un [`pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) es la definici√≥n de los procesos que llevar√° a cabo el sistema que creemos. Nos permite tener unificados todos los procesos a la vez que simplifica el c√≥digo de nuestro sistema.\n",
    "\n",
    "\n",
    "En nuestro caso, el pipeline ser√°:\n",
    "\n",
    "    Dataset -> Bag of Words -> NaiveBayes Clf\n",
    "\n",
    "\n",
    "#### Bag of Words y CountVectorizer üéí \n",
    "\n",
    "\n",
    "¬øQu√© era Bag of Words?\n",
    "\n",
    "Es un modelo en donde transformamos cada una de las oraciones de nuestro dataset en vectores. Cada vector contiene una columna por cada palabra / **token** del vocabulario. Al procesar el dataset, cada oraci√≥n es mapeada a un vector que cuenta las apariciones de cada una de sus tokens. \n",
    "\n",
    "Referencia: [BoW en wikipedia](https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras)\n",
    "\n",
    "**Un peque√±o ejemplo**\n",
    "\n",
    "Supongamos que nuestro tokenizador solo separa por espacios.\n",
    "\n",
    "    - Doc1 : 'I love dogs'\n",
    "    - Doc2: 'I hate dogs and knitting.\n",
    "    - Doc3: 'Knitting is my hobby and my passion.\n",
    "\n",
    "El bag of words quedar√≠a:\n",
    "\n",
    "<img src=\"https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg\" alt=\"BoW\" style=\"width: 600px;\"/>\n",
    "\n",
    "`CountVectorizer` es la clase de `scikit` que transformar√° nuestro texto a Bag of Words. Fijense que es tremendamente √∫til tenerla dentro de un pipeline ya que fija en un comienzo el vocabulario que tendr√° el Bag of Words, evitando discordancias entre los vectores del conjunto de entrenamiento y el de prueba.\n",
    "\n",
    "#### MultinomialNB\n",
    "\n",
    "Vamos a explicar el cl√°sificador de Bayes enfocados en nuestro problema:\n",
    "\n",
    "Escojamos un noticia, por ejemplo $d$ = `'7 tips que necesitas saber para que tus mascota...'`\n",
    "\n",
    "Y nuestro conjunto de clases $C$=`{america-latina, eeuu, chile, ..., virales, animales}`\n",
    "\n",
    "**Teorema de Bayes**\n",
    "\n",
    "Podemos usar el teorema de bayes para calcular la probabilidad de que una noticia pertenezca a una de nuestras clases como:\n",
    "\n",
    "$$p( c_i | d) = \\frac{p(d | c_i) * p(c_i)}{p(d)}$$\n",
    "\n",
    "\n",
    "Lo que puede ser escrito en nuestro ejemplo como: \n",
    "\n",
    "\n",
    "p( `america-latina` | `7 tips...`) = p( `7 tips...` | `america-latina`) * p(`america-latina`) /  p(`7 tips...`)\n",
    "\n",
    "**Clasificaci√≥n**\n",
    "\n",
    "\n",
    "Por lo tanto, la clase de cada noticia quedar√° representada simplemente como la m√°xima probabilidad que obtengamos al calcular bayes para todas las clases $c_i$, lo que puede ser representado como un argmax:\n",
    "\n",
    "$$argmax_{c_i \\in C} p(c_i|d)$$\n",
    "\n",
    "Ahora, despejemos un poco la ecuaci√≥n:\n",
    "\n",
    "1. Primero, veamos el denominador p(d) no nos entrega nada de informaci√≥n, ya que todas las noticias son distintas. Es decir $p(d) = \\frac{1}{numero\\ de\\ docs}$\n",
    "\n",
    "$$argmax_{c_i \\in C} p(c_i|d) = argmax_{c_i \\in C} p(d | c_i) * p(c_i)$$\n",
    "\n",
    "2. Por otra parte, como nuestras clases est√°n balanceadas, $p(c_i) = 250/5000 = 1 / 20$ . Nota que esto no siempre ocurre. Como puede pasar esto, dejaremos por ahora ese t√©rmino dentro de nuestra ecuaci√≥n.\n",
    "\n",
    "Ahora ¬øC√≥mo calculamos $p(d | c_i)$?:\n",
    "\n",
    "1. Primero, separamos el documento por tokens. Menos mal que ya lo hab√≠amos hecho conviertiendo nuestros documentos BoW...\n",
    "\n",
    "$$p(d | c_i) = p(x_1, x_2, x_n | c_i)$$\n",
    "\n",
    "2. Ahora, asumimos que las probabilidades de que cada una de las palabras pertenezca a la clase c_i es independiente de cualquier otra, es decir: \n",
    "\n",
    "$$ p(x_1, x_2, x_n | c_i) = p(x_1 | c_i) * p(x_2 | c_i) * ... * p(x_n | c_i)$$\n",
    "\n",
    "Observen que ahora, dado que asumimos esto, el orden de las palabras no importa. Otro punto mas a favor de usar BoW.\n",
    "\n",
    "**Entrenamiento ü•ã**\n",
    "\n",
    "Y aqu√≠ viene la parte del entrenamiento del modelo: para calcular $p(x | c_i)$ simplemente contamos la cantidad de veces que aparece esa palabra en los documentos de entrenamiento de la clase c_i. O sea:\n",
    "\n",
    "$$p(x | c_i) = \\frac{count(x, c_i)}{\\sum_{x_i \\in Vocab} count(x_i, c_i)}$$\n",
    "\n",
    "Qu√© pasa si hay una palabra que no vimos en el entrenamiento de la clase c_i?\n",
    "\n",
    "$p(x | c_i) = 0$ y se anula todo!\n",
    "\n",
    "Podemos aplicar suavizamientos para evitar este tipo de problemas\n",
    "\n",
    "$$p(x | c_i) = \\frac{count(x, c_i) + 1 }{\\sum_{x_j \\in Vocab} count(x_j, c_i) + |Vocab|}$$\n",
    "\n",
    "\n",
    "\n",
    "Juntando lo anterior, nuestro problema ahora se transforma a :\n",
    "\n",
    "$$c = argmax_{c_i \\in C} \\prod_{x_j \\in d} P(x_j | c_i) * p(c_i)$$\n",
    "\n",
    "Sin embargo, estas probabilidades son muy pero muy peque√±as cuando tenemos vectores Bag of Words muy grandes. Imaginense un vocabulario de 100.000 palabras. Adem√°s, multiplicar es muy complicado ‚ùåüò•\n",
    "\n",
    "As√≠ que convertimos nuestra multiplicatoria en una suma de logaritmos!\n",
    "\n",
    "$$c = argmax_{c_i \\in C}\\  log(p(c_i)) + \\sum_{x_i \\in d} log(P(x_i | c_i))$$\n",
    "\n",
    "\n",
    "\n",
    "Y con eso, ya tenemos definido como va a funcionar nuestro clasificador. Si bien, existen muchos mas clasificadores, este ya es lo suficientemente potente como para lograr muy buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:44:48.540062Z",
     "start_time": "2020-04-23T00:44:48.536046Z"
    }
   },
   "source": [
    "------------------------\n",
    "#### Creemos el clasificador üß™\n",
    "\n",
    "**Primero, definimos el pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:52.434405Z",
     "start_time": "2020-04-23T13:55:52.414283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer()  \n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf = MultinomialNB()   \n",
    "\n",
    "# Creamos el pipeline\n",
    "text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Luego, lo entrenamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:55:53.835214Z",
     "start_time": "2020-04-23T13:55:52.579576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos nuestro pipeline\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Y predecimos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:56:36.555111Z",
     "start_time": "2020-04-23T13:56:36.014456Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T00:44:29.825754Z",
     "start_time": "2020-04-23T00:44:29.805883Z"
    }
   },
   "source": [
    "**Veamos como nos fue:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:56:38.334663Z",
     "start_time": "2020-04-23T13:56:38.309320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>predicted category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>Vecinos de Melefqu√©n exigen claridad a las aut...</td>\n",
       "      <td>region-de-los-rios</td>\n",
       "      <td>region-de-los-rios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>Idea de legislar reforma a las pensiones se vo...</td>\n",
       "      <td>chile</td>\n",
       "      <td>chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>Due√±o busca a los responsables del asesinato d...</td>\n",
       "      <td>animales</td>\n",
       "      <td>animales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>Desconocidos rompen vidrios y lanzan pintura c...</td>\n",
       "      <td>region-del-bio-bio</td>\n",
       "      <td>region-del-bio-bio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Mario: el enternecedor ni√±o maestro de las ven...</td>\n",
       "      <td>viral</td>\n",
       "      <td>viral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>Camioneros anuncian querella criminal tras que...</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>Ir√°n acus√≥ a Israel de inventar pretextos para...</td>\n",
       "      <td>mediooriente</td>\n",
       "      <td>mediooriente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3576</th>\n",
       "      <td>Pronostican bajas temperaturas en Valpara√≠so t...</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3570</th>\n",
       "      <td>Menor de 12 a√±os queda con fractura expuesta t...</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>Migrantes indocumentados esperan angustiados l...</td>\n",
       "      <td>eeuu</td>\n",
       "      <td>eeuu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content              category  \\\n",
       "3467  Vecinos de Melefqu√©n exigen claridad a las aut...    region-de-los-rios   \n",
       "1200  Idea de legislar reforma a las pensiones se vo...                 chile   \n",
       "642   Due√±o busca a los responsables del asesinato d...              animales   \n",
       "3938  Desconocidos rompen vidrios y lanzan pintura c...    region-del-bio-bio   \n",
       "4561  Mario: el enternecedor ni√±o maestro de las ven...                 viral   \n",
       "3604  Camioneros anuncian querella criminal tras que...  region-de-valparaiso   \n",
       "2377  Ir√°n acus√≥ a Israel de inventar pretextos para...          mediooriente   \n",
       "3576  Pronostican bajas temperaturas en Valpara√≠so t...  region-de-valparaiso   \n",
       "3570  Menor de 12 a√±os queda con fractura expuesta t...  region-de-valparaiso   \n",
       "1694  Migrantes indocumentados esperan angustiados l...                  eeuu   \n",
       "\n",
       "        predicted category  \n",
       "3467    region-de-los-rios  \n",
       "1200                 chile  \n",
       "642               animales  \n",
       "3938    region-del-bio-bio  \n",
       "4561                 viral  \n",
       "3604  region-de-valparaiso  \n",
       "2377          mediooriente  \n",
       "3576                 chile  \n",
       "3570  region-de-valparaiso  \n",
       "1694                  eeuu  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algunos ejemplos:\n",
    "pd.DataFrame({'content': X_test, 'category':y_test, 'predicted category': y_pred}).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T16:56:50.571558Z",
     "start_time": "2020-04-23T16:56:50.557622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61  0  0  0  3  2  0  1  1  0 11  0  1  1  0  1  0  8  1]\n",
      " [ 1 71  1  0  3  2  5  0  3  0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0 63  1  0  5  0  0  0  0  0  0  1  0  0  0  0  0  5]\n",
      " [ 0  2  1 64  0  3  1  0  1  4  0  0  0  0  0  0  0  1  1]\n",
      " [10  2  0  1 55  2  1  1  1  0  4  2  2  2  4  4  6  3  1]\n",
      " [ 2  1  1  4  0 37  2  0  1  1  2  0  1  1  1  0  0  2 24]\n",
      " [ 1  7  0  9  0  4 56  0  2  2  3  0  0  0  0  0  0  0  2]\n",
      " [ 0  0  0  0  0  0  0 74  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 1  4  4  1  0  3  3  0 65  3  1  0  0  0  0  0  1  0  2]\n",
      " [ 0  1  1  1  0  0  9  0  1 71  0  0  0  0  0  0  0  0  1]\n",
      " [13  1  0  0  2  3  0  0  0  0 45  0  0  0  1  3  1  7  2]\n",
      " [ 0  0  0  0  2  1  0  0  0  0  0 72  3  0  3  1  2  2  0]\n",
      " [ 0  1  0  0  1  0  0  0  0  0  0  0 70  1  2  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  3 74  1  0  1  0  0]\n",
      " [ 1  0  1  0  3  0  0  1  0  0  2  0  0  2 63  1  1  0  1]\n",
      " [ 0  0  0  0  1  1  0  1  0  0  1  2  2  0  1 69  1  0  0]\n",
      " [ 1  1  0  0  4  0  0  0  0  0  3  0  3  0  3  1 66  1  0]\n",
      " [ 8  0  0  0  4  1  0  1  0  0 19  0  0  0  0  0  0 53  1]\n",
      " [ 0  0  8  3  0  9  0  1  0  0  2  0  0  0  0  0  1  0 50]]\n"
     ]
    }
   ],
   "source": [
    "# usando la matriz de confusi√≥n:\n",
    "\n",
    "# eje x -> predichos\n",
    "# eje y -> clase real\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M√©tricas de Evaluaci√≥n\n",
    "\n",
    "Las m√©tricas definen un puntaje de evaluaci√≥n que indica que tal le fue al sistema. Hay muchas formas distintas de medir su rendimiento. Entre estas, tenemos:\n",
    "\n",
    "- `precision`: El n√∫mero de documentos de una clase clasificados correctamente dividido por el n√∫mero de documentos totales clasificados como esa clase.\n",
    "\n",
    "- `recall`: El n√∫mero de documentos de una clase clasificados correctamente dividido por el n√∫mero de los documentos que se deber√≠an haber clasificado como esa clase.(n√∫mero de documentos reales de esa clase).\n",
    "\n",
    "- `f1-score` : Es la media arm√≥nica entre los anteriores.\n",
    "\n",
    "- `accuracy` : La cantidad de documentos clasificados correctamente versus todos los documentos\n",
    "\n",
    "Por otra parte, tenemos dos formas de ver dichas m√©tricas agrupadas:\n",
    "\n",
    "- `Macroaveraging`:    Se computan las m√©tricas por cada clase y luego de promedia.\n",
    "\n",
    "- `Microaveraging`:    Se recolectan las clasificaciones por cada clase, se computa la tabla de contingencia (todos los elementos clasificados) y se evalua. Representa un Macroaveraging ponderado por el n√∫mero de miembros de una clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:56:44.570489Z",
     "start_time": "2020-04-23T13:56:44.526612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "  actualidad-economica       0.56      0.70      0.62        91\n",
      "        america-latina       0.82      0.84      0.83        89\n",
      "              animales       0.75      0.91      0.82        75\n",
      "                  asia       0.75      0.82      0.79        78\n",
      "                 chile       0.48      0.58      0.53       101\n",
      "          curiosidades       0.49      0.62      0.55        80\n",
      "                  eeuu       0.74      0.73      0.74        86\n",
      "           entrevistas       0.79      0.93      0.85        75\n",
      "                europa       0.92      0.69      0.79        88\n",
      "          mediooriente       0.80      0.93      0.86        85\n",
      "   negocios-y-empresas       0.57      0.60      0.59        78\n",
      "region-de-la-araucania       1.00      0.42      0.59        86\n",
      "   region-de-los-lagos       0.81      0.70      0.75        77\n",
      "    region-de-los-rios       0.84      0.72      0.78        80\n",
      "  region-de-valparaiso       0.66      0.66      0.66        76\n",
      "    region-del-bio-bio       0.91      0.49      0.64        79\n",
      "  region-metropolitana       0.64      0.70      0.67        83\n",
      "           tu-bolsillo       0.66      0.75      0.70        87\n",
      "                 viral       0.73      0.61      0.66        74\n",
      "\n",
      "              accuracy                           0.70      1568\n",
      "             macro avg       0.73      0.71      0.71      1568\n",
      "          weighted avg       0.73      0.70      0.70      1568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usando el classification report:\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecutemos algunas consultas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:15:36.573237Z",
     "start_time": "2020-04-23T14:15:36.554130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['region-de-los-lagos'], dtype='<U22')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\n",
    "    (\"En puerto montt se encontr√≥ un perrito, que aparentemente,\"\n",
    "    \"habr√≠a consumido drogas de alto calibre. Producto de esto,\"\n",
    "    \"se ponost√≠ca que padecer√° severa ca√±a durante varios dias.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:15:37.893568Z",
     "start_time": "2020-04-23T14:15:37.873403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['asia'], dtype='<U22')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"kim jong un ser√° el pr√≥ximo candidato a ministro de educaci√≥n.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:15:38.713364Z",
     "start_time": "2020-04-23T14:15:38.693602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['actualidad-economica'], dtype='<U22')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([(\"El banco mundial present√≥ para chile un decrecimiento\"\n",
    "                   \"econ√≥mico de 92% y una inflaci√≥n de 8239832983289%.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "Se ven bastante buenos los resultados. ¬øPero, podremos mejorarlos?\n",
    "\n",
    "### Preprocesamiento del texto\n",
    "\n",
    "En clases vimos que hab√≠an varias t√©cnicas que permiter preprocesar los textos.\n",
    "Es decir, c√≥mo hacemos el proceso de tokenizaci√≥n (separaci√≥n de las palabras).\n",
    "\n",
    "\n",
    "\n",
    "Alguna de las t√©cnicas son:\n",
    "\n",
    "\n",
    "- Eliminaci√≥n de Stopwords\n",
    "- Stemming\n",
    "- Lematizaci√≥n\n",
    "\n",
    "Existen otros preprocesadores que agregan informaci√≥n a las oraciones, tales como aquellos que indican negaciones.\n",
    "\n",
    "A continuaci√≥n, describiremos con mas detalle cada uno de estas t√©cnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizar ‚ûó\n",
    "\n",
    "¬øQu√© era tokenizar?\n",
    "\n",
    "\n",
    "    Es el proceso de convertir una secuencia de car√°cteres (por ejemplo, una oraci√≥n) en una secuencia de valores distintos entre si llamados tokens.\n",
    "    \n",
    "Referencia: [Tokenizaci√≥n en wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy y el objeto nlp**\n",
    "\n",
    "`nlp` es el objeto que nos permite usar e interactuar con la librer√≠a [`spacy`](https://spacy.io/).\n",
    "Esta librer√≠a incluye variadas herramientras, tales como tokenizar, lematizar, descartar stopwords, entre otras (para este auxiliar, solo utilizaremos las mencionadas). El objeto nlp lo instanciamos en la secci√≥n de imports.\n",
    "\n",
    "Para usarla, simplemente se le pasa el texto como par√°metro, como veremos en el siguiente ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:11:53.353667Z",
     "start_time": "2020-04-23T14:11:53.323795Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hermanito, mio, te, estas, pegando, el, show]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC = \"hermanito mio te estas pegando el show\"\n",
    "\n",
    "tokens = []\n",
    "for word in nlp(DOC):\n",
    "    tokens.append(word)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T01:17:39.213266Z",
     "start_time": "2020-04-23T01:17:39.192949Z"
    }
   },
   "source": [
    "**Observaci√≥n**: Para este auxiliar usaremos `List Comprehension`, otra forma de hacer un for un poco mas reducida.\n",
    "Una muy buena referencia de esto [aqu√≠](https://www.programiz.com/python-programming/list-comprehension).\n",
    "\n",
    "La operaci√≥n anterior usando esta sint√°xis quedar√≠a como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:11:56.423657Z",
     "start_time": "2020-04-23T14:11:56.403841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hermanito, mio, te, estas, pegando, el, show]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [word for word in nlp(DOC)]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords üõë\n",
    "\n",
    "¬øQu√© eran las stopwords?\n",
    "\n",
    "    Las Stopwords son palabras muy comunes en nuestro lenguaje y que por lo tanto, no aportan mucha informaci√≥n. Existen m√∫ltiples listas de stopwords para muchos idiomas y la aplicaci√≥n de estas variar√° caso a caso.\n",
    "\n",
    "    \n",
    "Referencias: [Stopwords en Wikipedia](https://en.wikipedia.org/wiki/Stop_words)\n",
    "\n",
    "En este caso, utilizaremos las stopwords inlcuidas en la librer√≠a spaCy en espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:11:59.833711Z",
     "start_time": "2020-04-23T14:11:59.818559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551\n",
      "['sido', 'quienes', 'parte', 'conseguimos', 'conocer', 'estar√°', 'haya', 'm√≠as', 'total', 'breve', 'donde', 'nosotras', 'nuestros', 'ni', 'tus', 'realizar', 'lugar', 'esos', '√∫ltima', 'aquella']\n"
     ]
    }
   ],
   "source": [
    "print(len(STOP_WORDS))\n",
    "print(list(STOP_WORDS)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming üî™\n",
    "\n",
    "¬øQu√© era el stemming? \n",
    "\n",
    "    Son un conjunto de m√©todos enfocados en reducir cada palabra a su raiz.\n",
    "\n",
    "Referencia: [Stemming en Wikipedia](https://en.wikipedia.org/wiki/Stemming)\n",
    "  \n",
    "**Ejemplos: **\n",
    "\n",
    "\n",
    "| word | stem of the word  |\n",
    "|---|---|\n",
    "working | work\n",
    "worked | work\n",
    "works | work\n",
    "\n",
    "**nltk**\n",
    "\n",
    "En este caso, utilizaremos la segunda librer√≠a de herramientas de nlp: [`nltk`](https://www.nltk.org/). Esta provee una buena herramienta para hacer stemming en espa√±ol : `SnowballStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:12:02.928719Z",
     "start_time": "2020-04-23T14:12:02.923747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hermanit', 'mio', 'te', 'estas', 'peg', 'el', 'show']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmed_doc = [stemmer.stem(str(token)) for token in tokens]\n",
    "print(stemmed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lematizaci√≥n üôÄ\n",
    "\n",
    "¬øQu√© era lematizaci√≥n? \n",
    "\n",
    "    Es el proceso de transformar cada token a su lema, el cual es la palabra base sin ning√∫n tipo de flexi√≥n o alteraci√≥n como las conjugaciones, por ejemplo.\n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "Referencia: [Lematizaci√≥n en wikipedia](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "\n",
    "Refernecia: [Flexi√≥n en las palabras](https://es.wikipedia.org/wiki/Flexi%C3%B3n_(ling%C3%BC%C3%ADstica))\n",
    "\n",
    "**Ejemplos**\n",
    "\n",
    "| word | lemma  |\n",
    "|---|---|\n",
    "dije| decir \n",
    "guapas | guapo\n",
    "mesa | mesas\n",
    "\n",
    "\n",
    " <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flexi%C3%B3nGato-svg.svg/300px-Flexi%C3%B3nGato-svg.svg.png\" alt=\"Flexi√≥n de gato\" style=\"width: 200px;\"/>\n",
    " \n",
    "\n",
    "**Lematizar el texto**\n",
    "\n",
    "Al igual que la tokenizaci√≥n, utilizaremos `scpaCy` (a trav√©s del objeto `nlp`) para lematizar el contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:12:07.443327Z",
     "start_time": "2020-04-23T14:12:07.433649Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hermanito', 'mio', 'te', 'este', 'pegar', 'el', 'show']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_content = [word.lemma_ for word in nlp(DOC)]\n",
    "print(lemmatized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Discusi√≥n:**\n",
    "\n",
    "    ¬øCu√°l es mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistema de clasificaci√≥n con preprocesamiento\n",
    "\n",
    "Para agregar los tokenizadores en el sistema, creamos funciones que que cada documento de forma individual usando nuestro preprocesador favorito. Luego, `CountVectorizer` se encargar√° de usar estas funciones sobre todo el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizadores para CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T13:38:42.575919Z",
     "start_time": "2020-04-23T13:38:42.555823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizers para CountVectorizer\n",
    "\n",
    "# Solo tokenizar el doc usando spacy.\n",
    "def tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc)]\n",
    "\n",
    "\n",
    "# Tokenizar y remover las stopwords del doc\n",
    "def tokenizer_with_stopwords(doc):\n",
    "    return [x.orth_ for x in nlp(doc) if x.orth_ not in STOP_WORDS]\n",
    "\n",
    "\n",
    "# Tokenizar y lematizar.\n",
    "def tokenizer_with_lemmatization(doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "# Tokenizar y hacer stemming.\n",
    "def tokenizer_with_stemming(doc):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return [stemmer.stem(word) for word in [x.orth_ for x in nlp(doc)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos nuestro clasificador\n",
    "\n",
    "\n",
    "**Definimos el pipeline**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:14:21.743401Z",
     "start_time": "2020-04-23T14:14:21.713350Z"
    }
   },
   "outputs": [],
   "source": [
    "# seleccionamos el tokenizador a usar:\n",
    "TOKENIZER = tokenizer_with_stemming\n",
    "\n",
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=TOKENIZER,\n",
    "                             ngram_range=(1, 1))\n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf = MultinomialNB()   \n",
    "\n",
    "# Creamos el pipeline\n",
    "text_clf_2 = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos nuestro pipeline y predecimos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:15:29.393686Z",
     "start_time": "2020-04-23T14:14:22.684233Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf_2.fit(X_train, y_train)\n",
    "y_pred = text_clf_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:15:33.613642Z",
     "start_time": "2020-04-23T14:15:33.543366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[64  1  0  0  5  1  1  1  0  0 10  0  0  0  0  0  0  8  0]\n",
      " [ 0 76  1  3  0  5  2  1  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 67  1  0  3  0  0  0  0  0  0  0  0  0  0  0  0  4]\n",
      " [ 0  1  1 64  0  4  2  0  0  6  0  0  0  0  0  0  0  0  0]\n",
      " [10  5  0  1 54  1  1  6  2  0  7  0  3  2  2  3  2  2  0]\n",
      " [ 0  2  3  2  0 55  2  0  2  0  2  0  0  0  0  0  1  2  9]\n",
      " [ 0  4  0  9  0  6 59  0  1  6  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  2  0 71  0  0  0  0  0  0  0  0  0  2  0]\n",
      " [ 1  1  4  6  1  5  3  0 62  5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  2  3  0  0 79  0  0  0  0  0  0  0  0  0]\n",
      " [15  3  0  0  3  2  1  0  0  0 47  0  0  0  0  1  0  6  0]\n",
      " [ 1  0  0  0 13  0  0  2  0  0  1 37  1  9  6  0 13  3  0]\n",
      " [ 1  0  0  0  7  1  0  5  0  0  1  1 54  2  3  0  2  0  0]\n",
      " [ 1  0  0  0  8  0  0  1  0  0  3  1  7 52  3  0  3  1  0]\n",
      " [ 3  0  1  0  7  1  0  4  0  0  4  0  0  0 51  1  4  0  0]\n",
      " [ 6  0  0  0 10  2  0  3  0  0  3  1  2  1  8 34  8  1  0]\n",
      " [ 2  1  0  0 13  2  0  3  0  0  4  0  0  0  3  0 53  2  0]\n",
      " [10  0  0  0  2  1  0  2  0  0  7  0  0  0  0  0  0 65  0]\n",
      " [ 0  0 11  3  0 19  0  0  0  0  0  0  0  0  0  0  0  0 41]] \n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "  actualidad-economica       0.56      0.70      0.62        91\n",
      "        america-latina       0.80      0.85      0.83        89\n",
      "              animales       0.76      0.89      0.82        75\n",
      "                  asia       0.72      0.82      0.77        78\n",
      "                 chile       0.44      0.53      0.48       101\n",
      "          curiosidades       0.49      0.69      0.57        80\n",
      "                  eeuu       0.80      0.69      0.74        86\n",
      "           entrevistas       0.72      0.95      0.82        75\n",
      "                europa       0.91      0.70      0.79        88\n",
      "          mediooriente       0.82      0.93      0.87        85\n",
      "   negocios-y-empresas       0.53      0.60      0.56        78\n",
      "region-de-la-araucania       0.93      0.43      0.59        86\n",
      "   region-de-los-lagos       0.81      0.70      0.75        77\n",
      "    region-de-los-rios       0.79      0.65      0.71        80\n",
      "  region-de-valparaiso       0.67      0.67      0.67        76\n",
      "    region-del-bio-bio       0.87      0.43      0.58        79\n",
      "  region-metropolitana       0.62      0.64      0.63        83\n",
      "           tu-bolsillo       0.71      0.75      0.73        87\n",
      "                 viral       0.75      0.55      0.64        74\n",
      "\n",
      "              accuracy                           0.69      1568\n",
      "             macro avg       0.72      0.69      0.69      1568\n",
      "          weighted avg       0.72      0.69      0.69      1568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usando la matriz de confusi√≥n:\n",
    "print(confusion_matrix(y_test, y_pred),\n",
    "      '\\n\\n-------------------------------------------------------\\n')\n",
    "# usando el classification report:\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta abierta: ¬øPor qu√© no mejoran los resultados?\n",
    "\n",
    "[Aqu√≠](https://www.quora.com/Is-it-normal-to-get-better-accuracy-without-stemming-and-lemmatization-than-using-them-in-NLP-text-classification) hay una muy buena discusi√≥n al respecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T22:10:39.567162Z",
     "start_time": "2019-07-24T22:10:39.551542Z"
    }
   },
   "source": [
    "### Clasificaci√≥n usando Regresi√≥n Log√≠sitica\n",
    "\n",
    "No profundizaremos en este clasificador, mas del hecho de que se \"supone\" que deber√≠a tener mejor rendimiento que el de bayes.\n",
    "\n",
    "Referencia: [Regresi√≥n Log√≠stica](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definimos nuestro Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T16:23:54.375021Z",
     "start_time": "2020-04-23T16:23:54.369010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qu√© tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=TOKENIZER,\n",
    "                             ngram_range=(1, 1))\n",
    "\n",
    "# Ahora definimos regresi√≥n log√≠stica como clasificador.\n",
    "log_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos y predecimos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T16:24:44.533494Z",
     "start_time": "2020-04-23T16:23:55.137485Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe.fit(X_train, y_train)\n",
    "y_pred = log_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T16:24:44.608410Z",
     "start_time": "2020-04-23T16:23:56.759Z"
    }
   },
   "outputs": [],
   "source": [
    "# usando la matriz de confusi√≥n:\n",
    "print(confusion_matrix(y_test, y_pred),\n",
    "      '\\n\\n-------------------------------------------------------\\n')\n",
    "# usando el classification report:\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los n-gramas son conjuntos de n-tokens seguidos entre si. La idea de usar esto es que adem√°s, capturemos conceptos. \n",
    "\n",
    "Por ejemplo, si usamos 2-gramas sobre `'Hoy d√≠a com√≠ lentejas'`, esta quedar√≠a como:\n",
    "\n",
    "\n",
    "```python\n",
    "['hoy dia', 'd√≠a com√≠', 'com√≠ lentejas']\n",
    "```\n",
    "\n",
    "`CountVectorizer` tiene la opci√≥n para poner n-gramas del tama√±o que tu quieras, y adem√°s incluir mas peque√±os. Todo esto se define en el par√°metro `ngram_range`. Este recibe una tupla con los rangos del n-grama mas peque√±o y el mas grande. Por ejemplo, para (1,2), la oraci√≥n anterior quedar√≠a: \n",
    "\n",
    "\n",
    "```python\n",
    "['hoy', 'd√≠a', 'com√≠', 'lentejas', 'hoy dia', 'd√≠a com√≠', 'com√≠ lentejas']\n",
    "```\n",
    "\n",
    "Nota que esto incrementa el tama√±o de los vectores de Bag of words y por lo tanto, del entrenamiento y de la predicci√≥n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:37:56.941774Z",
     "start_time": "2020-04-23T14:37:56.931734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qu√© tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                     \n",
    "                             tokenizer=TOKENIZER,\n",
    "                             ngram_range=(1, 3))\n",
    "\n",
    "# Ahora definimos regresi√≥n log√≠stica como clasificador.\n",
    "log_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos y predecimos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:49:04.645815Z",
     "start_time": "2020-04-23T14:37:58.342033Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe.fit(X_train, y_train)\n",
    "y_pred = log_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:49:35.781126Z",
     "start_time": "2020-04-23T14:49:35.711349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53  0  0  0  6  2  0  1  0  0 15  0  1  0  0  2  0 10  1]\n",
      " [ 0 74  1  0  3  1  5  0  2  0  1  0  0  0  0  0  0  0  2]\n",
      " [ 0  0 66  1  0  2  0  0  0  0  0  0  0  0  0  0  0  0  6]\n",
      " [ 0  1  2 66  0  1  0  0  4  1  0  0  0  0  0  0  0  1  2]\n",
      " [ 9  1  0  1 58  1  0  1  1  0  4  1  2  2  4  5  9  1  1]\n",
      " [ 1  2  3  3  0 39  1  0  1  1  4  0  0  0  0  0  0  2 23]\n",
      " [ 0  6  0 10  0  4 57  0  1  2  2  1  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0 74  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 1  3  4  4  0  3  3  0 64  2  1  0  0  0  0  0  1  0  2]\n",
      " [ 0  1  0  2  0  1  5  0  0 75  0  0  0  0  0  0  0  0  1]\n",
      " [13  1  0  0  1  3  0  0  0  0 53  0  0  0  1  1  0  2  3]\n",
      " [ 0  0  0  0  2  1  0  0  0  0  1 74  1  0  3  1  2  1  0]\n",
      " [ 0  1  0  0  2  0  0  1  0  0  0  0 68  0  2  2  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  2 76  0  0  1  0  0]\n",
      " [ 1  0  1  0  1  0  0  1  0  0  1  0  2  0 67  0  1  0  1]\n",
      " [ 0  0  0  0  0  2  0  1  0  0  2  1  2  0  2 67  2  0  0]\n",
      " [ 1  1  0  0  3  0  0  0  0  0  3  0  4  0  1  0 69  1  0]\n",
      " [ 9  0  0  0  2  0  0  0  0  0 20  0  0  0  0  0  0 56  0]\n",
      " [ 0  0  3  3  0  8  0  0  0  0  2  0  0  0  0  0  1  1 56]] \n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "  actualidad-economica       0.60      0.58      0.59        91\n",
      "        america-latina       0.81      0.83      0.82        89\n",
      "              animales       0.82      0.88      0.85        75\n",
      "                  asia       0.73      0.85      0.79        78\n",
      "                 chile       0.74      0.57      0.65       101\n",
      "          curiosidades       0.57      0.49      0.53        80\n",
      "                  eeuu       0.80      0.66      0.73        86\n",
      "           entrevistas       0.94      0.99      0.96        75\n",
      "                europa       0.88      0.73      0.80        88\n",
      "          mediooriente       0.93      0.88      0.90        85\n",
      "   negocios-y-empresas       0.48      0.68      0.56        78\n",
      "region-de-la-araucania       0.95      0.86      0.90        86\n",
      "   region-de-los-lagos       0.83      0.88      0.86        77\n",
      "    region-de-los-rios       0.97      0.95      0.96        80\n",
      "  region-de-valparaiso       0.84      0.88      0.86        76\n",
      "    region-del-bio-bio       0.86      0.85      0.85        79\n",
      "  region-metropolitana       0.79      0.83      0.81        83\n",
      "           tu-bolsillo       0.75      0.64      0.69        87\n",
      "                 viral       0.55      0.76      0.64        74\n",
      "\n",
      "              accuracy                           0.77      1568\n",
      "             macro avg       0.78      0.78      0.78      1568\n",
      "          weighted avg       0.78      0.77      0.77      1568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usando la matriz de confusi√≥n:\n",
    "print(confusion_matrix(y_test, y_pred),\n",
    "      '\\n\\n-------------------------------------------------------\\n')\n",
    "# usando el classification report:\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Clasificaci√≥n de Autor√≠a de documentos\n",
    "\n",
    "¬øExistir√° un patr√≥n en como escriben los periodistas que nos permitan identificarlos a partir de sus textos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:51:41.280774Z",
     "start_time": "2020-04-23T14:51:41.250947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diego Vera               4471\n",
       "Emilio Lara              1380\n",
       "Mat√≠as Vega              1230\n",
       "C√©sar Vega Mart√≠nez      1202\n",
       "Mar√≠a Jos√© Villarroel    1157\n",
       "Gonzalo Cifuentes        1122\n",
       "Manuel Stuardo           1022\n",
       "Manuel Cabrera            999\n",
       "Valentina Gonz√°lez        966\n",
       "Nicole Briones            850\n",
       "Felipe Delgado            825\n",
       "Yessenia M√°rquez          764\n",
       "Paola Alem√°n              760\n",
       "Ver√≥nica Reyes            718\n",
       "Jonathan Flores           618\n",
       "Nicol√°s D√≠az              592\n",
       "Sebasti√°n Asencio         544\n",
       "Catalina D√≠az             523\n",
       "Ariela Mu√±oz              515\n",
       "Nicol√°s Parra             503\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_r.author.value_counts()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:54:17.800449Z",
     "start_time": "2020-04-23T14:54:17.781245Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 250\n",
    "\n",
    "def process_datasets_by_author(dataset):\n",
    "    \n",
    "    # creamos una nueva columna titulo y contenido.\n",
    "    content = dataset['title'] + '. ' + dataset['content'] \n",
    "    # obtenemos las clases\n",
    "    subcategory = dataset.author\n",
    "    # dejamos en el dataset solo contenido de la noticia y categoria\n",
    "    dataset = pd.DataFrame({'content': content, 'author': subcategory})\n",
    "\n",
    "    selected_authors = ['Diego Vera', 'Emilio Lara', 'Mat√≠as Vega', 'C√©sar Vega Mart√≠nez',\n",
    "           'Mar√≠a Jos√© Villarroel', 'Gonzalo Cifuentes', 'Manuel Stuardo',\n",
    "           'Manuel Cabrera', 'Valentina Gonz√°lez', 'Nicole Briones',\n",
    "           'Felipe Delgado', 'Yessenia M√°rquez', 'Paola Alem√°n', 'Ver√≥nica Reyes',\n",
    "           'Jonathan Flores', 'Nicol√°s D√≠az', 'Sebasti√°n Asencio', 'Catalina D√≠az',\n",
    "           'Ariela Mu√±oz', 'Nicol√°s Parra']\n",
    "\n",
    "    # filtrar solo categorias seleccionadas\n",
    "    dataset = dataset[dataset['author'].isin(selected_authors)]\n",
    "\n",
    "    # balancear clases\n",
    "    g = dataset.groupby('author')\n",
    "    dataset = pd.DataFrame(\n",
    "        g.apply(lambda x: x.sample(NUM_SAMPLES).reset_index(drop=True))).reset_index(\n",
    "            drop=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:54:20.800716Z",
     "start_time": "2020-04-23T14:54:20.771874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gobierno eval√∫a aumentar hasta un 40% la cuota...</td>\n",
       "      <td>Ariela Mu√±oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aeropuerto de Santiago mejor√≥ en un 6,4% la pu...</td>\n",
       "      <td>Ariela Mu√±oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Espa√±a pide \"sanciones\" de la Uni√≥n Europea co...</td>\n",
       "      <td>Ariela Mu√±oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gobierno firma convenio para tratar salud ment...</td>\n",
       "      <td>Ariela Mu√±oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vivienda se incendia por completo en La Arauca...</td>\n",
       "      <td>Ariela Mu√±oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Armas de fogueo, desorden y amenazas generan a...</td>\n",
       "      <td>Yessenia M√°rquez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Madre de joven que lleva 6 meses desaparecida ...</td>\n",
       "      <td>Yessenia M√°rquez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Este lunes formalizar√≠an a hombre acusado de p...</td>\n",
       "      <td>Yessenia M√°rquez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Gobernador de Palena acusa atrasos en barcazas...</td>\n",
       "      <td>Yessenia M√°rquez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Presidente de Brasil vota en Sao Paulo y llama...</td>\n",
       "      <td>Yessenia M√°rquez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content            author\n",
       "0     Gobierno eval√∫a aumentar hasta un 40% la cuota...      Ariela Mu√±oz\n",
       "1     Aeropuerto de Santiago mejor√≥ en un 6,4% la pu...      Ariela Mu√±oz\n",
       "2     Espa√±a pide \"sanciones\" de la Uni√≥n Europea co...      Ariela Mu√±oz\n",
       "3     Gobierno firma convenio para tratar salud ment...      Ariela Mu√±oz\n",
       "4     Vivienda se incendia por completo en La Arauca...      Ariela Mu√±oz\n",
       "...                                                 ...               ...\n",
       "4995  Armas de fogueo, desorden y amenazas generan a...  Yessenia M√°rquez\n",
       "4996  Madre de joven que lleva 6 meses desaparecida ...  Yessenia M√°rquez\n",
       "4997  Este lunes formalizar√≠an a hombre acusado de p...  Yessenia M√°rquez\n",
       "4998  Gobernador de Palena acusa atrasos en barcazas...  Yessenia M√°rquez\n",
       "4999  Presidente de Brasil vota en Sao Paulo y llama...  Yessenia M√°rquez\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T14:54:29.516731Z",
     "start_time": "2020-04-23T14:54:29.421040Z"
    }
   },
   "outputs": [],
   "source": [
    "author_dataset = process_datasets_by_author(dataset_r.copy(deep=True))\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    author_dataset.content,\n",
    "    author_dataset.author,\n",
    "    test_size=0.33,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T16:19:06.779816Z",
     "start_time": "2020-04-23T16:19:06.775812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             ngram_range=(1, 1))\n",
    "\n",
    "# Definimos el clasificador. Usaremos bayes, ya que regresi√≥n log√≠stica se demora 1/4 del tiempo del universo.\n",
    "clf = MultinomialNB()   \n",
    "\n",
    "\n",
    "# Creamos el pipeline\n",
    "log_pipe_by_author = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T16:19:09.422404Z",
     "start_time": "2020-04-23T16:19:07.664622Z"
    }
   },
   "outputs": [],
   "source": [
    "log_pipe_by_author.fit(X_train_2, y_train_2)\n",
    "y_pred = log_pipe_by_author.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T16:19:11.133509Z",
     "start_time": "2020-04-23T16:19:11.082608Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  0  1  8  0  6  1 26  0  0  6  3  1  3  3  9  3  3 15  1]\n",
      " [ 0 25  1  1  1  7  0  1  1  6  7  1  5 12  1  4  0  0  0 12]\n",
      " [ 0  0 55  8  0  0  0  5  0  0  0  0  0  0  0 22  0  0  0  0]\n",
      " [ 0  0  0 53  0  0  0  6  0  0  0  2  0  0  0 10  0  0  1  0]\n",
      " [ 1  2  2 34  2  1  0 27  2  2  3  0  0  4  1  4  0  0  9  2]\n",
      " [ 0  0  1 18  0 34  0  4  0  0  9  0  0  2  0  5  1  3  0  0]\n",
      " [ 0  0  2 28  1  4  1 28  1  0  4  2  0  5  0 10  0  0  5  1]\n",
      " [ 0  0  0 13  0  2  1 49  2  0  0  4  0  1  0  2  0  0  3  0]\n",
      " [ 0  1  0 10  0  4  0 12 17  0  2  1  1 18  5  5  1  0  1  4]\n",
      " [ 0  9  1  5  0  6  0  9  0  4 14  4  7 17  1  0  0  0  1  7]\n",
      " [ 1  0  1  5  0  4  0 13  1  0 24  1  1 12  2  4  1  4  2  1]\n",
      " [ 0  1  1 19  0  2  1 26  0  2  1  5  0  5  4  8  3  0  5  0]\n",
      " [ 0  8  0  5  0  3  0  6  1  4  8  1 11 15  1  2  0  0  0 22]\n",
      " [ 0  2  0  1  1  9  1 13  1  0  5  0  0 36  3  0  0  0  0  1]\n",
      " [ 1  3  1 18  0  3  0 11  7  0  4  1  0 11 16  1  0  0  3  0]\n",
      " [ 0  0  1 21  0  1  0  4  0  0  0  0  0  0  0 53  1  0  4  0]\n",
      " [ 0  0  0 21  0  1  1 25  0  0  0  1  0  4  1  7  3  0  5  3]\n",
      " [ 0  3  2 12  0  8  0 10  0  0 15  6  0  5  2  4  1  6  3  1]\n",
      " [ 0  0  3 21  1  2  0 22  0  0  0  3  0  0  2  5  2  0 20  0]\n",
      " [ 1  9  1  5  0  0  0  5  4  4  8  2  2 24  1  6  0  0  0 12]] \n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "         Ariela Mu√±oz       0.56      0.05      0.10        94\n",
      "        Catalina D√≠az       0.40      0.29      0.34        85\n",
      "  C√©sar Vega Mart√≠nez       0.75      0.61      0.67        90\n",
      "           Diego Vera       0.17      0.74      0.28        72\n",
      "          Emilio Lara       0.33      0.02      0.04        96\n",
      "       Felipe Delgado       0.35      0.44      0.39        77\n",
      "    Gonzalo Cifuentes       0.17      0.01      0.02        92\n",
      "      Jonathan Flores       0.16      0.64      0.26        77\n",
      "       Manuel Cabrera       0.46      0.21      0.29        82\n",
      "       Manuel Stuardo       0.18      0.05      0.07        85\n",
      "Mar√≠a Jos√© Villarroel       0.22      0.31      0.26        77\n",
      "          Mat√≠as Vega       0.14      0.06      0.08        83\n",
      "       Nicole Briones       0.39      0.13      0.19        87\n",
      "         Nicol√°s D√≠az       0.21      0.49      0.29        73\n",
      "        Nicol√°s Parra       0.37      0.20      0.26        80\n",
      "         Paola Alem√°n       0.33      0.62      0.43        85\n",
      "    Sebasti√°n Asencio       0.19      0.04      0.07        72\n",
      "   Valentina Gonz√°lez       0.38      0.08      0.13        78\n",
      "       Ver√≥nica Reyes       0.26      0.25      0.25        81\n",
      "     Yessenia M√°rquez       0.18      0.14      0.16        84\n",
      "\n",
      "             accuracy                           0.26      1650\n",
      "            macro avg       0.31      0.27      0.23      1650\n",
      "         weighted avg       0.32      0.26      0.23      1650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# usando la matriz de confusi√≥n:\n",
    "print(confusion_matrix(y_test_2, y_pred),\n",
    "      '\\n\\n-------------------------------------------------------\\n')\n",
    "# usando el classification report:\n",
    "print(classification_report(y_test_2, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:03.045562Z",
     "start_time": "2019-08-08T13:28:03.029690Z"
    }
   },
   "source": [
    "## Cr√©ditos\n",
    "\n",
    "Todas las noticias extraidas perteneces a [Biobio Chile](https://www.biobiochile.cl/), los cuales gentilmente licencian todo su material a trav√©s de la [licencia Creative Commons (CC-BY-NC)](https://creativecommons.org/licenses/by-nc/2.0/cl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:21.929923Z",
     "start_time": "2019-08-08T13:28:21.917993Z"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "Gitgub del curso: \n",
    "- https://github.com/dccuchile/CC6205\n",
    "\n",
    "Slides:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf\n",
    "\n",
    "\n",
    "An√°lisis de sentimientos como clasificaci√≥n de texto:\n",
    "- https://affectivetweets.cms.waikato.ac.nz/benchmark/\n",
    "\n",
    "Algunos Recursos √∫tiles\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Scikit-learn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "- [Spacy Tutorial](https://www.datacamp.com/community/blog/spacy-cheatsheet)\n",
    "- [NLTK Cheat sheet](http://sapir.psych.wisc.edu/programming_for_psychologists/cheat_sheets/Text-Analysis-with-NLTK-Cheatsheet.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "770px",
    "left": "1561px",
    "top": "111.133px",
    "width": "359px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
