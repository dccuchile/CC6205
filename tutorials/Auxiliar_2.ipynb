{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivaci√≥n\" data-toc-modified-id=\"Motivaci√≥n-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Motivaci√≥n</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Bag of Words</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word Embeddings</a></span></li><li><span><a href=\"#Word2vec\" data-toc-modified-id=\"Word2vec-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word2vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Skip-gram\" data-toc-modified-id=\"Skip-gram-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Skip-gram</a></span></li></ul></li><li><span><a href=\"#Detalles-del-Modelo\" data-toc-modified-id=\"Detalles-del-Modelo-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Detalles del Modelo</a></span></li><li><span><a href=\"#La-capa-Oculta\" data-toc-modified-id=\"La-capa-Oculta-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>La capa Oculta</a></span></li><li><span><a href=\"#Fuentes\" data-toc-modified-id=\"Fuentes-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Fuentes</a></span></li></ul></li><li><span><a href=\"#Entrenar-nuestros-Embeddings\" data-toc-modified-id=\"Entrenar-nuestros-Embeddings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Entrenar nuestros Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset-desde-el-archivo\" data-toc-modified-id=\"Cargar-el-dataset-desde-el-archivo-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Cargar el dataset desde el archivo</a></span></li><li><span><a href=\"#Limpiar-las-noticias\" data-toc-modified-id=\"Limpiar-las-noticias-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Limpiar las noticias</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comenzamos-el-preprocesamiento:\" data-toc-modified-id=\"Comenzamos-el-preprocesamiento:-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Comenzamos el preprocesamiento:</a></span></li></ul></li></ul></li><li><span><a href=\"#Extracci√≥n-de-Frases\" data-toc-modified-id=\"Extracci√≥n-de-Frases-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extracci√≥n de Frases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Separamos-todos-los-documentos-en-tokens.\" data-toc-modified-id=\"Separamos-todos-los-documentos-en-tokens.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Separamos todos los documentos en tokens.</a></span></li><li><span><a href=\"#Buscamos-bigramas-relevantes\" data-toc-modified-id=\"Buscamos-bigramas-relevantes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Buscamos bigramas relevantes</a></span></li><li><span><a href=\"#Retokenizar-el-corpus\" data-toc-modified-id=\"Retokenizar-el-corpus-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Retokenizar el corpus</a></span></li></ul></li><li><span><a href=\"#Entrenamiento-del-Modelo\" data-toc-modified-id=\"Entrenamiento-del-Modelo-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Entrenamiento del Modelo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Construir-el-vocabulario\" data-toc-modified-id=\"Construir-el-vocabulario-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Construir el vocabulario</a></span></li><li><span><a href=\"#Entrenar-el-Modelo\" data-toc-modified-id=\"Entrenar-el-Modelo-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Entrenar el Modelo</a></span></li><li><span><a href=\"#Guardar-y-cargar-el-modelo\" data-toc-modified-id=\"Guardar-y-cargar-el-modelo-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Guardar y cargar el modelo</a></span></li></ul></li><li><span><a href=\"#Tareas:-Palabras-mas-similares-y-Analog√≠as\" data-toc-modified-id=\"Tareas:-Palabras-mas-similares-y-Analog√≠as-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tareas: Palabras mas similares y Analog√≠as</a></span><ul class=\"toc-item\"><li><span><a href=\"#Palabras-mas-similares\" data-toc-modified-id=\"Palabras-mas-similares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Palabras mas similares</a></span></li></ul></li><li><span><a href=\"#Analog√≠as\" data-toc-modified-id=\"Analog√≠as-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analog√≠as</a></span></li><li><span><a href=\"#Visualizar\" data-toc-modified-id=\"Visualizar-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Visualizar</a></span></li><li><span><a href=\"#Visualizamos\" data-toc-modified-id=\"Visualizamos-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Visualizamos</a></span></li><li><span><a href=\"#Word-Embeddings-como-caracter√≠sticas-para-clasificar\" data-toc-modified-id=\"Word-Embeddings-como-caracter√≠sticas-para-clasificar-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Word Embeddings como caracter√≠sticas para clasificar</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dividimos-el-dataset-de-embeddings-en-training-y-test\" data-toc-modified-id=\"Dividimos-el-dataset-de-embeddings-en-training-y-test-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Dividimos el dataset de embeddings en training y test</a></span></li><li><span><a href=\"#Entrenamos-el-clasificador\" data-toc-modified-id=\"Entrenamos-el-clasificador-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Entrenamos el clasificador</a></span></li><li><span><a href=\"#Predecimos-y-evaluamos:\" data-toc-modified-id=\"Predecimos-y-evaluamos:-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Predecimos y evaluamos:</a></span></li></ul></li><li><span><a href=\"#En-comparaci√≥n-con-BoW\" data-toc-modified-id=\"En-comparaci√≥n-con-BoW-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>En comparaci√≥n con BoW</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 2 - Word Embeddings\n",
    "\n",
    "\n",
    "La clase auxiliar de esta semana tendr√° varios objetivos: \n",
    "\n",
    "- Entender lo b√°sico de word embeddings \n",
    "- Entrenar nuestros propios `word embeddings` usando el dataset de noticias de la radio biobio. \n",
    "- Experimentar y visualizar los embeddings entrenados. \n",
    "- Por √∫ltimo, resolver la misma tarea de topic classification de la clase auxiliar 1, pero usando los embeddings que hemos calculado. \n",
    "\n",
    "\n",
    "\n",
    "## Motivaci√≥n\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Pensemos en estas 2 frases como documentos (dieciocheros ü•üü•üüç∑):\n",
    "\n",
    "    ¬°Estuvo buena esa empanada !\n",
    "    ¬°Estuvo espectacular esa empanada!\n",
    "\n",
    "En la practica, sabemos que significan lo mismo.\n",
    "\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "Para esto, generamos un modelo `Bag of Words` sobre cada documento (es decir, transformamos cada palabra a un vector one-hot):\n",
    "\n",
    "$$v = \\{estuvo, buena, esa, empanada, espectacular\\}$$\n",
    "\n",
    "Entonces, el doc 1 quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el doc 2 quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**¬øCu√°l es el problema?**\n",
    "\n",
    "`buena` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan en la pr√°ctica lo mismo, \n",
    "\n",
    "pero en esta representaci√≥n **son totalmente distintos**, ya que ocupan distintas dimensiones en el modelo. Esto hace que en la pr√°ctica, que `buena` y `espectacular` sean tan distintas como `estuvo` y `empanada`. Esto evidentemente, repercute en la calidad de los modelos que creamos.\n",
    "\n",
    "\n",
    "Nos gustar√≠a que eso no sucediera. Que existiera alg√∫n m√©todo que nos permitiera hacer que palabras similares tengan representaciones similares...\n",
    "\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Pensemos un poco en la `hip√≥tesis distribucional`. Esta plantea que:\n",
    "\n",
    "    \"palabras que ocurren en similares contextos tienden a tener significados similares\" \n",
    "\n",
    "¬øPodr√≠amos crear alg√∫na representaci√≥n vectorial que capture los contextos de las palabras? - La respuesta es si. Estos son los **Word Embeddings**. \n",
    "\n",
    "La idea principal de Word Embeddings (O *Word vectors*) es basarse en la hipotesis distribuci√≥nal para crear, por cada palabra del vocabulario, representaciones vectoriales continuas que capturen su contexto.\n",
    "\n",
    "\n",
    "Es decir, en nuestro ejemplo, `buena` y `espectacular` ocurren en el mismo contexto, por lo que los word embeddings que los deber√≠an representan deber√≠an ser muy similares (ejemplos de mentira hechos a mano*):\n",
    "\n",
    "`buena` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `empanada`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cu√°l es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¬øC√≥mo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "\n",
    "###  Word2vec\n",
    "\n",
    "Word2Vec es una de las herramientas que nos permitiran construir estos vectores. Consiste en entrenar una **red neuronal de solo 1 capa oculta** a trav√©s de la resoluci√≥n de una task auxiliar: `Skip-Gram`\n",
    "\n",
    "¬øPor qu√© auxiliar?: Porque la usaremos solo para entrenar los pesos de la capa oculta de red. Una vez entrenada, nunca mas haremos Skip-Gram!. \n",
    "\n",
    "#### Skip-gram\n",
    "\n",
    "En pocas palabras, la task a resolver es la siguiente: \n",
    "\n",
    "- Toma una oraci√≥n cualquiera del corpus. \n",
    "\n",
    "- Dada una palabra cualquiera en la oraci√≥n, mira las palabras cercanas a esta (dentro de una ventana definida) y toma una aleatoria. \n",
    "\n",
    "- La red deber√° predecir que tan probable es que esta palabra sea cercana a la palabra que escogimos.\n",
    "\n",
    "\n",
    "Por ejemplo, para la t√≠pica frase: \n",
    "\n",
    "     ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
    "     \n",
    "Usando una ventana de 2 palabras, tendremos: \n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "Esto es lo que luego usar√° la red para aprender las relaciones entre las palabras.\n",
    "\n",
    "### Detalles del Modelo\n",
    "\n",
    "\n",
    "Pensemos por ejemplo, en un paso del entrenamiento: \n",
    "\n",
    "El vector de entrada ser√° un One-hot de la palabra que estemos viendo en ese momento. En este caso, `ants`.\n",
    "\n",
    "La red, usando su capa oculta, nos entregar√° la probabilidad de la que la palabra que estamos viendo aparezca con respecto a cada palabra del vocabulario.\n",
    "\n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "Nota: Esto es computacionalmente una locura, ya que por cada palabra debemos calcular la probabilidad de aparici√≥n de todas las otras. Imaginen el caso de un vocabulario de 10 millones de palabras...\n",
    "En clases se ver√° como se puede hacer esto de forma eficiente.\n",
    "\n",
    "Por ende, la salida ser√° un vector que representar√° una distribuci√≥n de probabilidades. \n",
    "\n",
    "Durante el entrenamiento, dicho vector ser√° comparado con las estad√≠sticas reales de co-ocurrencia de las palabras.\n",
    "\n",
    "### La capa Oculta\n",
    "\n",
    "Al terminar el entrenamiento, ¬øQu√© nos queda en la capa oculta?\n",
    "\n",
    "B√°sicamente, una matriz de $v$ filas por $300$ columnas, la cual contiene lo que buscabamos: Una representaci√≥n continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representaci√≥n continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)\n",
    "\n",
    "¬øC√≥mo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot del BoW y las multiplicamos por la matriz:\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "\n",
    "Word2vec:\n",
    "- mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "Gensim: \n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "Nota: Todas las imagenes pertenecen a [Chris McCormick](http://mccormickml.com/about/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementaci√≥n de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:09.569938Z",
     "start_time": "2019-09-30T14:42:06.019954Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# spacy\n",
    "import spacy \n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Cargar modelos de spacy en espa√±ol.\n",
    "nlp = spacy.load(\"es_core_news_sm\",  disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset desde el archivo\n",
    "\n",
    "Nota: Pandas descomprime por si mismo el archivo bz2. Pueden descomprimirlo manualmente usando 7zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.910436Z",
     "start_time": "2019-09-30T14:42:09.569938Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\n",
    "    'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_clean.bz2',\n",
    "    encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T15:44:59.327518Z",
     "start_time": "2019-08-26T15:44:59.312300Z"
    }
   },
   "source": [
    "**Examinemos un par de ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.950429Z",
     "start_time": "2019-09-30T14:42:17.910436Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "      <th>publication_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22459</th>\n",
       "      <td>Valentina Gonz√°lez</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Juez de Garant√≠a de Temuco presenta demanda in...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-la-araucania</td>\n",
       "      <td>Una demanda indemnizatoria en contra del fis...</td>\n",
       "      <td>[#Demanda, #Juez de Garant√≠a, #Luis Olivares A...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1564774560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22415</th>\n",
       "      <td>C√©sar Vega Mart√≠nez</td>\n",
       "      <td>/lista/autores/cevega</td>\n",
       "      <td>Rosa Bernile Nienau: as√≠ fue la c√°lida amistad...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/m...</td>\n",
       "      <td>sociedad</td>\n",
       "      <td>misterios</td>\n",
       "      <td>Fue en noviembre pasado cuando la casa de su...</td>\n",
       "      <td>[#Adolf Hitler, #Amistad, #borman, #Fotos, #ho...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1549106040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author               author_link  \\\n",
       "22459   Valentina Gonz√°lez  /lista/autores/vgonzalez   \n",
       "22415  C√©sar Vega Mart√≠nez     /lista/autores/cevega   \n",
       "\n",
       "                                                   title  \\\n",
       "22459  Juez de Garant√≠a de Temuco presenta demanda in...   \n",
       "22415  Rosa Bernile Nienau: as√≠ fue la c√°lida amistad...   \n",
       "\n",
       "                                                    link  category  \\\n",
       "22459  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "22415  https://www.biobiochile.cl/noticias/sociedad/m...  sociedad   \n",
       "\n",
       "                  subcategory  \\\n",
       "22459  region-de-la-araucania   \n",
       "22415               misterios   \n",
       "\n",
       "                                                 content  \\\n",
       "22459    Una demanda indemnizatoria en contra del fis...   \n",
       "22415    Fue en noviembre pasado cuando la casa de su...   \n",
       "\n",
       "                                                    tags  \\\n",
       "22459  [#Demanda, #Juez de Garant√≠a, #Luis Olivares A...   \n",
       "22415  [#Adolf Hitler, #Amistad, #borman, #Fotos, #ho...   \n",
       "\n",
       "                                          embedded_links  publication_datetime  \n",
       "22459  [https://media.biobiochile.cl/wp-content/uploa...         1564774560000  \n",
       "22415                                                 []         1549106040000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar las noticias\n",
    "\n",
    "Primero, vamos a definir la funci√≥n que definir√° como ser√° limpiado y tokenizado cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.970369Z",
     "start_time": "2019-09-30T14:42:17.950429Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Tokenizar y remover  stopwords\n",
    "    txt = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaci√≥n:  `Word2Vec` usa el contexto de las oraciones para aprender las representaciones de las palabras.\n",
    "Si la oraci√≥n es muy peque√±a, entonces, el entrenamiento no podr√° inferir nada. Por eso, se retorna la oraci√≥n solo cuando hay mas de 2 palabras dentro de la oraci√≥n.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comenzamos el preprocesamiento:\n",
    "\n",
    "Primero convertimos todas las palabras del documento en min√∫sculas y realizamos una limpieza r√°pida de s√≠mbolos y espacios extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.987662Z",
     "start_time": "2019-09-30T14:42:17.970369Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_cleaned_docs = (re.sub(r'[^\\w\\s+]', '', str(row)).lower() for row in data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, tokenizamos el texto usando la funci√≥n que definimos unas celdas mas arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:33.809990Z",
     "start_time": "2019-09-30T14:42:17.989775Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(pre_cleaned_docs, batch_size=5000, n_threads=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:33.829847Z",
     "start_time": "2019-09-30T14:43:33.809990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de texto procesado:\n",
      "\n",
      "   noticia desarrollo   recopilando antecedentes noticia qu√©date atento a actualizaciones     estructura restante casa cay√≥ valpara√≠so colaps√≥ ma√±ana mi√©rcoles   produjo a 1015 esquina aldunate huito murieron personas   informaci√≥n preliminar rescatistas labores suspendido peligro seguir trabajando habr√≠a lesionados  \n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo de texto procesado:\\n\\n{}\".format(txt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo, almacenamos los documentos tokenizados en un `DataFrame` y eliminamos vacios/ducplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:34.120021Z",
     "start_time": "2019-09-30T14:43:33.829847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26265, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "## Extracci√≥n de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`.\n",
    "\n",
    "\n",
    "### Separamos todos los documentos en tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:34.755160Z",
     "start_time": "2019-09-30T14:43:34.120021Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:34.774731Z",
     "start_time": "2019-09-30T14:43:34.755160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noticia',\n",
       " 'desarrollo',\n",
       " 'recopilando',\n",
       " 'antecedentes',\n",
       " 'noticia',\n",
       " 'qu√©date',\n",
       " 'atento',\n",
       " 'a',\n",
       " 'actualizaciones',\n",
       " 'estructura',\n",
       " 'restante',\n",
       " 'casa',\n",
       " 'cay√≥',\n",
       " 'valpara√≠so',\n",
       " 'colaps√≥',\n",
       " 'ma√±ana',\n",
       " 'mi√©rcoles',\n",
       " 'produjo',\n",
       " 'a',\n",
       " '1015',\n",
       " 'esquina',\n",
       " 'aldunate',\n",
       " 'huito',\n",
       " 'murieron',\n",
       " 'personas',\n",
       " 'informaci√≥n',\n",
       " 'preliminar',\n",
       " 'rescatistas',\n",
       " 'labores',\n",
       " 'suspendido',\n",
       " 'peligro',\n",
       " 'seguir',\n",
       " 'trabajando',\n",
       " 'habr√≠a',\n",
       " 'lesionados']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para ver como quedan las noticias, quitar comentario a la siguiente linea:\n",
    "sent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscamos bigramas relevantes\n",
    "\n",
    "Ahora, buscamos los bigramas relevantes dentro de nuesto corpus usando `Phrases`. La condici√≥n para que sean considerados es que aparezcan por lo menos 30 veces repetidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:44.724328Z",
     "start_time": "2019-09-30T14:43:34.779249Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:43:34: collecting all words and their counts\n",
      "INFO - 11:43:34: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 11:43:36: PROGRESS: at sentence #5000, processed 1095422 words and 759116 word types\n",
      "INFO - 11:43:38: PROGRESS: at sentence #10000, processed 1923387 words and 1200422 word types\n",
      "INFO - 11:43:40: PROGRESS: at sentence #15000, processed 2814795 words and 1627467 word types\n",
      "INFO - 11:43:42: PROGRESS: at sentence #20000, processed 3801078 words and 2167896 word types\n",
      "INFO - 11:43:44: PROGRESS: at sentence #25000, processed 4786655 words and 2672007 word types\n",
      "INFO - 11:43:44: collected 2802254 word types from a corpus of 5045011 words (unigram + bigrams) and 26265 sentences\n",
      "INFO - 11:43:44: using 2802254 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenizar el corpus\n",
    "\n",
    "Por √∫ltimo, usando `Phraser`, re-tokenizamos el corpus con los bigramas encontrados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:44:12.045998Z",
     "start_time": "2019-09-30T14:43:44.724328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:43:44: source_vocab length 2802254\n",
      "INFO - 11:44:12: Phraser built with 4039 4039 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:44:12.066117Z",
     "start_time": "2019-09-30T14:44:12.048035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noticia',\n",
       " 'desarrollo',\n",
       " 'recopilando',\n",
       " 'antecedentes',\n",
       " 'noticia',\n",
       " 'qu√©date',\n",
       " 'atento',\n",
       " 'a',\n",
       " 'actualizaciones',\n",
       " 'estructura',\n",
       " 'restante',\n",
       " 'casa',\n",
       " 'cay√≥',\n",
       " 'valpara√≠so',\n",
       " 'colaps√≥',\n",
       " 'ma√±ana_mi√©rcoles',\n",
       " 'produjo',\n",
       " 'a',\n",
       " '1015',\n",
       " 'esquina',\n",
       " 'aldunate',\n",
       " 'huito',\n",
       " 'murieron',\n",
       " 'personas',\n",
       " 'informaci√≥n_preliminar',\n",
       " 'rescatistas',\n",
       " 'labores',\n",
       " 'suspendido',\n",
       " 'peligro',\n",
       " 'seguir_trabajando',\n",
       " 'habr√≠a',\n",
       " 'lesionados']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para ver como quedan las noticias retokenizadas, quitar comentario a la siguiente linea:\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos par√°metros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada\n",
    "- `size` : El tama√±o de los embeddings que crearemos. Por lo general, se utilizan 300\n",
    "- `workers`: Cantidad de CPU que ser√°n utilizadas en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.103684Z",
     "start_time": "2019-09-30T14:28:05.143Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v = Word2Vec(min_count=10,\n",
    "                      window=2,\n",
    "                      size=300,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se crear√° un conjunto que contendr√° (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.103684Z",
     "start_time": "2019-09-30T14:28:06.021Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.build_vocab(sentences, progress_per=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuaci√≥n, entenaremos el modelo. \n",
    "Los par√°metros que usaremos ser√°n: \n",
    "\n",
    "- `total_examples`: N√∫mero de documentos.\n",
    "- `epochs`: N√∫mero de veces que se iterar√° sobre el corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.103684Z",
     "start_time": "2019-09-30T14:28:08.228Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=30, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. \n",
    "Esto nos permitir√° ejecutar eficientemente las tareas que realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.113824Z",
     "start_time": "2019-09-30T14:28:10.313Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:37:18.951314Z",
     "start_time": "2019-09-30T14:37:16.429053Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Si entrenaste el modelo y lo quieres guardar, descomentar el siguiente bloque.\n",
    "\n",
    "\"\"\"\n",
    "if not os.path.exists('./pretrained_models'):\n",
    "    os.mkdir('./pretrained_models')\n",
    "biobio_w2v.save('./pretrained_models/biobio_w2v.model')\n",
    "\"\"\"\n",
    "\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "#biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:48:36.608370Z",
     "start_time": "2019-09-30T14:48:16.858951Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:48:36: loading Word2VecKeyedVectors object from ./pretrained_models/biobio_w2v.model\n",
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning:\n",
      "\n",
      "This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "\n",
      "INFO - 11:48:36: loading wv recursively from ./pretrained_models/biobio_w2v.model.wv.* with mmap=r\n",
      "INFO - 11:48:36: loading vectors from ./pretrained_models/biobio_w2v.model.wv.vectors.npy with mmap=r\n",
      "INFO - 11:48:36: setting ignored attribute vectors_norm to None\n",
      "INFO - 11:48:36: loading vocabulary recursively from ./pretrained_models/biobio_w2v.model.vocabulary.* with mmap=r\n",
      "INFO - 11:48:36: loading trainables recursively from ./pretrained_models/biobio_w2v.model.trainables.* with mmap=r\n",
      "INFO - 11:48:36: loading syn1neg from ./pretrained_models/biobio_w2v.model.trainables.syn1neg.npy with mmap=r\n",
      "INFO - 11:48:36: setting ignored attribute cum_table to None\n",
      "INFO - 11:48:36: loaded ./pretrained_models/biobio_w2v.model\n"
     ]
    }
   ],
   "source": [
    "# descargar el modelo desde github\n",
    "def read_model_from_github(url):\n",
    "    if not os.path.exists('./pretrained_models'):\n",
    "        os.mkdir('./pretrained_models')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    filename = url.split('/')[-1]\n",
    "    with open('./pretrained_models/' + filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "\n",
    "[\n",
    "    read_model_from_github(file) for file in [\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model',\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model.trainables.syn1neg.npy',\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model.wv.vectors.npy'\n",
    "    ]\n",
    "]\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas: Palabras mas similares y Analog√≠as\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la informaci√≥n contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matem√°tico, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:48:51.768321Z",
     "start_time": "2019-09-30T14:48:51.668750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:48:51: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gato', 0.47238367795944214),\n",
       " ('cachorro', 0.45501261949539185),\n",
       " ('canino', 0.4421420991420746),\n",
       " ('felino', 0.4251091778278351),\n",
       " ('gatito', 0.4125193655490875),\n",
       " ('animal', 0.40201663970947266),\n",
       " ('perrita', 0.39842987060546875),\n",
       " ('zorro', 0.39238905906677246),\n",
       " ('mascota', 0.39086073637008667),\n",
       " ('perros', 0.38970085978507996)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"perro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:48:55.968331Z",
     "start_time": "2019-09-30T14:48:55.948531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald_trump', 0.6679385304450989),\n",
       " ('casa_blanca', 0.6449583768844604),\n",
       " ('unidos', 0.6023310422897339),\n",
       " ('washington', 0.5866063833236694),\n",
       " ('mandatario_estadounidense', 0.5600548386573792),\n",
       " ('presidente_donald', 0.5553752183914185),\n",
       " ('presidente_estadounidense', 0.5523015856742859),\n",
       " ('inquilino', 0.5197651386260986),\n",
       " ('unidos_donald', 0.5129491686820984),\n",
       " ('sessions', 0.5118564367294312)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:00.208512Z",
     "start_time": "2019-09-30T14:49:00.188447Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fidel_castro', 0.382219135761261),\n",
       " ('1959', 0.3599476218223572),\n",
       " ('isl√°mica', 0.35400423407554626),\n",
       " ('marxismo', 0.3403622508049011),\n",
       " ('sandinista', 0.3341415524482727),\n",
       " ('1979', 0.31937718391418457),\n",
       " ('transformaci√≥n', 0.31889623403549194),\n",
       " ('1917', 0.3090049624443054),\n",
       " ('burgues√≠a', 0.30192387104034424),\n",
       " ('yucat√°n', 0.29723119735717773)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revoluci√≥n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:04.408330Z",
     "start_time": "2019-09-30T14:49:04.388759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pablo_vidal', 0.6696338057518005),\n",
       " ('orsini', 0.639187216758728),\n",
       " ('jorge_brito', 0.6381471157073975),\n",
       " ('frenteamplista', 0.6183565855026245),\n",
       " ('rd', 0.6119238138198853),\n",
       " ('prd', 0.598182201385498),\n",
       " ('dem√≥cratacristiano', 0.5970538258552551),\n",
       " ('maite', 0.5770610570907593),\n",
       " ('vlado', 0.5761114954948425),\n",
       " ('partido_democracia', 0.5688214302062988)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revoluci√≥n_democr√°tica\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:08.528555Z",
     "start_time": "2019-09-30T14:49:08.508579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hut', 0.6154688000679016),\n",
       " ('hamburguesas', 0.4615939259529114),\n",
       " ('corner', 0.4598219394683838),\n",
       " ('sandwich', 0.4314725399017334),\n",
       " ('autoservicio', 0.42904579639434814),\n",
       " ('repartidor', 0.4275974631309509),\n",
       " ('cafeter√≠as', 0.42377257347106934),\n",
       " ('ekono', 0.4233115315437317),\n",
       " ('vodka', 0.4229376018047333),\n",
       " ('comida_r√°pida', 0.4187615215778351)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:12.623680Z",
     "start_time": "2019-09-30T14:49:12.603628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cabify', 0.5361112952232361),\n",
       " ('eats', 0.49480363726615906),\n",
       " ('beat', 0.4774724245071411),\n",
       " ('transporte_pasajeros', 0.4051392078399658),\n",
       " ('rappi', 0.38707831501960754),\n",
       " ('didi', 0.3694966435432434),\n",
       " ('vtc', 0.3587350845336914),\n",
       " ('aplicaciones', 0.35364559292793274),\n",
       " ('glovo', 0.35351526737213135),\n",
       " ('taxi', 0.3341372013092041)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"uber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:16.865324Z",
     "start_time": "2019-09-30T14:49:16.848729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gigante_chino', 0.5173695087432861),\n",
       " ('zte', 0.5074669122695923),\n",
       " ('directora_financiera', 0.4465712308883667),\n",
       " ('5_g', 0.4312366247177124),\n",
       " ('china', 0.4225515127182007),\n",
       " ('meng_wanzhou', 0.4068417549133301),\n",
       " ('pekin', 0.38649022579193115),\n",
       " ('telecomunicaciones', 0.38293468952178955),\n",
       " ('tel√©fonos_inteligentes', 0.3672794699668884),\n",
       " ('chips', 0.36506158113479614)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"huawei\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:20.888567Z",
     "start_time": "2019-09-30T14:49:20.868418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canal_13', 0.4343094825744629),\n",
       " ('mega', 0.36687153577804565),\n",
       " ('presidente_directorio', 0.3579253554344177),\n",
       " ('matinal', 0.3533702492713928),\n",
       " ('cupr√≠fera', 0.336997389793396),\n",
       " ('directorio', 0.3351971507072449),\n",
       " ('orrego', 0.3331133723258972),\n",
       " ('fesur', 0.3310084342956543),\n",
       " ('rapu', 0.3075915575027466),\n",
       " ('comisi√≥n_investigadora', 0.3052249550819397)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"tvn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:25.168371Z",
     "start_time": "2019-09-30T14:49:25.148319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intelectualmente', 0.5410281419754028),\n",
       " ('comuneros', 0.5132812857627869),\n",
       " ('pehuenches', 0.4597381353378296),\n",
       " ('comunidades_mapuches', 0.44987913966178894),\n",
       " ('mapuche', 0.4347573518753052),\n",
       " ('auc√°n', 0.4004420042037964),\n",
       " ('pehuenche', 0.37423285841941833),\n",
       " ('vecinales', 0.3617860972881317),\n",
       " ('ancestrales', 0.3590352535247803),\n",
       " ('hortaliceras', 0.35829445719718933)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"mapuches\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:29.528561Z",
     "start_time": "2019-09-30T14:49:29.508488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('camilo_catrillanca', 0.5488350987434387),\n",
       " ('comunero_mapuche', 0.5073081254959106),\n",
       " ('comunero', 0.4860221743583679),\n",
       " ('√∫nico_imputado', 0.4092996120452881),\n",
       " ('chadwick', 0.3861224055290222),\n",
       " ('causa_derivada', 0.3794987201690674),\n",
       " ('tokman', 0.36469513177871704),\n",
       " ('sambuceti', 0.364555686712265),\n",
       " ('luchsinger', 0.35905542969703674),\n",
       " ('caus√°ndole', 0.3463499844074249)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"catrillanca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:33.843172Z",
     "start_time": "2019-09-30T14:49:33.827576Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mag√≠ster', 0.47122281789779663),\n",
       " ('universidad_santiago', 0.4577624201774597),\n",
       " ('universidad_concepci√≥n', 0.43267691135406494),\n",
       " ('odontolog√≠a', 0.4286888837814331),\n",
       " ('agronom√≠a', 0.4259319305419922),\n",
       " ('universidad_cat√≥lica', 0.41985636949539185),\n",
       " ('mba', 0.4132372736930847),\n",
       " ('federaci√≥n_estudiantes', 0.4114116430282593),\n",
       " ('facultad_ciencias', 0.37800729274749756),\n",
       " ('universidad_andr√©s', 0.377147912979126)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"universidad_chile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:37.744434Z",
     "start_time": "2019-09-30T14:49:37.726374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ultraderecha', 0.5372439622879028),\n",
       " ('extrema_derecha', 0.506591260433197),\n",
       " ('jair_bolsonaro', 0.4804544150829315),\n",
       " ('fascismo', 0.45208853483200073),\n",
       " ('antiinmigraci√≥n', 0.452001690864563),\n",
       " ('ultraderechista_liga', 0.45156916975975037),\n",
       " ('excapit√°n_ej√©rcito', 0.444217324256897),\n",
       " ('interior_matteo', 0.4428623914718628),\n",
       " ('matteo_salvini', 0.43859899044036865),\n",
       " ('marine_pen', 0.4377690553665161)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"ultraderechista\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:41.812835Z",
     "start_time": "2019-09-30T14:49:41.797262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('augusto_pinochet', 0.4894692301750183),\n",
       " ('19761983', 0.3469034433364868),\n",
       " ('nost√°lgico', 0.3454264998435974),\n",
       " ('stroessner', 0.3346376419067383),\n",
       " ('franquista', 0.3176833987236023),\n",
       " ('fascista', 0.31446966528892517),\n",
       " ('torturador', 0.31268107891082764),\n",
       " ('19641985', 0.30675196647644043),\n",
       " ('anastasio', 0.3066090941429138),\n",
       " ('exdictador', 0.30496591329574585)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pinochet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:45.668317Z",
     "start_time": "2019-09-30T14:49:45.652696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vilos', 0.46391141414642334),\n",
       " ('coquimbo', 0.4321473240852356),\n",
       " ('santiago', 0.4237024188041687),\n",
       " ('valle_elqui', 0.40091782808303833),\n",
       " ('pica', 0.3972700834274292),\n",
       " ('ovalle', 0.3957822620868683),\n",
       " ('puerto_natales', 0.3800183832645416),\n",
       " ('balmaceda', 0.3758486211299896),\n",
       " ('chill√°n', 0.37579774856567383),\n",
       " ('gustavo_fricke', 0.3721628785133362)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"serena\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analog√≠as\n",
    "\n",
    "Por otra parte, la analog√≠a consiste en comparar 3 terminos mediante una operaci√≥n del estilo: \n",
    "\n",
    "$$palabra1 - palabra2 \\approx palabra 3 - x$$\n",
    "\n",
    "para encontrar relaciones entre estos.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| palabra 1 (pos) |  palabra 2 (neg) |\n",
    "|-----------------|------------------|\n",
    "|  macri          | pi√±era           |\n",
    "| chile           |  x               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:50.092063Z",
     "start_time": "2019-09-30T14:49:50.061262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brasil', 0.46593397855758667),\n",
       " ('jair_bolsonaro', 0.4054754078388214),\n",
       " ('excapit√°n_ej√©rcito', 0.35641491413116455)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"bolsonaro\", \"argentina\"], negative=['macri'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:54.192597Z",
     "start_time": "2019-09-30T14:49:54.176976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('walmart_chile', 0.2904326319694519),\n",
       " ('lipigas', 0.28741344809532166),\n",
       " ('empresa', 0.286331444978714)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"chile\", \"huawei\"], negative=['china'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar\n",
    "\n",
    "Para visualizar, usaremos una t√©cnica de reducci√≥n de dimensionalidad llamada [`T-SNE`](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "\n",
    "B√°sicamente, ejecuta una reducci√≥n de dimensionalidad, que transforma las 300 dimensiones de los embeddings en 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:48.053701Z",
     "start_time": "2019-09-30T14:49:06.721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutamos la transformaci√≥n. Por lo general, demora una media hora mas o menos...\n",
    "X_embedded = TSNE(n_components=2, verbose=True).fit_transform(\n",
    "    biobio_w2v.wv[biobio_w2v.wv.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la frecuencia de las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:16.690620Z",
     "start_time": "2019-08-29T17:39:16.666685Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = dict()\n",
    "for item in biobio_w2v.wv.vocab:\n",
    "    word_counts[item]=biobio_w2v.wv.vocab[item].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nuevos vectores que nos entrego TSNE mas el vocabulario y la frecuencia del vocabulario, formamos un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:22.784472Z",
     "start_time": "2019-08-29T17:39:22.757545Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = pd.DataFrame({\n",
    "    'x': X_embedded[:,0],\n",
    "    'y': X_embedded[:,1],\n",
    "    'vocab': list(biobio_w2v.wv.vocab.keys()) , \n",
    "    'count': list(word_counts.values())\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guardar y Cargar TSNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:28.809083Z",
     "start_time": "2019-09-30T14:50:26.161920Z"
    }
   },
   "outputs": [],
   "source": [
    "#Guardar\n",
    "#tsne.to_csv('./pretrained_models/biobio_w2v_tsne.csv', index= False, index_label=False)\n",
    "# Cargar\n",
    "#tsne = pd.read_csv('./pretrained_models/biobio_w2v_tsne.csv')\n",
    "\n",
    "\n",
    "# Cargar desde github\n",
    "tsne = pd.read_csv('https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v_tsne.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos el Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:32.850839Z",
     "start_time": "2019-09-30T14:50:32.835018Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tsne = tsne.sort_values(by=['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos\n",
    "\n",
    "Para la visualizaci√≥n, usaremos plotly. Este nos deja tener una barra interactiva que nos mostrar√° las 100 palabras mas parecidas a la que escribimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:36.745447Z",
     "start_time": "2019-09-30T14:50:36.729664Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne_sample = sorted_tsne.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:40.844304Z",
     "start_time": "2019-09-30T14:50:40.812831Z"
    }
   },
   "outputs": [],
   "source": [
    "described_tsne = tsne_sample.describe()\n",
    "min_y = described_tsne.y['min']\n",
    "max_y = described_tsne.y['max']\n",
    "\n",
    "min_x =  described_tsne.x['min']\n",
    "max_x =  described_tsne.x['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:08:08.767155Z",
     "start_time": "2019-09-24T18:08:08.754179Z"
    }
   },
   "outputs": [],
   "source": [
    "textbox = widgets.Text(description='Palabra:')\n",
    "\n",
    "def validate(word):\n",
    "    if word in tsne_sample.vocab.values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def response(change):\n",
    "\n",
    "    word = textbox.value.lower()\n",
    "    \n",
    "    if (word == ''):\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = tsne_sample.x.values\n",
    "            fig.data[0].y = tsne_sample.y.values\n",
    "            fig.data[0].text = tsne_sample.vocab.values\n",
    "    else :\n",
    "        if validate(word):\n",
    "\n",
    "            most_similar_words = [word]\n",
    "\n",
    "            for word_tuple in biobio_w2v.wv.most_similar(positive=[word],topn = 100 ):\n",
    "                if word_tuple[0] in tsne_sample.vocab.values:\n",
    "                    most_similar_words.append(word_tuple[0])\n",
    "\n",
    "            filtered_words = [word in most_similar_words for word in tsne_sample.vocab]\n",
    "            temp = tsne_sample.loc[filtered_words]\n",
    "\n",
    "            with fig.batch_update():\n",
    "                fig.data[0].x = temp.x.values\n",
    "                fig.data[0].y = temp.y.values\n",
    "                fig.data[0].text = temp.vocab.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:08:12.877944Z",
     "start_time": "2019-09-24T18:08:11.572647Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.FigureWidget(data=[\n",
    "    go.Scatter(x=tsne_sample.x,\n",
    "               y=tsne_sample.y,\n",
    "               mode='markers',\n",
    "               text=tsne_sample.vocab)\n",
    "],\n",
    "                      layout=go.Layout(title=go.layout.Title(\n",
    "                          text=\"Visualizaci√≥n 2D Embeddings Biobio\"),\n",
    "                                       yaxis =dict(range=[min_y, max_y]),\n",
    "                                       xaxis =dict(range=[min_x, max_x])\n",
    "                                      ))\n",
    "\n",
    "container = widgets.HBox([textbox])\n",
    "textbox.observe(response, names=\"value\")\n",
    "\n",
    "widgets.VBox([container, fig])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T19:55:09.459852Z",
     "start_time": "2019-08-28T19:55:09.454865Z"
    }
   },
   "source": [
    "## Word Embeddings como caracter√≠sticas para clasificar\n",
    "\n",
    "\n",
    "En esta secci√≥n, veremos como utilizar los word embeddings como caracter√≠stica para **clasificar nuevamente el t√≥pico de las noticias de la radio biobio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:58:12.927399Z",
     "start_time": "2019-09-24T12:58:12.921416Z"
    }
   },
   "source": [
    "Definimos el tokenizador (el mismo de antes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:44.986523Z",
     "start_time": "2019-09-30T14:50:44.970867Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    preprocessed_doc = re.sub(r'[^\\w\\s+]', '', str(doc)).lower() \n",
    "    return [x.orth_ for x in nlp(preprocessed_doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos como vamos a representar el documento. En este caso, haremos lo mismo que `BoW`.\n",
    "\n",
    "Es decir, obtendremos la representaci√≥n vectorial de cada palabra (ahora como vector continuo) y luego las sumaremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:49.166630Z",
     "start_time": "2019-09-30T14:50:49.151006Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_wv(wv, document, aggregation_function= np.sum):\n",
    "    tokenized_document = tokenizer(document)\n",
    "    selected_wv = []\n",
    "\n",
    "    for token in tokenized_document:\n",
    "        if token in wv.vocab:\n",
    "            selected_wv.append(wv[token])\n",
    "\n",
    "    selected_wv = np.array(selected_wv)\n",
    "    return aggregation_function(selected_wv, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:52:34.760231Z",
     "start_time": "2019-09-30T14:50:53.362114Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# transformamos cada documento del dataset en la suma de los embeddings.\n",
    "procesed_wv = np.array([get_doc_wv(biobio_w2v.wv, item) for item in data.content.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:25.817110Z",
     "start_time": "2019-09-23T20:43:25.797172Z"
    }
   },
   "source": [
    "### Dividimos el dataset de embeddings en training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:52:38.855864Z",
     "start_time": "2019-09-30T14:52:38.818075Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(procesed_wv,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:24.788311Z",
     "start_time": "2019-09-30T14:52:42.829264Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0,\n",
    "                         solver='saga',\n",
    "                         multi_class='auto',\n",
    "                         max_iter=5000,\n",
    "                         n_jobs=4).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:47.337340Z",
     "start_time": "2019-09-23T20:43:47.317007Z"
    }
   },
   "source": [
    "### Predecimos y evaluamos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:29.446723Z",
     "start_time": "2019-09-30T14:59:29.415481Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:33.639984Z",
     "start_time": "2019-09-30T14:59:33.577449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 789   19  120   10   16]\n",
      " [  24 3205   38    2  120]\n",
      " [  87   29 3130   15   17]\n",
      " [  16    2   65  147    4]\n",
      " [  15  150   24    9  664]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:38.004822Z",
     "start_time": "2019-09-30T14:59:37.750625Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     economia       0.85      0.83      0.84       954\n",
      "internacional       0.94      0.95      0.94      3389\n",
      "     nacional       0.93      0.95      0.94      3278\n",
      "      opinion       0.80      0.63      0.71       234\n",
      "     sociedad       0.81      0.77      0.79       862\n",
      "\n",
      "     accuracy                           0.91      8717\n",
      "    macro avg       0.87      0.83      0.84      8717\n",
      " weighted avg       0.91      0.91      0.91      8717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:57:03.817026Z",
     "start_time": "2019-09-24T12:57:03.814008Z"
    }
   },
   "source": [
    "## En comparaci√≥n con BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:41.956520Z",
     "start_time": "2019-09-30T14:59:41.934383Z"
    }
   },
   "outputs": [],
   "source": [
    "def classificate_with_bow(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Tokenizar y lematizar.\n",
    "    def tokenizer_with_lemmatization(doc):\n",
    "        return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "    # Definimos el vectorizador para convertir el texto a BoW:\n",
    "    vectorizer = CountVectorizer(analyzer='word',\n",
    "                                 tokenizer=tokenizer_with_lemmatization,\n",
    "                                 \n",
    "                                 ngram_range=(1, 1))\n",
    "\n",
    "    # Definimos el clasificador que usaremos.\n",
    "    clf = LogisticRegression(solver='saga',\n",
    "                             multi_class='ovr',\n",
    "                             max_iter=5000,\n",
    "                             n_jobs=4)\n",
    "\n",
    "    # Definimos el pipeline\n",
    "    text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    clf_report = classification_report(y_test, predicted)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predicted)\n",
    "    print(clf_report, '\\n\\n', conf_matrix)\n",
    "\n",
    "    return predicted, conf_matrix, clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:45.765054Z",
     "start_time": "2019-09-30T14:59:45.749434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data.content,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:09:04.394161Z",
     "start_time": "2019-09-30T14:59:49.646878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     economia       0.88      0.83      0.85       954\n",
      "internacional       0.95      0.96      0.95      3389\n",
      "     nacional       0.94      0.96      0.95      3278\n",
      "      opinion       0.90      0.80      0.85       234\n",
      "     sociedad       0.83      0.82      0.83       862\n",
      "\n",
      "     accuracy                           0.93      8717\n",
      "    macro avg       0.90      0.87      0.89      8717\n",
      " weighted avg       0.93      0.93      0.93      8717\n",
      " \n",
      "\n",
      " [[ 795   13  122    4   20]\n",
      " [  12 3242   33    0  102]\n",
      " [  80   32 3139   12   15]\n",
      " [   8    3   29  188    6]\n",
      " [  13  113   23    4  709]]\n"
     ]
    }
   ],
   "source": [
    "bow_pred, bow_conf_matrix, bow_clf_report = classificate_with_bow(X_train_2, y_train_2, X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:09:08.823694Z",
     "start_time": "2019-09-30T15:09:08.804053Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     economia       0.88      0.83      0.85       954\n",
      "internacional       0.95      0.96      0.95      3389\n",
      "     nacional       0.94      0.96      0.95      3278\n",
      "      opinion       0.90      0.80      0.85       234\n",
      "     sociedad       0.83      0.82      0.83       862\n",
      "\n",
      "     accuracy                           0.93      8717\n",
      "    macro avg       0.90      0.87      0.89      8717\n",
      " weighted avg       0.93      0.93      0.93      8717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bow_clf_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
