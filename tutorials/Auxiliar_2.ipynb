{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivaci√≥n\" data-toc-modified-id=\"Motivaci√≥n-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Motivaci√≥n</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Bag of Words</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word Embeddings</a></span></li><li><span><a href=\"#Word2vec\" data-toc-modified-id=\"Word2vec-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word2vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Skip-gram\" data-toc-modified-id=\"Skip-gram-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Skip-gram</a></span></li></ul></li><li><span><a href=\"#Detalles-del-Modelo\" data-toc-modified-id=\"Detalles-del-Modelo-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Detalles del Modelo</a></span></li><li><span><a href=\"#La-capa-Oculta\" data-toc-modified-id=\"La-capa-Oculta-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>La capa Oculta</a></span></li><li><span><a href=\"#Fuentes\" data-toc-modified-id=\"Fuentes-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Fuentes</a></span></li></ul></li><li><span><a href=\"#Entrenar-nuestros-Embeddings\" data-toc-modified-id=\"Entrenar-nuestros-Embeddings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Entrenar nuestros Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset-desde-el-archivo\" data-toc-modified-id=\"Cargar-el-dataset-desde-el-archivo-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Cargar el dataset desde el archivo</a></span></li><li><span><a href=\"#Limpiar-las-noticias\" data-toc-modified-id=\"Limpiar-las-noticias-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Limpiar las noticias</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comenzamos-el-preprocesamiento:\" data-toc-modified-id=\"Comenzamos-el-preprocesamiento:-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Comenzamos el preprocesamiento:</a></span></li></ul></li></ul></li><li><span><a href=\"#Extracci√≥n-de-Frases\" data-toc-modified-id=\"Extracci√≥n-de-Frases-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extracci√≥n de Frases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Separamos-todos-los-documentos-en-tokens.\" data-toc-modified-id=\"Separamos-todos-los-documentos-en-tokens.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Separamos todos los documentos en tokens.</a></span></li><li><span><a href=\"#Buscamos-bigramas-relevantes\" data-toc-modified-id=\"Buscamos-bigramas-relevantes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Buscamos bigramas relevantes</a></span></li><li><span><a href=\"#Retokenizar-el-corpus\" data-toc-modified-id=\"Retokenizar-el-corpus-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Retokenizar el corpus</a></span></li></ul></li><li><span><a href=\"#Entrenamiento-del-Modelo\" data-toc-modified-id=\"Entrenamiento-del-Modelo-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Entrenamiento del Modelo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Construir-el-vocabulario\" data-toc-modified-id=\"Construir-el-vocabulario-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Construir el vocabulario</a></span></li><li><span><a href=\"#Entrenar-el-Modelo\" data-toc-modified-id=\"Entrenar-el-Modelo-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Entrenar el Modelo</a></span></li><li><span><a href=\"#Guardar-y-cargar-el-modelo\" data-toc-modified-id=\"Guardar-y-cargar-el-modelo-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Guardar y cargar el modelo</a></span></li></ul></li><li><span><a href=\"#Tareas:-Palabras-mas-similares-y-Analog√≠as\" data-toc-modified-id=\"Tareas:-Palabras-mas-similares-y-Analog√≠as-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tareas: Palabras mas similares y Analog√≠as</a></span><ul class=\"toc-item\"><li><span><a href=\"#Palabras-mas-similares\" data-toc-modified-id=\"Palabras-mas-similares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Palabras mas similares</a></span></li></ul></li><li><span><a href=\"#Analog√≠as\" data-toc-modified-id=\"Analog√≠as-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analog√≠as</a></span></li><li><span><a href=\"#Visualizar\" data-toc-modified-id=\"Visualizar-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Visualizar</a></span></li><li><span><a href=\"#Visualizamos\" data-toc-modified-id=\"Visualizamos-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Visualizamos</a></span></li><li><span><a href=\"#Word-Embeddings-como-caracter√≠sticas-para-clasificar\" data-toc-modified-id=\"Word-Embeddings-como-caracter√≠sticas-para-clasificar-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Word Embeddings como caracter√≠sticas para clasificar</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dividimos-el-dataset-de-embeddings-en-training-y-test\" data-toc-modified-id=\"Dividimos-el-dataset-de-embeddings-en-training-y-test-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Dividimos el dataset de embeddings en training y test</a></span></li><li><span><a href=\"#Entrenamos-el-clasificador\" data-toc-modified-id=\"Entrenamos-el-clasificador-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Entrenamos el clasificador</a></span></li><li><span><a href=\"#Predecimos-y-evaluamos:\" data-toc-modified-id=\"Predecimos-y-evaluamos:-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Predecimos y evaluamos:</a></span></li></ul></li><li><span><a href=\"#En-comparaci√≥n-con-BoW\" data-toc-modified-id=\"En-comparaci√≥n-con-BoW-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>En comparaci√≥n con BoW</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 2 - Word Embeddings\n",
    "\n",
    "\n",
    "La clase auxiliar de esta semana tendr√° varios objetivos: \n",
    "\n",
    "- Entender lo b√°sico de word embeddings \n",
    "- Entrenar nuestros propios `word embeddings` usando el dataset de noticias de la radio biobio. \n",
    "- Experimentar y visualizar los embeddings entrenados. \n",
    "- Por √∫ltimo, resolver la misma tarea de topic classification de la clase auxiliar 1, pero usando los embeddings que hemos calculado. \n",
    "\n",
    "\n",
    "\n",
    "## Motivaci√≥n\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Pensemos en estas 2 frases como documentos (dieciocheros ü•üü•üüç∑):\n",
    "\n",
    "    ¬°Estuvo buena esa empanada !\n",
    "    ¬°Estuvo espectacular esa empanada!\n",
    "\n",
    "En la practica, sabemos que significan lo mismo.\n",
    "\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "Para esto, generamos un modelo `Bag of Words` sobre cada documento (es decir, transformamos cada palabra a un vector one-hot):\n",
    "\n",
    "$$v = \\{estuvo, buena, esa, empanada, espectacular\\}$$\n",
    "\n",
    "Entonces, el doc 1 quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el doc 2 quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**¬øCu√°l es el problema?**\n",
    "\n",
    "`buena` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan en la pr√°ctica lo mismo, \n",
    "\n",
    "pero en esta representaci√≥n **son totalmente distintos**, ya que ocupan distintas dimensiones en el modelo. Esto hace que en la pr√°ctica, que `buena` y `espectacular` sean tan distintas como `estuvo` y `empanada`. Esto evidentemente, repercute en la calidad de los modelos que creamos.\n",
    "\n",
    "\n",
    "Nos gustar√≠a que eso no sucediera. Que existiera alg√∫n m√©todo que nos permitiera hacer que palabras similares tengan representaciones similares...\n",
    "\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Pensemos un poco en la `hip√≥tesis distribucional`. Esta plantea que:\n",
    "\n",
    "    \"palabras que ocurren en similares contextos tienden a tener significados similares\" \n",
    "\n",
    "¬øPodr√≠amos crear alg√∫na representaci√≥n vectorial que capture los contextos de las palabras? - La respuesta es si. Estos son los **Word Embeddings**. \n",
    "\n",
    "La idea principal de Word Embeddings (O *Word vectors*) es basarse en la hipotesis distribuci√≥nal para crear, por cada palabra del vocabulario, representaciones vectoriales continuas que capturen su contexto.\n",
    "\n",
    "\n",
    "Es decir, en nuestro ejemplo, `buena` y `espectacular` ocurren en el mismo contexto, por lo que los word embeddings que los deber√≠an representan deber√≠an ser muy similares (ejemplos de mentira hechos a mano*):\n",
    "\n",
    "`buena` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `empanada`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cu√°l es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¬øC√≥mo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "\n",
    "###  Word2vec\n",
    "\n",
    "Word2Vec es una de las herramientas que nos permitiran construir estos vectores. Consiste en entrenar una **red neuronal de solo 1 capa oculta** a trav√©s de la resoluci√≥n de una task auxiliar: `Skip-Gram`\n",
    "\n",
    "¬øPor qu√© auxiliar?: Porque la usaremos solo para entrenar los pesos de la capa oculta de red. Una vez entrenada, nunca mas haremos Skip-Gram!. \n",
    "\n",
    "#### Skip-gram\n",
    "\n",
    "En pocas palabras, la task a resolver es la siguiente: \n",
    "\n",
    "- Toma una oraci√≥n cualquiera del corpus. \n",
    "\n",
    "- Dada una palabra cualquiera en la oraci√≥n, mira las palabras cercanas a esta (dentro de una ventana definida) y toma una aleatoria. \n",
    "\n",
    "- La red deber√° predecir que tan probable es que esta palabra sea cercana a la palabra que escogimos.\n",
    "\n",
    "\n",
    "Por ejemplo, para la t√≠pica frase: \n",
    "\n",
    "     ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
    "     \n",
    "Usando una ventana de 2 palabras, tendremos: \n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "Esto es lo que luego usar√° la red para aprender las relaciones entre las palabras.\n",
    "\n",
    "### Detalles del Modelo\n",
    "\n",
    "\n",
    "Pensemos por ejemplo, en un paso del entrenamiento: \n",
    "\n",
    "El vector de entrada ser√° un One-hot de la palabra que estemos viendo en ese momento. En este caso, `ants`.\n",
    "\n",
    "La red, usando su capa oculta, nos entregar√° la probabilidad de la que la palabra que estamos viendo aparezca con respecto a cada palabra del vocabulario.\n",
    "\n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "Nota: Esto es computacionalmente una locura, ya que por cada palabra debemos calcular la probabilidad de aparici√≥n de todas las otras. Imaginen el caso de un vocabulario de 10 millones de palabras...\n",
    "En clases se ver√° como se puede hacer esto de forma eficiente.\n",
    "\n",
    "Por ende, la salida ser√° un vector que representar√° una distribuci√≥n de probabilidades. \n",
    "\n",
    "Durante el entrenamiento, dicho vector ser√° comparado con las estad√≠sticas reales de co-ocurrencia de las palabras.\n",
    "\n",
    "### La capa Oculta\n",
    "\n",
    "Al terminar el entrenamiento, ¬øQu√© nos queda en la capa oculta?\n",
    "\n",
    "B√°sicamente, una matriz de $v$ filas por $300$ columnas, la cual contiene lo que buscabamos: Una representaci√≥n continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representaci√≥n continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)\n",
    "\n",
    "¬øC√≥mo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot del BoW y las multiplicamos por la matriz:\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "\n",
    "Word2vec:\n",
    "- mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "Gensim: \n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "Nota: Todas las imagenes pertenecen a [Chris McCormick](http://mccormickml.com/about/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementaci√≥n de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:30:46.315573Z",
     "start_time": "2019-09-24T14:30:42.817847Z"
    }
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "\n",
    "# spacy\n",
    "import spacy \n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Cargar modelos de spacy en espa√±ol.\n",
    "nlp = spacy.load(\"es_core_news_sm\",  disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset desde el archivo\n",
    "\n",
    "Nota: Pandas descomprime por si mismo el archivo bz2. Pueden descomprimirlo manualmente usando 7zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:27.188964Z",
     "start_time": "2019-09-24T14:41:26.549218Z"
    }
   },
   "outputs": [],
   "source": [
    "# leer desde directorio local\n",
    "#data = pd.read_json('./datasets/biobio_clean.bz2', encoding = \"utf-8\")\n",
    "\n",
    "#leer desde github\n",
    "data = pd.read_json('https://github.com/dccuchile/CC6205/blob/master/tutorials/datasets/biobio_clean.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T15:44:59.327518Z",
     "start_time": "2019-08-26T15:44:59.312300Z"
    }
   },
   "source": [
    "**Examinemos un par de ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:32.588592Z",
     "start_time": "2019-09-24T14:41:32.570679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar las noticias\n",
    "\n",
    "Primero, vamos a definir la funci√≥n que definir√° como ser√° limpiado y tokenizado cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:34.588581Z",
     "start_time": "2019-09-24T14:41:34.583595Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Tokenizar y remover  stopwords\n",
    "    txt = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaci√≥n:  `Word2Vec` usa el contexto de las oraciones para aprender las representaciones de las palabras.\n",
    "Si la oraci√≥n es muy peque√±a, entonces, el entrenamiento no podr√° inferir nada. Por eso, se retorna la oraci√≥n solo cuando hay mas de 2 palabras dentro de la oraci√≥n.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comenzamos el preprocesamiento:\n",
    "\n",
    "Primero convertimos todas las palabras del documento en min√∫sculas y realizamos una limpieza r√°pida de s√≠mbolos y espacios extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:35.072032Z",
     "start_time": "2019-09-24T14:41:35.067071Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_cleaned_docs = (re.sub(r'[^\\w\\s+]', '', str(row)).lower() for row in data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, tokenizamos el texto usando la funci√≥n que definimos unas celdas mas arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:42:56.915741Z",
     "start_time": "2019-09-24T14:41:36.315711Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(pre_cleaned_docs, batch_size=5000, n_threads=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:42:57.970692Z",
     "start_time": "2019-09-24T14:42:57.964708Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ejemplo de texto procesado:\\n\\n{}\".format(txt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo, almacenamos los documentos tokenizados en un `DataFrame` y eliminamos vacios/ducplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:42:59.216401Z",
     "start_time": "2019-09-24T14:42:58.936111Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "## Extracci√≥n de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`.\n",
    "\n",
    "\n",
    "### Separamos todos los documentos en tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:00.805427Z",
     "start_time": "2019-09-24T14:43:00.189761Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:03.630837Z",
     "start_time": "2019-09-24T14:43:03.624853Z"
    }
   },
   "outputs": [],
   "source": [
    "# para ver como quedan las noticias, quitar comentario a la siguiente linea:\n",
    "sent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscamos bigramas relevantes\n",
    "\n",
    "Ahora, buscamos los bigramas relevantes dentro de nuesto corpus usando `Phrases`. La condici√≥n para que sean considerados es que aparezcan por lo menos 30 veces repetidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:16.081560Z",
     "start_time": "2019-09-24T14:43:06.488201Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenizar el corpus\n",
    "\n",
    "Por √∫ltimo, usando `Phraser`, re-tokenizamos el corpus con los bigramas encontrados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:46.938115Z",
     "start_time": "2019-09-24T14:43:18.940918Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:49.749575Z",
     "start_time": "2019-09-24T14:43:49.743591Z"
    }
   },
   "outputs": [],
   "source": [
    "# para ver como quedan las noticias retokenizadas, quitar comentario a la siguiente linea:\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos par√°metros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada\n",
    "- `size` : El tama√±o de los embeddings que crearemos. Por lo general, se utilizan 300\n",
    "- `workers`: Cantidad de CPU que ser√°n utilizadas en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:52.676789Z",
     "start_time": "2019-09-24T14:43:52.670767Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v = Word2Vec(min_count=10,\n",
    "                      window=2,\n",
    "                      size=300,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se crear√° un conjunto que contendr√° (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:24:34.585495Z",
     "start_time": "2019-09-24T14:24:19.317916Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.build_vocab(sentences, progress_per=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuaci√≥n, entenaremos el modelo. \n",
    "Los par√°metros que usaremos ser√°n: \n",
    "\n",
    "- `total_examples`: N√∫mero de documentos.\n",
    "- `epochs`: N√∫mero de veces que se iterar√° sobre el corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T16:57:49.936306Z",
     "start_time": "2019-08-29T16:48:47.066955Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=30, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. \n",
    "Esto nos permitir√° ejecutar eficientemente las tareas que realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T16:57:56.282837Z",
     "start_time": "2019-08-29T16:57:55.929734Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:49:23.897409Z",
     "start_time": "2019-09-24T14:49:23.462536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Si entrenaste el modelo y lo quieres guardar, eliminar el comentario de la siguiente linea.\n",
    "#biobio_w2v.save('./pretrained_models/biobio_w2v.model')\n",
    "\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n",
    "\n",
    "def load_model_from_github():\n",
    "    import requests\n",
    "    fnames = ['biobio_w2v.model', 'biobio_w2v.model.trainables.syn1neg.npy', 'biobio_w2v.model.wv.vectors.npy']\n",
    "    url = 'https://github.com/dccuchile/CC6205/tree/master/tutorials/pretrained_models/'\n",
    "    for fname in fnames:\n",
    "        r = requests.get(url + fname)\n",
    "        open(fname , 'wb').write(r.content)\n",
    "\n",
    "# cargar desde github (eliminar el comentario)\n",
    "#load_model_from_github()        \n",
    "\n",
    "# cargar desde local o github\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas: Palabras mas similares y Analog√≠as\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la informaci√≥n contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matem√°tico, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:49:27.192117Z",
     "start_time": "2019-09-24T14:49:27.104351Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"perro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:42.947043Z",
     "start_time": "2019-09-23T20:13:42.916806Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:45.746655Z",
     "start_time": "2019-09-23T20:13:45.726960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revoluci√≥n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:48.487053Z",
     "start_time": "2019-09-23T20:13:48.466879Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revoluci√≥n_democr√°tica\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:51.246820Z",
     "start_time": "2019-09-23T20:13:51.227020Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:54.006593Z",
     "start_time": "2019-09-23T20:13:53.987031Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"uber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:56.726861Z",
     "start_time": "2019-09-23T20:13:56.706907Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"huawei\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:59.521901Z",
     "start_time": "2019-09-23T20:13:59.506636Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"tvn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:51:23.372000Z",
     "start_time": "2019-08-26T21:51:23.363072Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"mapuches\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:51:51.446650Z",
     "start_time": "2019-08-26T21:51:51.438671Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"catrillanca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:52:35.655340Z",
     "start_time": "2019-08-26T21:52:35.645367Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"universidad_chile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:14:06.366867Z",
     "start_time": "2019-09-23T20:14:06.340254Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"ultraderechista\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:14:26.707154Z",
     "start_time": "2019-09-23T20:14:26.687181Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pinochet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:14:15.636723Z",
     "start_time": "2019-09-23T20:14:15.616965Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"serena\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analog√≠as\n",
    "\n",
    "Por otra parte, la analog√≠a consiste en comparar 3 terminos mediante una operaci√≥n del estilo: \n",
    "\n",
    "$$palabra1 - palabra2 \\approx palabra 3 - x$$\n",
    "\n",
    "para encontrar relaciones entre estos.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| palabra 1 (pos) |  palabra 2 (neg) |\n",
    "|-----------------|------------------|\n",
    "|  macri          | pi√±era           |\n",
    "| chile           |  x               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:15:14.846789Z",
     "start_time": "2019-09-23T20:15:14.827133Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"bolsonaro\", \"argentina\"], negative=['macri'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:15:27.566927Z",
     "start_time": "2019-09-23T20:15:27.547155Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"chile\", \"huawei\"], negative=['china'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar\n",
    "\n",
    "Para visualizar, usaremos una t√©cnica de reducci√≥n de dimensionalidad llamada [`T-SNE`](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "\n",
    "B√°sicamente, ejecuta una reducci√≥n de dimensionalidad, que transforma las 300 dimensiones de los embeddings en 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:10.628609Z",
     "start_time": "2019-08-29T17:10:44.814442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutamos la transformaci√≥n. Por lo general, demora una media hora mas o menos...\n",
    "X_embedded = TSNE(n_components=2, verbose=True).fit_transform(\n",
    "    biobio_w2v.wv[biobio_w2v.wv.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la frecuencia de las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:16.690620Z",
     "start_time": "2019-08-29T17:39:16.666685Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = dict()\n",
    "for item in biobio_w2v.wv.vocab:\n",
    "    word_counts[item]=biobio_w2v.wv.vocab[item].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nuevos vectores que nos entrego TSNE mas el vocabulario y la frecuencia del vocabulario, formamos un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:22.784472Z",
     "start_time": "2019-08-29T17:39:22.757545Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = pd.DataFrame({\n",
    "    'x': X_embedded[:,0],\n",
    "    'y': X_embedded[:,1],\n",
    "    'vocab': list(biobio_w2v.wv.vocab.keys()) , \n",
    "    'count': list(word_counts.values())\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guardar y Cargar TSNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:14.072636Z",
     "start_time": "2019-09-24T14:50:13.465573Z"
    }
   },
   "outputs": [],
   "source": [
    "#Guardar\n",
    "#tsne.to_csv('./pretrained_models/biobio_w2v_tsne.csv', index= False, index_label=False)\n",
    "# Cargar\n",
    "#tsne = pd.read_csv('./pretrained_models/biobio_w2v_tsne.csv')\n",
    "\n",
    "# Cargar desde github\n",
    "tsne = pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/pretrained_models/biobio_w2v_tsne.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos el Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:16.969679Z",
     "start_time": "2019-09-24T14:50:16.958719Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tsne = tsne.sort_values(by=['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos\n",
    "\n",
    "Para la visualizaci√≥n, usaremos plotly. Este nos deja tener una barra interactiva que nos mostrar√° las 100 palabras mas parecidas a la que escribimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:19.907893Z",
     "start_time": "2019-09-24T14:50:19.903903Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne_sample = sorted_tsne.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:22.633640Z",
     "start_time": "2019-09-24T14:50:22.613691Z"
    }
   },
   "outputs": [],
   "source": [
    "described_tsne = tsne_sample.describe()\n",
    "min_y = described_tsne.y['min']\n",
    "max_y = described_tsne.y['max']\n",
    "\n",
    "min_x =  described_tsne.x['min']\n",
    "max_x =  described_tsne.x['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:25.381294Z",
     "start_time": "2019-09-24T14:50:25.366339Z"
    }
   },
   "outputs": [],
   "source": [
    "textbox = widgets.Text(description='Palabra:')\n",
    "\n",
    "def validate(word):\n",
    "    if word in tsne_sample.vocab.values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def response(change):\n",
    "\n",
    "    word = textbox.value.lower()\n",
    "    \n",
    "    if (word == ''):\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = tsne_sample.x.values\n",
    "            fig.data[0].y = tsne_sample.y.values\n",
    "            fig.data[0].text = tsne_sample.vocab.values\n",
    "    else :\n",
    "        if validate(word):\n",
    "\n",
    "            most_similar_words = [word]\n",
    "\n",
    "            for word_tuple in biobio_w2v.wv.most_similar(positive=[word],topn = 100 ):\n",
    "                if word_tuple[0] in tsne_sample.vocab.values:\n",
    "                    most_similar_words.append(word_tuple[0])\n",
    "\n",
    "            filtered_words = [word in most_similar_words for word in tsne_sample.vocab]\n",
    "            temp = tsne_sample.loc[filtered_words]\n",
    "\n",
    "            with fig.batch_update():\n",
    "                fig.data[0].x = temp.x.values\n",
    "                fig.data[0].y = temp.y.values\n",
    "                fig.data[0].text = temp.vocab.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:29.606877Z",
     "start_time": "2019-09-24T14:50:28.112993Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.FigureWidget(data=[\n",
    "    go.Scatter(x=tsne_sample.x,\n",
    "               y=tsne_sample.y,\n",
    "               mode='markers',\n",
    "               text=tsne_sample.vocab)\n",
    "],\n",
    "                      layout=go.Layout(title=go.layout.Title(\n",
    "                          text=\"Visualizaci√≥n 2D Embeddings Biobio\"),\n",
    "                                       yaxis =dict(range=[min_y, max_y]),\n",
    "                                       xaxis =dict(range=[min_x, max_x])\n",
    "                                      ))\n",
    "\n",
    "container = widgets.HBox([textbox])\n",
    "textbox.observe(response, names=\"value\")\n",
    "\n",
    "widgets.VBox([container, fig])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T19:55:09.459852Z",
     "start_time": "2019-08-28T19:55:09.454865Z"
    }
   },
   "source": [
    "## Word Embeddings como caracter√≠sticas para clasificar\n",
    "\n",
    "\n",
    "En esta secci√≥n, veremos como utilizar los word embeddings como caracter√≠stica para **clasificar nuevamente el t√≥pico de las noticias de la radio biobio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:58:12.927399Z",
     "start_time": "2019-09-24T12:58:12.921416Z"
    }
   },
   "source": [
    "Definimos el tokenizador (el mismo de antes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:31:11.371736Z",
     "start_time": "2019-09-24T13:31:11.366776Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    preprocessed_doc = re.sub(r'[^\\w\\s+]', '', str(doc)).lower() \n",
    "    return [x.orth_ for x in nlp(preprocessed_doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos como vamos a representar el documento. En este caso, haremos lo mismo que `BoW`.\n",
    "\n",
    "Es decir, obtendremos la representaci√≥n vectorial de cada palabra (ahora como vector continuo) y luego las sumaremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:42:32.875456Z",
     "start_time": "2019-09-24T13:42:32.870442Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_wv(wv, document, aggregation_function= np.mean):\n",
    "    tokenized_document = tokenizer(document)\n",
    "    selected_wv = []\n",
    "\n",
    "    for token in tokenized_document:\n",
    "        if token in wv.vocab:\n",
    "            selected_wv.append(wv[token])\n",
    "\n",
    "    selected_wv = np.array(selected_wv)\n",
    "    return aggregation_function(selected_wv, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:44:03.869709Z",
     "start_time": "2019-09-24T13:42:33.594713Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# transformamos cada documento del dataset en la suma de los embeddings.\n",
    "procesed_wv = np.array([get_doc_wv(biobio_w2v.wv, item) for item in data.content.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:25.817110Z",
     "start_time": "2019-09-23T20:43:25.797172Z"
    }
   },
   "source": [
    "### Dividimos el dataset de embeddings en training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:09.609721Z",
     "start_time": "2019-09-24T13:46:09.581768Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(procesed_wv,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:14.109965Z",
     "start_time": "2019-09-24T13:46:10.839565Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0,\n",
    "                         solver='saga',\n",
    "                         multi_class='auto',\n",
    "                         max_iter=5000,\n",
    "                         n_jobs=4).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:47.337340Z",
     "start_time": "2019-09-23T20:43:47.317007Z"
    }
   },
   "source": [
    "### Predecimos y evaluamos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:16.403640Z",
     "start_time": "2019-09-24T13:46:16.395688Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:17.245827Z",
     "start_time": "2019-09-24T13:46:16.975547Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:57:03.817026Z",
     "start_time": "2019-09-24T12:57:03.814008Z"
    }
   },
   "source": [
    "## En comparaci√≥n con BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:10:50.593431Z",
     "start_time": "2019-09-24T14:10:50.586451Z"
    }
   },
   "outputs": [],
   "source": [
    "def classificate_with_bow(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Tokenizar y lematizar.\n",
    "    def tokenizer_with_lemmatization(doc):\n",
    "        return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "    # Definimos el vectorizador para convertir el texto a BoW:\n",
    "    vectorizer = CountVectorizer(analyzer='word', tokenizer = tokenizer_with_lemmatization, ngram_range=(1,1))  \n",
    "\n",
    "    # Definimos el clasificador que usaremos.\n",
    "    clf = LogisticRegression(solver='saga', multi_class='ovr', max_iter = 1000, n_jobs=4)   \n",
    "\n",
    "    # Definimos el pipeline\n",
    "    text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])\n",
    "    \n",
    "    text_clf.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = text_clf.predict(X_test)\n",
    "    \n",
    "    clf_report = classification_report(y_test, predicted)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, predicted)\n",
    "    print(clf_report, '\\n\\n', conf_matrix)\n",
    "    \n",
    "    return predicted, conf_matrix, clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:10:47.272602Z",
     "start_time": "2019-09-24T14:10:47.257642Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data.content,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:14:43.553064Z",
     "start_time": "2019-09-24T14:10:51.919560Z"
    }
   },
   "outputs": [],
   "source": [
    "bow_pred, bow_conf_matrix, bow_clf_report = classificate_with_bow(X_train_2, y_train_2, X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:14:43.671747Z",
     "start_time": "2019-09-24T14:14:43.661774Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(bow_clf_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "189.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
