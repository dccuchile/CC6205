{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#üìö-Objetivos-de-la-clase-üìö\" data-toc-modified-id=\"üìö-Objetivos-de-la-clase-üìö-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>üìö Objetivos de la clase üìö</a></span></li><li><span><a href=\"#Motivaci√≥n\" data-toc-modified-id=\"Motivaci√≥n-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Motivaci√≥n</a></span><ul class=\"toc-item\"><li><span><a href=\"#El-gran-problema-de-Bag-of-Words\" data-toc-modified-id=\"El-gran-problema-de-Bag-of-Words-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>El gran problema de Bag of Words</a></span></li><li><span><a href=\"#Hip√≥tesis-Distribucional\" data-toc-modified-id=\"Hip√≥tesis-Distribucional-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Hip√≥tesis Distribucional</a></span></li><li><span><a href=\"#Word-Context-Matrices\" data-toc-modified-id=\"Word-Context-Matrices-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Word-Context Matrices</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Word Embeddings</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Word2vec-y-Skip-gram\" data-toc-modified-id=\"Word2vec-y-Skip-gram-2.4.0.1\"><span class=\"toc-item-num\">2.4.0.1&nbsp;&nbsp;</span>Word2vec y Skip-gram</a></span></li></ul></li><li><span><a href=\"#Detalles-del-Modelo\" data-toc-modified-id=\"Detalles-del-Modelo-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Detalles del Modelo</a></span></li></ul></li><li><span><a href=\"#La-capa-Oculta-y-los-Embeddings\" data-toc-modified-id=\"La-capa-Oculta-y-los-Embeddings-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>La capa Oculta y los Embeddings</a></span></li><li><span><a href=\"#Fuentes\" data-toc-modified-id=\"Fuentes-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Fuentes</a></span></li></ul></li><li><span><a href=\"#Entrenar-nuestros-Embeddings\" data-toc-modified-id=\"Entrenar-nuestros-Embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Entrenar nuestros Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset-y-limpiar\" data-toc-modified-id=\"Cargar-el-dataset-y-limpiar-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Cargar el dataset y limpiar</a></span></li><li><span><a href=\"#Extracci√≥n-de-Frases\" data-toc-modified-id=\"Extracci√≥n-de-Frases-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Extracci√≥n de Frases</a></span></li><li><span><a href=\"#Definir-el-modelo\" data-toc-modified-id=\"Definir-el-modelo-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Definir el modelo</a></span></li><li><span><a href=\"#Construir-el-vocabulario\" data-toc-modified-id=\"Construir-el-vocabulario-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Construir el vocabulario</a></span></li><li><span><a href=\"#Entrenar-el-Modelo\" data-toc-modified-id=\"Entrenar-el-Modelo-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Entrenar el Modelo</a></span></li><li><span><a href=\"#Guardar-y-cargar-el-modelo\" data-toc-modified-id=\"Guardar-y-cargar-el-modelo-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Guardar y cargar el modelo</a></span></li></ul></li><li><span><a href=\"#Tasks:-Palabras-mas-similares-y-Analog√≠as\" data-toc-modified-id=\"Tasks:-Palabras-mas-similares-y-Analog√≠as-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Tasks: Palabras mas similares y Analog√≠as</a></span><ul class=\"toc-item\"><li><span><a href=\"#Palabras-mas-similares\" data-toc-modified-id=\"Palabras-mas-similares-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Palabras mas similares</a></span></li><li><span><a href=\"#Analog√≠as\" data-toc-modified-id=\"Analog√≠as-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Analog√≠as</a></span></li><li><span><a href=\"#Visualizar\" data-toc-modified-id=\"Visualizar-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Visualizar</a></span></li></ul></li><li><span><a href=\"#Word-Embeddings-como-caracter√≠sticas-para-clasificar\" data-toc-modified-id=\"Word-Embeddings-como-caracter√≠sticas-para-clasificar-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Word Embeddings como caracter√≠sticas para clasificar</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset\" data-toc-modified-id=\"Cargar-el-dataset-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Cargar el dataset</a></span></li><li><span><a href=\"#Dividir-el-dataset-en-training-y-test\" data-toc-modified-id=\"Dividir-el-dataset-en-training-y-test-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Dividir el dataset en training y test</a></span></li><li><span><a href=\"#Doc2vec\" data-toc-modified-id=\"Doc2vec-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Doc2vec</a></span></li><li><span><a href=\"#Definimos-el-pipeline\" data-toc-modified-id=\"Definimos-el-pipeline-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Definimos el pipeline</a></span></li></ul></li><li><span><a href=\"#Usandolo-con-BoW\" data-toc-modified-id=\"Usandolo-con-BoW-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Usandolo con BoW</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bias-en-Embeddings\" data-toc-modified-id=\"Bias-en-Embeddings-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Bias en Embeddings</a></span></li><li><span><a href=\"#Propuesto...\" data-toc-modified-id=\"Propuesto...-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Propuesto...</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T21:20:42.007937Z",
     "start_time": "2020-05-05T21:20:41.987638Z"
    }
   },
   "source": [
    "# Auxiliar 2 - Word Embeddings\n",
    "\n",
    "\n",
    "## üìö Objetivos de la clase üìö\n",
    "\n",
    "La clase auxiliar de esta semana tendr√° varios objetivos: \n",
    "\n",
    "- Motivaci√≥n y repaso de qu√© son los Word Embeddings. \n",
    "- Entrenar nuestros propios `word embeddings` usando el dataset de noticias de la radio biobio. \n",
    "- Experimentar y visualizar los embeddings entrenados. \n",
    "- Por √∫ltimo, resolver la misma tarea de la clase auxiliar 1, pero ahora usando los embeddings que hemos calculado. \n",
    "\n",
    "\n",
    "Para desarrollar ustedes mismos el auxiliar, necesitar√°n los siguientes paquetes: \n",
    "\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `gensim`\n",
    "- `plotly`\n",
    "- `sklearn`\n",
    "- `wefe`\n",
    "\n",
    "Una vez resuelto, pueden utilizar cualquier parte del c√≥digo que les parezca prudente para la tarea 1 (que tambi√©n es de clasificaci√≥n de texto! üòä).\n",
    "\n",
    "\n",
    "El notebook del auxiliar ya ejecutado se encuentra en el [github](https://github.com/dccuchile/CC6205/tree/master/tutorials) del curso (Recuerden dejar su Star ‚≠êüòâ!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivaci√≥n\n",
    "\n",
    "### El gran problema de Bag of Words\n",
    "\n",
    "Pensemos en estas 3 frases como documentos:\n",
    "\n",
    "- $doc_1$: `¬°Buen√≠sima la marraqueta!`\n",
    "- $doc_2$: `¬°Estuvo espectacular ese pan franc√©s!`\n",
    "- $doc_3$: `!Buen√≠sima esa pintura!`\n",
    "\n",
    "Sabemos $doc_1$ y $doc_2$ hablan de lo mismo üçûüçûüëå y que $doc_3$ üé® no tiene mucho que ver con los otros.\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "Para esto, generamos un modelo `Bag of Words` sobre el documento. Es decir, transformamos cada palabra a un vector one-hot y luego los sumamos por documento. Adem√°s, omitimos algunas stopwords y consideramos pan frances como un solo token.\n",
    "\n",
    "$$v = \\{buen√≠sima, marraqueta, estuvo, espectacular, pan\\ franc√©s, pintura\\}$$\n",
    "\n",
    "Entonces, el $\\vec{doc_1}$ quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "El $\\vec{doc_2}$ quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 0\\end{bmatrix} = \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el $\\vec{doc_3}$: \n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "**¬øCu√°l es el problema?**\n",
    "\n",
    "`buen√≠sima` $\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan ideas muy similares. Por otra parte, sabemos que `marraqueta` $\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `pan franc√©s` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\0\\end{bmatrix}$ se refieren al mismo objeto. Pero en este modelo, estos **son totalmente distintos**. Es decir, los vectores de las palabras que `buen√≠sima` y `espectacular` son tan distintas como `marraqueta` y `pan franc√©s`. Esto evidentemente, repercute en la calidad de los modelos que creamos a partir de nuestro Bag of Words.\n",
    "\n",
    "Ahora, si queremos ver que documento es mas similar a otro usando distancia euclidiana, veremos que:\n",
    "\n",
    "$$d(doc_1, doc_2) = 2.236$$\n",
    "$$d(doc_1, doc_3) = 1.414$$\n",
    "\n",
    "Es decir, $doc_1$ se parece mas a $doc_3$ aunque nosotros sabemos que $doc_1$ y $doc_2$ nos est√°n diciendo lo mismo!\n",
    "\n",
    "\n",
    "Nos gustar√≠a que eso no sucediera. Que existiera alg√∫n m√©todo que nos permitiera hacer que palabras similares tengan representaciones similares. Y que con estas, representemos mejor a los documentos.\n",
    "\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hip√≥tesis Distribucional\n",
    "\n",
    "Estamos buscando alg√∫n enfoque que nos permita representar las palabras de forma no aislada, si no como algo que adem√°s capture el significado de esta.\n",
    "\n",
    "Pensemos un poco en la **hip√≥tesis distribucional**. Esta plantea que:\n",
    "\n",
    "    \"Palabras que ocurren en contextos iguales tienden a tener significados similares.\" \n",
    "\n",
    "O equivalentemente,\n",
    "\n",
    "    \"Una palabra es caracterizada por la compa√±√≠a que esta lleva.\"\n",
    "\n",
    "Esto nos puede hacer pensar que podr√≠amos usar los contextos de las palabras para generar vectores que describan mejor dichas palabras: en otras palabras, los `Distributional Vectors`.\n",
    "\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Context Matrices\n",
    "\n",
    "Es una matriz donde cada celda $(i,j)$ representa la co-ocurrencia entre una palabra objetivo/centro $w_i$ y un contexto $c_j$. El contexto son las palabras dentro de ventana de tama√±o $k$ que rodean la palabra central. \n",
    "\n",
    "Cada fila representa a una palabra a trav√©s de su contexto. Como pueden ver, ya no es un vector one-hot, si no que ahora contiene mayor informaci√≥n.\n",
    "\n",
    "El tama√±o de la matriz es el tama√±o del vocabulario $V$ al cuadrado. Es decir $|V|*|V|$.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/pics/distributionalSocher.png\" alt=\"Word-context matrices\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "**Problema: Creada a partir de un corpus respetable, es gigantezca**. \n",
    "\n",
    "Por ejemplo, para $|v| = 100.000$, la matriz tendr√° $\\frac{100000 * 100000 * 4}{10^9} = 40gb $.\n",
    "\n",
    "- Es caro mantenerla en memoria \n",
    "- Los clasificadores no funcionan tan bien con tantas dimensiones (ver [maldici√≥n de la dimensionalidad](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)).\n",
    "\n",
    "¬øHabr√° una mejor soluci√≥n?\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "\n",
    "La idea principal de los Word Embeddings es crear representaciones vectoriales densas y de baja dimensionalidad $(d << |V|)$ de las palabras a partir de su contexto.  Para esto, se usan distintos modelos que emplean redes neuronales *shallow* o poco profundas.\n",
    "\n",
    "Volvamos a nuestro ejemplo anterior: `buen√≠sima` y `espectacular` ocurren muchas veces en el mismo contexto, por lo que los embeddings que los representan debiesen ser muy similares... (*ejemplos de mentira hechos a mano*):\n",
    "\n",
    "`buen√≠sima` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `marraqueta`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cu√°l es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¬øC√≥mo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "- Depender√° del modelo que utilizemos.\n",
    "\n",
    "\n",
    "##### Word2vec y Skip-gram\n",
    "\n",
    "Word2Vec es probablemente el paquete de software mas famoso para crear word embeddings. Este nos provee herramientas para crear distintos tipos de modelos, tales como `Skip-Gram` y `Continuous Bag of Word (CBOW)`. En este caso, solo veremos `Skip-Gram`.\n",
    "\n",
    "**Skip-gram** es una task auxiliar con la que crearemos nuestros embeddings. Esta consiste en que por cada palabra del dataset, predigamos las palabras de su contexto (las palabras presentes en ventana de alg√∫n tama√±o $k$).\n",
    "\n",
    "Para resolverla, usaremos una red de una sola capa oculta. Los pesos ya entrenados de esta capa ser√°n los que usaremos como embeddings.\n",
    "\n",
    "#### Detalles del Modelo\n",
    "\n",
    "- Como dijimos, el modelo ser√° una red de una sola capa. La capa oculta tendr√° una dimensi√≥n $d$ la cual nosotros determinaremos. Esta capa no tendr√° funci√≥n de activaci√≥n. Sin embargo, la de salida si, la cual ser√° una softmax.\n",
    "\n",
    "- El vector de entrada, de tama√±o $|V|$, ser√° un vector one-hot de la palabra que estemos viendo en ese momento.\n",
    "\n",
    "- La salida, tambi√©n de tama√±o $|V|$, ser√° un vector que contenga la distribuci√≥n de probabilidad de que cada palabra del vocabulario pertenezca al contexto de la palabra de entrada.\n",
    "\n",
    "- Al entrenar, se comparar√° la distribuci√≥n de los contextos con la suma de los vectores one-hot del contexto real.\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"Skip Gram\" style=\"width: 600px;\"/>\n",
    "\n",
    "Nota: Esto es computacionalmente una locura. Por cada palabra de entrada, debemos calcular la probabilidad de aparici√≥n de todas las otras. Imaginen el caso de un vocabulario de 100.000 de palabras y de 10000000 oraciones...\n",
    "\n",
    "La soluci√≥n a esto es modificar la task a *Negative Sampling*. Esta transforma este problema de $|V|$ clases a uno binario. Sin embargo, no lo veremos por el tiempo, pero est√°n muy bien explicado en el [video de la c√°tedra](https://www.youtube.com/watch?v=XDxzQ7JU95U&feature=youtu.be).\n",
    "\n",
    "\n",
    "### La capa Oculta y los Embeddings\n",
    "\n",
    "Al terminar el entrenamiento, ¬øQu√© nos queda en la capa oculta?\n",
    "\n",
    "Una matriz de $v$ filas por $d$ columnas, la cual contiene lo que buscabamos: Una representaci√≥n continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representaci√≥n continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png\" alt=\"Capa Oculta 1\" style=\"width: 400px;\"/>\n",
    "\n",
    "¬øC√≥mo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot de la entrada y las multiplicamos por la matriz:\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\" alt=\"Skip Gram\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "\n",
    "Word2vec:\n",
    "- mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "Gensim: \n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "Nota: Las √∫ltimas 3 imagenes pertenecen a [Chris McCormick](http://mccormickml.com/about/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementaci√≥n de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:26:56.028758Z",
     "start_time": "2020-05-17T20:26:52.011925Z"
    }
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset y limpiar\n",
    "\n",
    "Nota: Pandas descomprime por si mismo el archivo bz2. Pueden descomprimirlo manualmente usando 7zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.022904Z",
     "start_time": "2020-05-17T20:26:56.037686Z"
    }
   },
   "outputs": [],
   "source": [
    "# descargamos el dataset completo (~40mb)\n",
    "dataset = pd.read_json(\n",
    "    'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_clean.bz2',\n",
    "    encoding=\"utf-8\")\n",
    "\n",
    "dataset_r = dataset.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.136649Z",
     "start_time": "2020-05-17T20:27:38.050830Z"
    }
   },
   "outputs": [],
   "source": [
    "# unir titulo con contenido de la noticia\n",
    "content = dataset['title'] + dataset['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.183476Z",
     "start_time": "2020-05-17T20:27:38.175498Z"
    }
   },
   "outputs": [],
   "source": [
    "content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:27:38.227359Z",
     "start_time": "2020-05-17T20:27:38.221374Z"
    }
   },
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T21:21:12.916380Z",
     "start_time": "2020-05-07T21:21:04.087799Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# limpiar puntuaciones y separar por tokens.\n",
    "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
    "stopwords = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
    ").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n",
    "\n",
    "\n",
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "\n",
    "    tokenized_doc = [\n",
    "        token for token in tokenized_doc if token.lower() not in stopwords\n",
    "    ]\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "print(punctuation)\n",
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T21:21:15.069544Z",
     "start_time": "2020-05-07T21:21:15.063560Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ejemplo de alguna noticia: {}\".format(cleaned_content[14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "### Extracci√≥n de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:18:18.064454Z",
     "start_time": "2020-05-07T19:18:03.208281Z"
    }
   },
   "outputs": [],
   "source": [
    "#La condici√≥n para que sean considerados es que aparezcan por lo menos 100 veces repetidas.\n",
    "\n",
    "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:18:24.375161Z",
     "start_time": "2020-05-07T19:18:23.796702Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usamos `Phraser` para re-tokenizamos el corpus con los bigramas encontrados. Es decir, juntamos los tokens separados que detectamos como frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:20:57.432220Z",
     "start_time": "2020-05-07T19:20:13.544388Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[cleaned_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:21:03.964097Z",
     "start_time": "2020-05-07T19:21:03.958113Z"
    }
   },
   "outputs": [],
   "source": [
    "# para ver como quedan las noticias retokenizadas, quitar comentario a la siguiente linea:\n",
    "print(sentences[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "### Definir el modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos par√°metros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada.\n",
    "- `window` : Tama√±o de la ventana. Usaremos 4.\n",
    "- `size` : El tama√±o de los embeddings que crearemos. Por lo general, el rendimiento sube cuando se usan mas dimensiones, pero despu√©s de 300 ya no se nota cambio. Ahora, usaremos solo 200.\n",
    "- `workers`: Cantidad de CPU que ser√°n utilizadas en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:29:43.525865Z",
     "start_time": "2020-05-07T15:29:43.521840Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v = Word2Vec(min_count=10,\n",
    "                      window=4,\n",
    "                      size=200,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se crear√° un conjunto que contendr√° (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:30:09.864087Z",
     "start_time": "2020-05-07T15:29:45.813793Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuaci√≥n, entenaremos el modelo. \n",
    "Los par√°metros que usaremos ser√°n: \n",
    "\n",
    "- `total_examples`: N√∫mero de documentos.\n",
    "- `epochs`: N√∫mero de veces que se iterar√° sobre el corpus.\n",
    "\n",
    "Es recomendable que tengan instalado `cpython` antes de continuar. Aumenta bastante la velocidad de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:34:44.255083Z",
     "start_time": "2020-05-07T15:30:12.141203Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=15, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. \n",
    "Esto nos permitir√° ejecutar eficientemente las tareas que realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:34:46.358716Z",
     "start_time": "2020-05-07T15:34:46.331779Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:23:19.548567Z",
     "start_time": "2020-05-07T19:23:18.572531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Si entrenaste el modelo y lo quieres guardar, descomentar el siguiente bloque.\n",
    "#if not os.path.exists('./pretrained_models'):\n",
    "#    os.mkdir('./pretrained_models')\n",
    "#biobio_w2v.save('./pretrained_models/biobio_w2v.model')\n",
    "\n",
    "\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T17:05:18.639618Z",
     "start_time": "2020-05-07T17:02:10.981724Z"
    }
   },
   "outputs": [],
   "source": [
    "# descargar el modelo desde github\n",
    "def read_model_from_github(url):\n",
    "    if not os.path.exists('./pretrained_models'):\n",
    "        os.mkdir('./pretrained_models')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    filename = url.split('/')[-1]\n",
    "    with open('./pretrained_models/' + filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "\n",
    "[\n",
    "    read_model_from_github(file) for file in [\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model',\n",
    "    ]\n",
    "]\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: Palabras mas similares y Analog√≠as\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la informaci√≥n contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matem√°tico, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:28:33.343259Z",
     "start_time": "2020-05-07T19:28:33.262476Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"perro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:28:59.154190Z",
     "start_time": "2020-05-07T19:28:59.146214Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Per√∫\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:49.464492Z",
     "start_time": "2020-05-07T19:29:49.432164Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:29:56.028795Z",
     "start_time": "2020-05-07T19:29:56.014832Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:32:29.842609Z",
     "start_time": "2020-05-07T19:32:29.829644Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:32:39.355210Z",
     "start_time": "2020-05-07T19:32:39.348228Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:32:59.165524Z",
     "start_time": "2020-05-07T19:32:59.153557Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Uber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:34.899941Z",
     "start_time": "2020-05-07T19:33:34.885980Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Huawei\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:41.167456Z",
     "start_time": "2020-05-07T19:33:41.155482Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"TVN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:47.881474Z",
     "start_time": "2020-05-07T19:33:47.871502Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"ultraderechista\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:33:55.044392Z",
     "start_time": "2020-05-07T19:33:55.033420Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Concepci√≥n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analog√≠as\n",
    "\n",
    "Por otra parte, la analog√≠a consiste en comparar 3 terminos mediante una operaci√≥n del estilo: \n",
    "\n",
    "$$palabra1 - palabra2 \\approx palabra 3 - x$$\n",
    "\n",
    "para encontrar relaciones entre estos.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| palabra 1 (pos) |  palabra 2 (neg) |\n",
    "|-----------------|------------------|\n",
    "|  macri          | pi√±era           |\n",
    "| chile           |  x               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:36:24.480150Z",
     "start_time": "2020-05-07T19:36:24.462198Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Bolsonaro\", \"Argentina\"], negative=['Macri'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:36:10.444907Z",
     "start_time": "2020-05-07T19:36:10.435930Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"Chile\", \"Huawei\"], negative=['China'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar\n",
    "\n",
    "Para visualizar, reduciremos las 200 dimensiones a 2. Para esto, usaremos [`T-SNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:38:06.260264Z",
     "start_time": "2020-05-07T19:38:06.208183Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EMBEDDINGS_TO_VISUALIZE = 10000\n",
    "\n",
    "word_counts = dict()\n",
    "for item in biobio_w2v.wv.vocab:\n",
    "    word_counts[item]=biobio_w2v.wv.vocab[item].count\n",
    "    \n",
    "sorted_word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:38:12.009463Z",
     "start_time": "2020-05-07T19:38:11.999489Z"
    }
   },
   "outputs": [],
   "source": [
    "words_to_visualize = list(sorted_word_counts.keys())[0:NUM_EMBEDDINGS_TO_VISUALIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:38:18.104614Z",
     "start_time": "2020-05-07T19:38:18.066715Z"
    }
   },
   "outputs": [],
   "source": [
    "wv_to_visualize = np.array([biobio_w2v.wv[word] for word in words_to_visualize])\n",
    "wv_to_visualize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:39:45.349695Z",
     "start_time": "2020-05-07T19:38:28.069987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutamos las transformaciones.\n",
    "tsne_components = TSNE(n_components=2, verbose=True,\n",
    "                  n_jobs=-1).fit_transform(wv_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la frecuencia de las palabras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nuevos vectores que nos entrego TSNE mas el vocabulario y la frecuencia del vocabulario, formamos un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:39:50.210194Z",
     "start_time": "2020-05-07T19:39:50.202212Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = pd.DataFrame({\n",
    "    'x': tsne_components[:,0],\n",
    "    'y': tsne_components[:,1],\n",
    "    'vocab': words_to_visualize , \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la visualizaci√≥n, usaremos plotly. Este nos deja tener una barra interactiva que nos mostrar√° las 100 palabras mas parecidas a la que escribimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:39:55.187070Z",
     "start_time": "2020-05-07T19:39:55.171110Z"
    }
   },
   "outputs": [],
   "source": [
    "described_tsne = tsne.describe()\n",
    "min_y = described_tsne.y['min']\n",
    "max_y = described_tsne.y['max']\n",
    "\n",
    "min_x =  described_tsne.x['min']\n",
    "max_x =  described_tsne.x['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:40:00.737661Z",
     "start_time": "2020-05-07T19:40:00.709715Z"
    }
   },
   "outputs": [],
   "source": [
    "textbox = widgets.Text(description='Palabra:')\n",
    "\n",
    "\n",
    "def validate(word):\n",
    "    if word in tsne.vocab.values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def response(change):\n",
    "\n",
    "    word = textbox.value\n",
    "\n",
    "    if (word == ''):\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = tsne.x.values\n",
    "            fig.data[0].y = tsne.y.values\n",
    "            fig.data[0].text = tsne.vocab.values\n",
    "    else:\n",
    "        if validate(word):\n",
    "\n",
    "            most_similar_words = [word]\n",
    "\n",
    "            for word_tuple in biobio_w2v.wv.most_similar(positive=[word],\n",
    "                                                         topn=100):\n",
    "                if word_tuple[0] in tsne.vocab.values:\n",
    "                    most_similar_words.append(word_tuple[0])\n",
    "\n",
    "            filtered_words = [\n",
    "                word in most_similar_words for word in tsne.vocab\n",
    "            ]\n",
    "            temp = tsne.loc[filtered_words]\n",
    "\n",
    "            with fig.batch_update():\n",
    "                fig.data[0].x = temp.x.values\n",
    "                fig.data[0].y = temp.y.values\n",
    "                fig.data[0].text = temp.vocab.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:27.345723Z",
     "start_time": "2020-05-07T19:44:26.863014Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.FigureWidget(data=[\n",
    "    go.Scatter(x=tsne.x,\n",
    "               y=tsne.y,\n",
    "               mode='markers',\n",
    "               text=tsne.vocab)\n",
    "],\n",
    "                      layout=go.Layout(title=go.layout.Title(\n",
    "                          text=\"Visualizaci√≥n 2D Embeddings Biobio\"),\n",
    "                                       yaxis=dict(range=[min_y, max_y]),\n",
    "                                       xaxis=dict(range=[min_x, max_x])))\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=800,\n",
    ")\n",
    "container = widgets.HBox([textbox])\n",
    "textbox.observe(response, names=\"value\")\n",
    "\n",
    "widgets.VBox([container, fig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T19:55:09.459852Z",
     "start_time": "2019-08-28T19:55:09.454865Z"
    }
   },
   "source": [
    "## Word Embeddings como caracter√≠sticas para clasificar\n",
    "\n",
    "\n",
    "En esta secci√≥n, veremos como utilizar los word embeddings como caracter√≠stica para **clasificar nuevamente el t√≥pico de las noticias de la radio biobio**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, obtendremos los datos y sus categor√≠as y dejamos solo las primeras 20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:39.200212Z",
     "start_time": "2020-05-07T19:44:39.176275Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset_r.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:45.081609Z",
     "start_time": "2020-05-07T19:44:45.065652Z"
    }
   },
   "outputs": [],
   "source": [
    "# creamos una nueva columna titulo y contenido.\n",
    "content = dataset['content'] \n",
    "\n",
    "# obtenemos las clases\n",
    "subcategory = dataset.subcategory\n",
    "\n",
    "# dejamos en el dataset solo contenido de la noticia y categoria\n",
    "dataset = pd.DataFrame({'content': content, 'category': subcategory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:44:58.537303Z",
     "start_time": "2020-05-07T19:44:58.486439Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 250\n",
    "\n",
    "categorias_seleccionadas = [\n",
    "    'america-latina', 'eeuu', 'europa', 'chile', 'region-metropolitana',\n",
    "    'region-del-bio-bio', 'negocios-y-empresas', 'region-de-los-lagos',\n",
    "    'actualidad-economica', 'region-de-valparaiso', 'region-de-la-araucania',\n",
    "    'curiosidades', 'asia', 'region-de-los-rios', 'entrevistas', 'debates',\n",
    "    'mediooriente', 'viral', 'animales', 'tu-bolsillo'\n",
    "]\n",
    "\n",
    "# filtrar solo categorias seleccionadas\n",
    "dataset = dataset[dataset['category'].isin(categorias_seleccionadas)]\n",
    "\n",
    "# balancear clases\n",
    "g = dataset.groupby('category')\n",
    "dataset = pd.DataFrame(\n",
    "    g.apply(lambda x: x.sample(NUM_SAMPLES).reset_index(drop=True))\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, transformamos cada documento del dataset en el promedio de sus embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:25.817110Z",
     "start_time": "2019-09-23T20:43:25.797172Z"
    }
   },
   "source": [
    "### Dividir el dataset en training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:45:05.304455Z",
     "start_time": "2020-05-07T19:45:05.296477Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
    "                                                    dataset.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, crearemos el Transformer con el cual convertiremos el documento a vector. (puede que les sirva para la tarea...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T20:42:22.105698Z",
     "start_time": "2020-05-17T20:42:22.094754Z"
    }
   },
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Transforma tweets a representaciones vectoriales usando alg√∫n modelo de Word Embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, aggregation_func):\n",
    "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
    "        self.model = model.wv \n",
    "        \n",
    "        # indicamos la funci√≥n de agregaci√≥n (np.min, np.max, np.mean, np.sum, ...)\n",
    "        self.aggregation_func = aggregation_func\n",
    "\n",
    "    def simple_tokenizer(self, doc, lower=False):\n",
    "        \"\"\"Tokenizador. Elimina signos de puntuaci√≥n, lleva las letras a min√∫scula(opcional) y \n",
    "           separa el tweet por espacios.\n",
    "        \"\"\"\n",
    "        if lower:\n",
    "            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        doc_embeddings = []\n",
    "        \n",
    "        for doc in X:\n",
    "            # tokenizamos el documento. Se llevan todos los tokens a min√∫scula. \n",
    "            # ojo con esto, ya que puede que tokens con min√∫scula y may√∫scula tengan\n",
    "            # distintas representaciones\n",
    "            tokens = self.simple_tokenizer(doc, lower = True) \n",
    "            \n",
    "            selected_wv = []\n",
    "            for token in tokens:\n",
    "                if token in self.model.vocab:\n",
    "                    selected_wv.append(self.model[token])\n",
    "                    \n",
    "            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo a√±adimos.\n",
    "            if len(selected_wv) > 0:\n",
    "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
    "                doc_embeddings.append(doc_embedding)\n",
    "            # si no, a√±adimos un vector de ceros que represente a ese documento.\n",
    "            else: \n",
    "                print('No pude encontrar ning√∫n embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
    "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
    "\n",
    "        return np.array(doc_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos el pipeline\n",
    "\n",
    "\n",
    "Usaremos la transformaci√≥n que creamos antes mas una regresi√≥n log√≠stica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:50:34.393237Z",
     "start_time": "2020-05-07T19:50:34.387253Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000000)\n",
    "\n",
    "doc2vec_mean = Doc2VecTransformer(biobio_w2v, np.mean)\n",
    "doc2vec_sum = Doc2VecTransformer(biobio_w2v, np.sum)\n",
    "doc2vec_max = Doc2VecTransformer(biobio_w2v, np.max)\n",
    "\n",
    "pipeline = Pipeline([('doc2vec', doc2vec_sum), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:50:59.881521Z",
     "start_time": "2020-05-07T19:50:40.532331Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:47.337340Z",
     "start_time": "2019-09-23T20:43:47.317007Z"
    }
   },
   "source": [
    "**Predecimos y evaluamos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:07.108644Z",
     "start_time": "2020-05-07T19:51:05.799089Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:12.601957Z",
     "start_time": "2020-05-07T19:51:12.589989Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:18.863055Z",
     "start_time": "2020-05-07T19:51:18.805699Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:51:25.139392Z",
     "start_time": "2020-05-07T19:51:25.132412Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.predict(\n",
    "    [(\"Alguna noticia...\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:57:03.817026Z",
     "start_time": "2019-09-24T12:57:03.814008Z"
    }
   },
   "source": [
    "## Usandolo con BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:39:09.997634Z",
     "start_time": "2020-05-07T16:39:09.992647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf_2 = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Definimos el pipeline\n",
    "pipeline_2 = Pipeline([('features',\n",
    "                        FeatureUnion([('bow', CountVectorizer()),\n",
    "                                      ('doc2vec', doc2vec_sum)])), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:44:04.514995Z",
     "start_time": "2020-05-07T16:39:13.770150Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:44:08.965415Z",
     "start_time": "2020-05-07T16:44:07.164065Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_2 = pipeline_2.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_2)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:44:11.609183Z",
     "start_time": "2020-05-07T16:44:11.553969Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias en Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:02:04.707045Z",
     "start_time": "2020-05-07T20:02:04.675157Z"
    }
   },
   "outputs": [],
   "source": [
    "from wefe.query import Query\n",
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "from wefe.metrics import RND, WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:06:09.985618Z",
     "start_time": "2020-05-07T20:06:09.979635Z"
    }
   },
   "outputs": [],
   "source": [
    "male_words = ['hombre', 'hombres', 'ni√±o', 'padre', 't√≠o', 'abuelo', 'sobrino']\n",
    "female_words = ['mujer', 'mujeres', 'ni√±a', 'madre', 't√≠a', 'abuela', 'sobrina']\n",
    "\n",
    "career = [\n",
    "    'ingeniero', 'ingeniera','abogado', 'm√©dico', 'astronauta', 'presidente',\n",
    "    'investigador', 'juez', 'acad√©mico'\n",
    "]\n",
    "\n",
    "science = [\n",
    "    'ciencia', 'tecnolog√≠a', 'f√≠sica', 'qu√≠mica', 'astronom√≠a', 'biolog√≠a',\n",
    "    'investigaci√≥n', 'computaci√≥n'\n",
    "]\n",
    "arts = [\n",
    "    'arte', 'm√∫sica', 'danza', 'cocina', 'cine', 'lectura', 'escritura',\n",
    "    'libros', 'libro', 'ficci√≥n', 'teatro'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:06:01.029061Z",
     "start_time": "2020-05-07T20:06:01.023079Z"
    }
   },
   "outputs": [],
   "source": [
    "'ingeniera' in biobio_w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:02:16.765474Z",
     "start_time": "2020-05-07T20:02:16.758493Z"
    }
   },
   "outputs": [],
   "source": [
    "model = WordEmbeddingModel(biobio_w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:06:16.285136Z",
     "start_time": "2020-05-07T20:06:16.278154Z"
    }
   },
   "outputs": [],
   "source": [
    "RND().run_query(\n",
    "    Query([male_words, female_words], [career], ['Hombre', 'Mujer'],\n",
    "          ['Carrera']), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:03:02.693925Z",
     "start_time": "2020-05-07T20:03:02.685941Z"
    }
   },
   "outputs": [],
   "source": [
    "RND().run_query(\n",
    "    Query([male_words, female_words], [science], ['Hombre', 'Mujer'],\n",
    "          ['Ciencia']), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T20:03:40.273279Z",
     "start_time": "2020-05-07T20:03:40.266298Z"
    }
   },
   "outputs": [],
   "source": [
    "RND().run_query(\n",
    "    Query([male_words, female_words], [arts], ['Hombre', 'Mujer'],\n",
    "          ['Arte']), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propuesto...\n",
    "\n",
    "- Usar su modelo de embeddings favorito para ver si mejora la clasificaci√≥n: \n",
    "    \n",
    " - Fast y word2vec en espa√±ol, [cortes√≠a](https://github.com/dccuchile/spanish-word-embeddings) de los grandes del DCC\n",
    " - [Conceptnet](https://github.com/commonsense/conceptnet-numberbatch)\n",
    "\n",
    "\n",
    "- Visualizar los documentos usando `doc2vec`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
