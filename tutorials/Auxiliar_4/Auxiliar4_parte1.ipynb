{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Auxiliar4_parte1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOBdG3sFwZD+0k1C/rBGT/l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OeILwNymKkjn","colab_type":"text"},"source":["# Auxiliar 4, parte 1\n","Esta auxiliar va a tener 3 partes, primero vamos a volver a visitar el ejemplo de clasificación de documentos que vimos en la auxiliar pasada, pero esta vez usaremos algunas de las utilidades que nos ofrece pytorch para facilitar el trabajo. Además, probaremos cargando embeddings preentrenados para ver como afecta esto al desempeño de nuestro modelo. Las otras dos partes son ejemplos de CNN y de RNN, pero lo voy a separar en varios notebooks para que quede mas organizado.\n","\n","Esta clase sera principalmente práctica, así que la mayor parte de las explicaciones las voy a incluir en el código."]},{"cell_type":"code","metadata":{"id":"2Bjt7jpp57a5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593659197924,"user_tz":240,"elapsed":8072,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["%%capture --no-stderr\n","!pip install --upgrade torchtext\n","!pip install --upgrade tqdm # resuelve un problema de mas adelante (?)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnCnZXoC4mqZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593659198482,"user_tz":240,"elapsed":6473,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["# Si quieren tener resultados reproducibles (obtener los mismos resultados\n","# al correr el notebook varias veces) es necesario setear la \"semilla\" para\n","# las librerias que general los numeros aleatorios usados en el script.\n","# Esta aleatoriedad puede aparecer al crear splits aleatorios de un dataset,\n","# al inicializar los pesos de una red, al agregar una capa de dropout, etc.\n","\n","# Pueden leer un poco mas acerca de esto aca\n","# https://pytorch.org/docs/stable/notes/randomness.html\n","import random\n","import torch\n","torch.manual_seed(8888)\n","random.seed(8888)\n","torch.backends.cudnn.deterministic = True"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Yd4ZZJkKfY0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593659198485,"user_tz":240,"elapsed":5471,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["import csv\n","import gzip\n","import os\n","import shutil\n","\n","import requests\n","\n","# Descargar el dataset, lo mismo que antes, solo que ahora lo voy a\n","# guardar en un archivo porque la API de torch text usa\n","# archivos y no file-like objects :(\n","\n","# Leer los datos de esta forma permite realizar filtrado y modificaciones\n","# a medida que se van stremeando los datos de la red. Esto no es para nada\n","# necesario, y podrian usar algo como wget para descargar los datos y luego\n","# hacer preprocesamiento mas adelante en su codigo.\n","DATASET_FILE = \"argument_mining.tsv\"\n","GLOVE_FILE = \"glove300d.vec\"\n","if not os.path.exists(DATASET_FILE):\n","    print(f\"Descargando {DATASET_FILE}\")\n","    # Descargar los datos directamente de github\n","    url = \"https://github.com/uchile-nlp/ArgumentMining2017/raw/master/data/complete_data.csv.gz\"\n","    # Usar stream=True para que no se descarguen todos los datos a la vez\n","    response = requests.get(url, stream=True)\n","    try:\n","        with open(DATASET_FILE, \"w\") as out_file:\n","            writer = csv.writer(out_file, delimiter=\"\\t\")\n","            for row in csv.DictReader(\n","                # Descomprimir a medida que se leen los datos de la red\n","                gzip.open(response.raw, mode=\"rt\"),\n","                strict=True,\n","                escapechar=\"\\\\\",\n","            ):\n","                # Nos quedamos solamente con el primer topico para efectos\n","                # del ejemplo\n","                if row[\"topic\"] == \"1\" and row[\"argument\"]:\n","                    # Solamente las columnas que nos interesan del dataset\n","                    writer.writerow(\n","                        [row[\"constitutional_concept\"], row[\"argument\"]]\n","                    )\n","    except Exception as e:\n","        os.remove(DATASET_FILE)\n","        raise e\n","\n","# Descargar vectores glove de aca\n","# https://github.com/dccuchile/spanish-word-embeddings\n","if not os.path.exists(GLOVE_FILE):\n","    print(f\"Descargando {GLOVE_FILE}\")\n","    url = \"http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.vec.gz\"\n","    response = requests.get(url, stream=True)\n","    try:\n","        with gzip.open(response.raw, \"rb\") as f_in:\n","            with open(GLOVE_FILE, \"wb\") as f_out:\n","                # Funcion util para copiar de un file-like object a otro\n","                shutil.copyfileobj(f_in, f_out)\n","    except Exception as e:\n","        os.remove(GLOVE_FILE)\n","        raise e\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKsEMe0j_j04","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593659203694,"user_tz":240,"elapsed":8967,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["%%capture --no-stderr\n","!python -m spacy download es"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrAJnR6x-zma","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593659213423,"user_tz":240,"elapsed":17886,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["from string import whitespace, punctuation\n","from spacy.lang.es.stop_words import STOP_WORDS\n","from torchtext import data\n","\n","# Para cargar un dataset en torchtext primero deben definir\n","# los campos del dataset (Fields), donde tambien se guarda informacion\n","# asociada a este campo. Por ejemplo, si quieren que se tokenize (los \n","# campos de tipo `sequential` son tokenizados), que tokenizer usar,\n","# as stop_words que seran ignoradas luego al crear el vocabulario, etc.\n","# Son estos mismos Fields los que se usan luego para crear sus vocabularios\n","# asociados\n","\n","# La documentacion sobre un Field la pueden encontrar aca\n","# https://pytorch.org/text/data.html#torchtext.data.Field\n","\n","LABEL = data.Field(\n","    sequential=False, pad_token=None, unk_token=None, is_target=True\n",")\n","TEXT = data.Field(\n","    tokenize=\"spacy\",\n","    tokenizer_language=\"es\",\n","    stop_words=STOP_WORDS.union(whitespace, punctuation),\n","    lower=True,\n","    batch_first=True,\n",")\n","\n","# Ahora con los Fields definidos es super simple cargar el dataset, le dan\n","# la ubicacion del dataset, el tipo de separacion entre columnas y los fields\n","# que definieron y torchtext sabe que hacer para separar el dataset en esos\n","# fields.\n","dataset = data.TabularDataset(\n","    path=\"argument_mining.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"label\", LABEL), (\"text\", TEXT)],\n",")\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnCiYbheImq_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593659608334,"user_tz":240,"elapsed":125299,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"e36c0337-dd05-4e8a-ab0d-c2c67c299098"},"source":["from torchtext import vocab\n","\n","# Para cargar los vectores de embeddings (que son esencialmente un vocabulario\n","# donde cada palabra tiene asociado un vector) pueden usar la clase vocab.Vectors\n","es_embeddings = vocab.Vectors(GLOVE_FILE)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["  0%|          | 0/855380 [00:00<?, ?it/s]Skipping token b'855380' with 1-dimensional vector [b'300']; likely a header\n","100%|██████████| 855380/855380 [01:57<00:00, 7309.86it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NyTLNZhSAWbt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593659725022,"user_tz":240,"elapsed":1345,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["from operator import attrgetter\n","\n","# Para construir el vocabulario es muy simple, llaman build_vocab para cada\n","# Field y le pasan al metodo la el dataset de donde se debe obtener el\n","# vocabulario para este field. Tambien le pueden pasar varios datasets y el\n","# Field va a obtener el vocabulario de todos ellos\n","LABEL.build_vocab(dataset)\n","TEXT.build_vocab(dataset)\n","\n","# Para asociarle a los tokens vectores de embedding a partir de una lista\n","# de embeddings pueden usar el metodo .set_vector sobre el vocab del Field.\n","# Este metodo recibe 3 argumentos, un mapeo del token al indice dentro de la\n","# lista de embeddings que corresponde a dicho token, la lista de vectores\n","# de embedding y la dimension de los embeddings\n","TEXT.vocab.set_vectors(*attrgetter(\"stoi\", \"vectors\", \"dim\")(es_embeddings))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGYOphaaI9Iy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"status":"ok","timestamp":1593659741100,"user_tz":240,"elapsed":943,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"4f219917-c3ac-4fec-d61c-c99620338611"},"source":["from pprint import pprint\n","# Mostremos algunos ejemplos\n","pprint(LABEL.vocab.itos[:10])\n","print()\n","pprint(TEXT.vocab.freqs.most_common(10))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["['Justicia',\n"," 'Respeto / Conservación de la naturaleza o medio ambiente',\n"," 'Igualdad',\n"," 'Democracia',\n"," 'Descentralización',\n"," 'Bien Común / Comunidad',\n"," 'Respeto',\n"," 'Dignidad',\n"," 'Autonomía / Libertad',\n"," 'Equidad de género']\n","\n","[('y', 43237),\n"," ('a', 22099),\n"," ('respeto', 5656),\n"," ('derechos', 5592),\n"," ('igualdad', 5417),\n"," ('personas', 5282),\n"," ('sociedad', 5047),\n"," ('país', 4266),\n"," ('constitución', 3714),\n"," ('desarrollo', 3665)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lq-S-GI9GEWK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593659920407,"user_tz":240,"elapsed":1041,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["import torch.nn as nn\n","\n","# Aca sedefine la arquitectura, para esto usamos una capa de embedding a traves\n","# de nn.EmbeddingBag, que al pasarle una frase, pasa los tokens por la lista\n","# de embeddings y los agrega segun `mode` (en este caso los promedia).\n","# Usamos una capa lineal para hacer clasificacion.\n","class ArgumentClassifier(nn.Module):\n","    def __init__(self, embedding_weights, num_class, freeze_embeddings=True):\n","        super().__init__()\n","        self.embedding = nn.EmbeddingBag.from_pretrained(\n","            embedding_weights.clone(), freeze=freeze_embeddings, mode=\"mean\"\n","        )\n","        self.fc = nn.Linear(embedding_weights.shape[1], num_class)\n","\n","    def forward(self, text):\n","        # text -> (B, N)\n","        embedded = self.embedding(text)\n","        return self.fc(embedded)\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOdZC412IBYu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593660154613,"user_tz":240,"elapsed":819,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}}},"source":["# Funciones de entrenamiento de toda la vida. Noten que no es necesario\n","# para los tensores `text` y `labels` a la GPU porque ya estan ahi. Esto\n","# es porque los iteradores de torchtext permiten definir un device para\n","# que los tensores sean trasladados automaticamente.\n","def train_func(train_iter):\n","    train_loss, train_acc, n_total = 0, 0, 0\n","    for text, labels in train_iter:\n","        optimizer.zero_grad()\n","        output = model(text)\n","        loss = criterion(output, labels)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += (output.argmax(dim=1) == labels).sum().item()\n","        n_total += len(labels)\n","\n","    return train_loss / n_total, train_acc / n_total\n","\n","\n","def test_func(test_iter):\n","    test_loss, test_acc, n_total = 0, 0, 0\n","    for text, labels in test_iter:\n","        with torch.no_grad():\n","            output = model(text)\n","            loss = criterion(output, labels)\n","            test_loss += loss.item()\n","            test_acc += (output.argmax(dim=1) == labels).sum().item()\n","            n_total += len(labels)\n","\n","    return test_loss / n_total, test_acc / n_total\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"R__cSEuYL2lr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":330},"executionInfo":{"status":"ok","timestamp":1593660182602,"user_tz":240,"elapsed":27420,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"cce3cff3-e655-4f36-83de-7627fc045377"},"source":["import time\n","\n","# Hyperparametros de la red\n","N_EPOCHS = 6\n","LEARN_RATE = 4.0\n","BATCH_SIZE = 64\n","\n","# Esta parte permite que el codigo corra aunque no haya cuda disponible\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Instanciamos el modelo y lo pasamos al device correspondiente\n","model = ArgumentClassifier(\n","    embedding_weights=TEXT.vocab.vectors, num_class=len(LABEL.vocab),\n",").to(device)\n","\n","# Instanciamos el criterio que escogimos (en este caso CrossEntropyLoss)\n","# y el optimizer (la forma en la que se actualizan los parametros)\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n","\n","\n","# Aca usamos el BucketIterator y creams iteradores a partir de splits. Usamos\n","# el metodo split() del dataset para separarlo en 2 splits. Noten que podemos\n","# definir un `device` en esta etapa.\n","train_iter, val_iter = data.BucketIterator.splits(\n","    dataset.split(0.7, stratified=True, strata_field=\"label\"),\n","    batch_sizes=(BATCH_SIZE, 256),\n","    shuffle=True,\n","    device=device,\n","    sort=False, # si no ordena los que no son train (esto creo que es una falla de disenno en torchtext)\n",")\n","\n","\n","# Esto es lo de toda la vida para entrenar y reportar resultados por epoca\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(train_iter)\n","    valid_loss, valid_acc = test_func(val_iter)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs // 60\n","    secs = secs % 60\n","\n","    print(\n","        f\"Epoch: {epoch + 1}\", f\" | time in {mins} minutes, {secs} seconds\",\n","    )\n","    print(\n","        f\"\\tLoss: {train_loss:.4f}(train)\\t|\"\n","        f\"\\tAcc: {train_acc * 100:.1f}%(train)\"\n","    )\n","    print(\n","        f\"\\tLoss: {valid_loss:.4f}(valid)\\t|\"\n","        f\"\\tAcc: {valid_acc * 100:.1f}%(valid)\"\n","    )\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Epoch: 1  | time in 0 minutes, 1 seconds\n","\tLoss: 0.0407(train)\t|\tAcc: 37.2%(train)\n","\tLoss: 0.0097(valid)\t|\tAcc: 44.8%(valid)\n","Epoch: 2  | time in 0 minutes, 1 seconds\n","\tLoss: 0.0331(train)\t|\tAcc: 48.7%(train)\n","\tLoss: 0.0089(valid)\t|\tAcc: 46.9%(valid)\n","Epoch: 3  | time in 0 minutes, 1 seconds\n","\tLoss: 0.0308(train)\t|\tAcc: 51.7%(train)\n","\tLoss: 0.0084(valid)\t|\tAcc: 49.7%(valid)\n","Epoch: 4  | time in 0 minutes, 1 seconds\n","\tLoss: 0.0296(train)\t|\tAcc: 52.9%(train)\n","\tLoss: 0.0082(valid)\t|\tAcc: 50.6%(valid)\n","Epoch: 5  | time in 0 minutes, 1 seconds\n","\tLoss: 0.0287(train)\t|\tAcc: 54.1%(train)\n","\tLoss: 0.0079(valid)\t|\tAcc: 51.7%(valid)\n","Epoch: 6  | time in 0 minutes, 1 seconds\n","\tLoss: 0.0282(train)\t|\tAcc: 54.6%(train)\n","\tLoss: 0.0078(valid)\t|\tAcc: 53.3%(valid)\n"],"name":"stdout"}]}]}