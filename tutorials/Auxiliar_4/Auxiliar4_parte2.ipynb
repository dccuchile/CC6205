{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Auxiliar4_parte2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPlqPPrtsgCKw+wcSdxL32l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ja6suqJmuHJt","colab_type":"text"},"source":["# Ejemplo de red con capa convolucional\n","\n","En esta segunda parte de la auxiliar vamos a comparar el desempeño de una capa lineal + softmax con la arquitectura de CNN que se menciona en las diapositivas. Nuevamente esta parte es super práctica así que revisen el código y lean los comentarios."]},{"cell_type":"code","metadata":{"id":"8jXTLEsBVty1","colab_type":"code","colab":{}},"source":["%%capture --no-stderr\n","!pip install --upgrade torchtext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnTtFWP8IoGu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1593663946503,"user_tz":240,"elapsed":22505,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"6c780f59-0ae4-43b3-817e-cdbf37017d1d"},"source":["# Referencia https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n","# Este ejemplo de clasificacion de texto lo saque de la referencia de arriba\n","# La idea es comparar la red convolucional con los resultados de una red\n","# feed forward simple para clasificar texto. Le hice algunas modificaciones\n","# pequennas, ya sea para simplificarlo o para mostrar alguna otra utilidad\n","# de pytorch\n","import os\n","import random\n","import torch\n","import torchtext\n","from torchtext.datasets import text_classification\n","\n","# Setear seed para que los resultados sean replicables\n","torch.manual_seed(8888)\n","random.seed(8888)\n","\n","# Cargar el dataset y cachearlo\n","if not os.path.isdir('./.data'):\n","    os.mkdir('./.data')\n","\n","train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n","    root='./.data', vocab=None)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["120000lines [00:05, 23554.77lines/s]\n","120000lines [00:11, 10676.09lines/s]\n","7600lines [00:00, 11008.10lines/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cjjHzDEmV6Na","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Definir la primera arquitectura, esta es nuevamente una capa de embeddings\n","# y una capa lineal. Fijense en la funcion para inicializar los pesos. Esta\n","# comentada para no alterar la comparacion con la otra, pero la pueden\n","# descomentar y probar como cambian los resultados. Ademas les sirve como\n","# ejemplo para  saber como cambiar la inicializacion de los pesos para sus\n","# experimentos en la competencia 2\n","class TextSentimentLinear(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super().__init__()\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"max\")\n","        self.fc = nn.Linear(embed_dim, num_class)\n","        # self.init_weights() # probar agregando esto\n","\n","    def init_weights(self): \n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.embedding.weight.data[train_dataset.get_vocab()[\"<pad>\"]].zero_()\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        return self.fc(embedded)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXng7SD1q9Ya","colab_type":"code","colab":{}},"source":["# Esta es una red con capa convolucional. La documentacion la pueden pilla\n","# aqui: https://pytorch.org/docs/stable/nn.html#conv1d\n","# Lo demas deberia ser identico a la red vista en clases\n","class TextSentimentConv1d(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_class):\n","        super().__init__()\n","        self.embedding = nn.Embedding(\n","            vocab_size,\n","            embed_dim,\n","            padding_idx=train_dataset.get_vocab()[\"<pad>\"]\n","        )\n","        # El primer argumento corresponde a la cantidad de canales de entrada,\n","        # a diferencia de las imagenes que tienen 3 canales (RGB) nuestra frase\n","        # es un puro canal. El segundo argumento corresponde a los canales de\n","        # salida, que para este caso es la cantidad de clases que tenemos,\n","        # porque queremos tomar el tensor de salida, hacer maxpooling y usarlo\n","        # directo para clasificar. El tamanno del kernel, dado que queremos\n","        # hacer una convolucion para 3 paralabras, tiene que ser 3 x embed_dim\n","        # porque en la practica la convolucion se realiza sobre los embeddings\n","        # de la palabra y no la palabra misma. Finalmente el stride corresponde\n","        # a la cantidad de elementos que se `saltan` al avanzar la ventana.\n","        # Como no queremos hacer convoluciones con la mitad del embedding de una\n","        # y la mitad del embedding de otra, tenemos que saltarnos una cantidad\n","        # de elementos igual al tamanno de los vectores de embedding\n","        self.conv = nn.Conv1d(\n","            in_channels=1,\n","            out_channels=num_class,\n","            kernel_size=3*embed_dim,\n","            stride=embed_dim\n","        )\n","    \n","    def forward(self, text):\n","        # embedded -> B, N, E\n","        embedded = self.embedding(text)\n","        # ahora hay que hacerle un flatten a las frases, es decir, concatenar\n","        # los vectores de embedding para hacer la convolucion. El 1 extra\n","        # en el metodo view es por un requisito de la capa convolucional, \n","        # corresponse a la cantidad de `canales` (piensen en imagenes rgb),\n","        # pero como nuestra frase es un puro canal, ahi va un 1\n","        embedded = embedded.view(embedded.shape[0], 1, -1)\n","        z = F.relu(self.conv(embedded))\n","        # luego hacemos maxpooling, se usa el atributo .values porque la funcion\n","        # que hace saca el maximo, cuando se le pasa una dimension, retorna\n","        # tambien los indices en los que se encontraron los maximos.\n","        return z.max(dim=-1).values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4zMQqDwvWDi3","colab_type":"code","colab":{}},"source":["# Esta es la funcion que se usa para generar los batch a partir de una lista de \n","# frases. Fijense el uso de la funcion de utilidad `pad_sequence` que realiza\n","# el padding necesario a cada frase para que queden todas del mismo largo.\n","def generate_batch(batch):\n","    labels = torch.tensor([entry[0] for entry in batch])\n","    sequences = [entry[1] for entry in batch]\n","    text = nn.utils.rnn.pad_sequence(\n","        sequences, batch_first=True, padding_value=train_dataset.get_vocab()[\"<pad>\"]\n","    )\n","    return text, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4LMOVa-WWInG","colab_type":"code","colab":{}},"source":["# Esta celda es lo de toda la vida, definir funciones para entrenar y evaluar\n","from torch.utils.data import DataLoader\n","\n","def train_func(sub_train_, model):\n","\n","    # Train the model\n","    train_loss ,train_acc = 0, 0\n","    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n","                      collate_fn=generate_batch)\n","    for text, cls in data:\n","        optimizer.zero_grad()\n","        text, cls = text.to(device), cls.to(device)\n","        output = model(text)\n","        loss = criterion(output, cls)\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += (output.argmax(1) == cls).sum().item()\n","\n","    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n","\n","def test(data_, model):\n","    loss, acc = 0, 0\n","    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    for text, cls in data:\n","        text, cls = text.to(device), cls.to(device)\n","        with torch.no_grad():\n","            output = model(text)\n","            loss = criterion(output, cls)\n","            loss += loss.item()\n","            acc += (output.argmax(1) == cls).sum().item()\n","\n","    return loss / len(data_), acc / len(data_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ft69D7zSWN6k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":642},"executionInfo":{"status":"ok","timestamp":1593664284063,"user_tz":240,"elapsed":33514,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"985d9229-ddf6-458b-bf5f-8a104a72463c"},"source":["# Esta celda y la siguiente entrenan los modelos para poder comparar el \n","# desempenno. Todo esto deberia ser ya conocido por ustedes asi que no lo voy\n","# a comentar. Cualquier duda siempre estamos disponibles por el foro o telegram\n","\n","import time\n","from torch.utils.data.dataset import random_split\n","\n","N_EPOCHS = 10\n","BATCH_SIZE = 128\n","EMBED_DIM = 32\n","LEARN_RATE = 3.0\n","\n","VOCAB_SIZE = len(train_dataset.get_vocab())\n","NUM_CLASS = len(train_dataset.get_labels())\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","train_len = int(len(train_dataset) * 0.85)\n","sub_train_, sub_valid_ = \\\n","    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n","\n","model = TextSentimentLinear(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n","\n","print(f\"\\nEntrenando modelo {model}\\n\")\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(sub_train_, model)\n","    valid_loss, valid_acc = test(sub_valid_, model)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs / 60\n","    secs = secs % 60\n","\n","    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Entrenando modelo TextSentimentLinear(\n","  (embedding): EmbeddingBag(95812, 32, mode=max)\n","  (fc): Linear(in_features=32, out_features=4, bias=True)\n",")\n","\n","Epoch: 1  | time in 0 minutes, 2 seconds\n","\tLoss: 0.1200(train)\t|\tAcc: 68.5%(train)\n","\tLoss: 0.0003(valid)\t|\tAcc: 86.2%(valid)\n","Epoch: 2  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0216(train)\t|\tAcc: 84.0%(train)\n","\tLoss: 0.0007(valid)\t|\tAcc: 69.8%(valid)\n","Epoch: 3  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0144(train)\t|\tAcc: 87.0%(train)\n","\tLoss: 0.0003(valid)\t|\tAcc: 83.6%(valid)\n","Epoch: 4  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0096(train)\t|\tAcc: 89.5%(train)\n","\tLoss: 0.0003(valid)\t|\tAcc: 86.9%(valid)\n","Epoch: 5  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0069(train)\t|\tAcc: 91.4%(train)\n","\tLoss: 0.0004(valid)\t|\tAcc: 83.1%(valid)\n","Epoch: 6  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0051(train)\t|\tAcc: 93.2%(train)\n","\tLoss: 0.0002(valid)\t|\tAcc: 86.1%(valid)\n","Epoch: 7  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0034(train)\t|\tAcc: 94.5%(train)\n","\tLoss: 0.0003(valid)\t|\tAcc: 86.1%(valid)\n","Epoch: 8  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0025(train)\t|\tAcc: 95.6%(train)\n","\tLoss: 0.0003(valid)\t|\tAcc: 87.0%(valid)\n","Epoch: 9  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0013(train)\t|\tAcc: 96.9%(train)\n","\tLoss: 0.0003(valid)\t|\tAcc: 84.5%(valid)\n","Epoch: 10  | time in 0 minutes, 2 seconds\n","\tLoss: 0.0013(train)\t|\tAcc: 97.4%(train)\n","\tLoss: 0.0003(valid)\t|\tAcc: 86.6%(valid)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K9WRk5BEYP-L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593664381428,"user_tz":240,"elapsed":94219,"user":{"displayName":"Gabriel Chaperon","photoUrl":"","userId":"04620641081898711799"}},"outputId":"b8d22b7e-7c0a-4b5b-e9ea-65a4323e9a20"},"source":["N_EPOCHS = 20\n","BATCH_SIZE = 128\n","EMBED_DIM = 32\n","LEARN_RATE = 3.0\n","\n","model = TextSentimentConv1d(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n","\n","print(f\"\\nEntrenando modelo {model}\\n\")\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    train_loss, train_acc = train_func(sub_train_, model)\n","    valid_loss, valid_acc = test(sub_valid_, model)\n","\n","    secs = int(time.time() - start_time)\n","    mins = secs / 60\n","    secs = secs % 60\n","\n","    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n","    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n","    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Entrenando modelo TextSentimentConv1d(\n","  (embedding): Embedding(95812, 32, padding_idx=1)\n","  (conv): Conv1d(1, 4, kernel_size=(96,), stride=(32,))\n",")\n","\n","Epoch: 1  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0117(train)\t|\tAcc: 57.7%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 69.2%(valid)\n","Epoch: 2  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0071(train)\t|\tAcc: 72.3%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 75.5%(valid)\n","Epoch: 3  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0059(train)\t|\tAcc: 76.9%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 77.1%(valid)\n","Epoch: 4  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0053(train)\t|\tAcc: 79.2%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 80.0%(valid)\n","Epoch: 5  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0049(train)\t|\tAcc: 80.4%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 79.8%(valid)\n","Epoch: 6  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0046(train)\t|\tAcc: 81.6%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 81.7%(valid)\n","Epoch: 7  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0044(train)\t|\tAcc: 82.4%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 81.9%(valid)\n","Epoch: 8  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0043(train)\t|\tAcc: 82.8%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 82.5%(valid)\n","Epoch: 9  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0041(train)\t|\tAcc: 83.7%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 81.8%(valid)\n","Epoch: 10  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0040(train)\t|\tAcc: 84.2%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 82.9%(valid)\n","Epoch: 11  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0038(train)\t|\tAcc: 84.7%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 84.2%(valid)\n","Epoch: 12  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0037(train)\t|\tAcc: 85.2%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 83.9%(valid)\n","Epoch: 13  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0036(train)\t|\tAcc: 85.4%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 84.5%(valid)\n","Epoch: 14  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0035(train)\t|\tAcc: 85.9%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 84.2%(valid)\n","Epoch: 15  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0034(train)\t|\tAcc: 86.3%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 83.9%(valid)\n","Epoch: 16  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0033(train)\t|\tAcc: 86.6%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 84.9%(valid)\n","Epoch: 17  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0032(train)\t|\tAcc: 86.9%(train)\n","\tLoss: 0.0001(valid)\t|\tAcc: 84.9%(valid)\n","Epoch: 18  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0031(train)\t|\tAcc: 87.2%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 85.1%(valid)\n","Epoch: 19  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0030(train)\t|\tAcc: 87.5%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 84.4%(valid)\n","Epoch: 20  | time in 0 minutes, 4 seconds\n","\tLoss: 0.0030(train)\t|\tAcc: 87.7%(train)\n","\tLoss: 0.0000(valid)\t|\tAcc: 85.6%(valid)\n"],"name":"stdout"}]}]}